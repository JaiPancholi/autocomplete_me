{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "autocomplete_me",
      "language": "python",
      "name": "autocomplete_me"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "BBC Politics - Trial Own Process.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvWcI8T2eOvw",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNXYErppeOvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b1db7b79-8c28-44a4-a10a-20676a70cfa2"
      },
      "source": [
        "# Google Only\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ROOT_FOLDER = '/content/drive/My Drive/Code/autocomplete_me/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9p9MmnceOv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Set Variables for Local and Cloud File Finding\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(ROOT_FOLDER)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W397RMzgCvy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "86920f7c-4a01-4723-cfcd-537bf2f65e4c"
      },
      "source": [
        "!ls -l '/content/drive/My Drive/Code/autocomplete_me/src'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 27\n",
            "-rw------- 1 root root 2516 Jul 14 22:20 predict_utils.py\n",
            "drwx------ 2 root root 4096 Jul 11 13:20 __pycache__\n",
            "-rw------- 1 root root 3340 Jul 15 12:47 reader.py\n",
            "-rw------- 1 root root 3580 Jul 14 22:20 train_model_baseline.py\n",
            "-rw------- 1 root root 3203 Jul 14 22:20 train_utils.py\n",
            "-rw------- 1 root root 9341 Jul 12 14:24 utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fiv0531eOv7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26c5b508-b963-40d7-daaf-bb56d0604272"
      },
      "source": [
        "from src import utils, reader, predict_utils, train_utils\n",
        "from importlib import reload\n",
        "reload(utils)\n",
        "reload(reader)\n",
        "reload(predict_utils)\n",
        "reload(train_utils)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'src.train_utils' from '/content/drive/My Drive/Code/autocomplete_me/src/train_utils.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpn1lT4IeOv_",
        "colab_type": "text"
      },
      "source": [
        "## Load Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhWMNtqceOv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = reader.read_bbc_politics()\n",
        "content_type = 'BBC-Politics'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YyHF8aQeOwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "4b82c4e5-1116-4c25-f8ea-3d618671970c"
      },
      "source": [
        "text[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Tory expert denies defeat warning\\n\\nThe Conservatives\\' campaign director has denied a report claiming he warned Michael Howard the party could not win the next general election.\\n\\nThe Times on Monday said Australian Lynton Crosby told the party leader to focus on trying to increase the Tories\\' Commons presence by 25 to 30 seats. But Mr Crosby said in a statement: \"I have never had any such conversation... and I do not hold that view.\" Mr Howard later added there was not \"one iota\" of truth in the report. The strategist helped Australia\\'s PM, John Howard, win four elections. Mr Howard appointed Mr Crosby as his elections chief last October. Mr Crosby\\'s statement said: \"The Conservative Party has been making an impact on the issues of lower tax and controlled immigration over the past week.\" It added: \"The Labour Party will be wanting to do all they can to distract attention away from the issues that really matter to people.\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8y2Nv_MeOwE",
        "colab_type": "text"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Q8y2ILeOwH",
        "colab_type": "text"
      },
      "source": [
        "## Process Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtdMjwySG_ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences, num_words, word_idx, idx_word = train_utils.preprocess_text(text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESiYIwBkHN7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54cb527a-b198-4406-ad53-d96ee64dea3c"
      },
      "source": [
        "features, labels = train_utils.pass_sliding_window(sequences, sequence_len=10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 186099 sequences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gny329GmHSMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bb842209-e532-4ad1-d1ba-ce8d2ac35a8e"
      },
      "source": [
        "labels = train_utils.one_hot_labels_and_improve_efficiency(labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels matrix shape:  (186099, 11963)\n",
            "Labels matrix shape:  (186099, 11963)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eny8qjVzeOwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Test Train Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20, random_state=42, shuffle=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x5EeUMbkpPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ac87d78-2e09-436f-d624-07f580b1bc72"
      },
      "source": [
        "import gc\n",
        "gc.enable()\n",
        "del labels\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8N7zK7IeOwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5f7a4faf-c4c3-46b7-ccf0-694d345a9167"
      },
      "source": [
        "print('X_train shape: ', X_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape:  (148879, 10)\n",
            "X_test shape:  (37220, 10)\n",
            "y_train shape:  (148879, 11963)\n",
            "y_test shape:  (37220, 11963)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOA6uzXKlu0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d643325b-2e94-4746-a23b-7cba3ac10b09"
      },
      "source": [
        "import sys\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "                         key= lambda x: -x[1])[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       y_train:  1.7 GiB\n",
            "                        y_test: 424.6 MiB\n",
            "                      features: 14.2 MiB\n",
            "                       X_train: 11.4 MiB\n",
            "                        X_test:  2.8 MiB\n",
            "                      word_idx: 576.1 KiB\n",
            "                      idx_word: 576.1 KiB\n",
            "                          text:  3.7 KiB\n",
            "                     sequences:  3.5 KiB\n",
            "                            __:  985.0 B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DROHQO32eOwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "8a915a8b-e819-412d-a02b-b4c74fda27b6"
      },
      "source": [
        "# Embedding Matrix\n",
        "# embedding_matrix = utils.create_embedding_matrix(word_idx, num_words, '/Users/jaipancholi/data/glove.6B.100d.txt')\n",
        "embedding_matrix = utils.create_embedding_matrix(word_idx, num_words, '/content/drive/My Drive/Code/autocomplete_me/data/glove.6B.100d.txt')\n",
        "embedding_matrix"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glove Vectors loading with dimension 100\n",
            "There were 986 words without pre-trained embeddings.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Code/autocomplete_me/src/utils.py:180: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.00656124, -0.04206555,  0.12508174, ..., -0.02506376,\n",
              "         0.14220549,  0.04648907],\n",
              "       [-0.02940788,  0.00775488,  0.02958461, ..., -0.0617054 ,\n",
              "         0.07386386, -0.02477734],\n",
              "       ...,\n",
              "       [-0.00428263,  0.25175653,  0.0238415 , ...,  0.0984367 ,\n",
              "        -0.01810912, -0.17835365],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.04594895,  0.09532217, -0.11963347, ...,  0.12868619,\n",
              "        -0.04211046,  0.03951213]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IZp-EmFeOwY",
        "colab_type": "text"
      },
      "source": [
        "# Design Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gstEw4vgGsPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1eSpudQeOwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZOti2z7JJW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=False, epochs=100):\n",
        "  if not model and not use_pretrained_model:\n",
        "    print('Provide one of either model or use_pretrained_model.')\n",
        "  elif model and use_pretrained_model:\n",
        "      print('Provide one of either model or use_pretrained_model.')\n",
        "  elif use_pretrained_model:\n",
        "    model = load_model(model_filepath)\n",
        "  \n",
        "  callbacks = [\n",
        "      EarlyStopping(monitor='val_accuracy', patience=25),\n",
        "      ModelCheckpoint(f'{model_filepath}', save_best_only=True, save_weights_only=False, monitor='val_accuracy')\n",
        "  ]\n",
        "\n",
        "  history = model.fit(\n",
        "      X_train, \n",
        "      y_train, \n",
        "      epochs=epochs, \n",
        "      batch_size=2048, \n",
        "      validation_data=(X_test, y_test), \n",
        "      verbose=1,\n",
        "      callbacks=callbacks\n",
        "  )\n",
        "\n",
        "  return history"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHNsyM7H_qkC",
        "colab_type": "text"
      },
      "source": [
        "##V1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoT44K4meOwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(64))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBtAHhA1eOwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-1.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3pRn4k_2Jx",
        "colab_type": "text"
      },
      "source": [
        "## V2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSI6BxH5ALw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfRMwq-rALzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b450af99-9652-417f-d6fe-2f52bc41d7dc"
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-2.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "73/73 [==============================] - 9s 127ms/step - loss: 7.4230 - accuracy: 0.0612 - val_loss: 6.9656 - val_accuracy: 0.0647\n",
            "Epoch 2/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 6.8681 - accuracy: 0.0648 - val_loss: 6.8936 - val_accuracy: 0.0647\n",
            "Epoch 3/500\n",
            "73/73 [==============================] - 9s 124ms/step - loss: 6.7655 - accuracy: 0.0658 - val_loss: 6.8369 - val_accuracy: 0.0717\n",
            "Epoch 4/500\n",
            "73/73 [==============================] - 9s 117ms/step - loss: 6.6445 - accuracy: 0.0740 - val_loss: 6.7268 - val_accuracy: 0.0760\n",
            "Epoch 5/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 6.4829 - accuracy: 0.0820 - val_loss: 6.6161 - val_accuracy: 0.0859\n",
            "Epoch 6/500\n",
            "73/73 [==============================] - 8s 116ms/step - loss: 6.3351 - accuracy: 0.0948 - val_loss: 6.5429 - val_accuracy: 0.0955\n",
            "Epoch 7/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 6.2007 - accuracy: 0.1038 - val_loss: 6.4578 - val_accuracy: 0.1027\n",
            "Epoch 8/500\n",
            "73/73 [==============================] - 9s 117ms/step - loss: 6.0513 - accuracy: 0.1131 - val_loss: 6.3765 - val_accuracy: 0.1141\n",
            "Epoch 9/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 5.9108 - accuracy: 0.1248 - val_loss: 6.3172 - val_accuracy: 0.1195\n",
            "Epoch 10/500\n",
            "73/73 [==============================] - 9s 117ms/step - loss: 5.7958 - accuracy: 0.1312 - val_loss: 6.2740 - val_accuracy: 0.1236\n",
            "Epoch 11/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 5.6906 - accuracy: 0.1373 - val_loss: 6.2407 - val_accuracy: 0.1276\n",
            "Epoch 12/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 5.5904 - accuracy: 0.1417 - val_loss: 6.2235 - val_accuracy: 0.1308\n",
            "Epoch 13/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 5.4986 - accuracy: 0.1471 - val_loss: 6.2105 - val_accuracy: 0.1353\n",
            "Epoch 14/500\n",
            "73/73 [==============================] - 8s 116ms/step - loss: 5.4121 - accuracy: 0.1519 - val_loss: 6.2029 - val_accuracy: 0.1387\n",
            "Epoch 15/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 5.3303 - accuracy: 0.1568 - val_loss: 6.2132 - val_accuracy: 0.1418\n",
            "Epoch 16/500\n",
            "73/73 [==============================] - 9s 117ms/step - loss: 5.2522 - accuracy: 0.1611 - val_loss: 6.2234 - val_accuracy: 0.1439\n",
            "Epoch 17/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 5.1807 - accuracy: 0.1654 - val_loss: 6.2375 - val_accuracy: 0.1456\n",
            "Epoch 18/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 5.1117 - accuracy: 0.1696 - val_loss: 6.2531 - val_accuracy: 0.1484\n",
            "Epoch 19/500\n",
            "73/73 [==============================] - 9s 119ms/step - loss: 5.0464 - accuracy: 0.1740 - val_loss: 6.2853 - val_accuracy: 0.1508\n",
            "Epoch 20/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 4.9809 - accuracy: 0.1781 - val_loss: 6.3120 - val_accuracy: 0.1544\n",
            "Epoch 21/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 4.9188 - accuracy: 0.1821 - val_loss: 6.3351 - val_accuracy: 0.1562\n",
            "Epoch 22/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 4.8600 - accuracy: 0.1852 - val_loss: 6.3696 - val_accuracy: 0.1574\n",
            "Epoch 23/500\n",
            "73/73 [==============================] - 9s 119ms/step - loss: 4.7988 - accuracy: 0.1885 - val_loss: 6.4059 - val_accuracy: 0.1584\n",
            "Epoch 24/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 4.7436 - accuracy: 0.1920 - val_loss: 6.4358 - val_accuracy: 0.1599\n",
            "Epoch 25/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 4.6864 - accuracy: 0.1952 - val_loss: 6.4671 - val_accuracy: 0.1624\n",
            "Epoch 26/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 4.6302 - accuracy: 0.1993 - val_loss: 6.5256 - val_accuracy: 0.1629\n",
            "Epoch 27/500\n",
            "73/73 [==============================] - 9s 119ms/step - loss: 4.5740 - accuracy: 0.2033 - val_loss: 6.5544 - val_accuracy: 0.1630\n",
            "Epoch 28/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 4.5227 - accuracy: 0.2075 - val_loss: 6.5875 - val_accuracy: 0.1628\n",
            "Epoch 29/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 4.4703 - accuracy: 0.2110 - val_loss: 6.6272 - val_accuracy: 0.1650\n",
            "Epoch 30/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 4.4169 - accuracy: 0.2154 - val_loss: 6.6664 - val_accuracy: 0.1674\n",
            "Epoch 31/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 4.3668 - accuracy: 0.2189 - val_loss: 6.7277 - val_accuracy: 0.1679\n",
            "Epoch 32/500\n",
            "73/73 [==============================] - 8s 116ms/step - loss: 4.3162 - accuracy: 0.2233 - val_loss: 6.7558 - val_accuracy: 0.1679\n",
            "Epoch 33/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 4.2696 - accuracy: 0.2269 - val_loss: 6.7970 - val_accuracy: 0.1695\n",
            "Epoch 34/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 4.2201 - accuracy: 0.2309 - val_loss: 6.8504 - val_accuracy: 0.1692\n",
            "Epoch 35/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 4.1740 - accuracy: 0.2341 - val_loss: 6.8792 - val_accuracy: 0.1701\n",
            "Epoch 36/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 4.1247 - accuracy: 0.2393 - val_loss: 6.9782 - val_accuracy: 0.1700\n",
            "Epoch 37/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 4.0807 - accuracy: 0.2433 - val_loss: 7.0035 - val_accuracy: 0.1707\n",
            "Epoch 38/500\n",
            "73/73 [==============================] - 9s 118ms/step - loss: 4.0341 - accuracy: 0.2471 - val_loss: 7.0642 - val_accuracy: 0.1712\n",
            "Epoch 39/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 3.9902 - accuracy: 0.2511 - val_loss: 7.0925 - val_accuracy: 0.1711\n",
            "Epoch 40/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 3.9465 - accuracy: 0.2559 - val_loss: 7.1523 - val_accuracy: 0.1724\n",
            "Epoch 41/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 3.9003 - accuracy: 0.2596 - val_loss: 7.1990 - val_accuracy: 0.1737\n",
            "Epoch 42/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.8591 - accuracy: 0.2631 - val_loss: 7.2564 - val_accuracy: 0.1723\n",
            "Epoch 43/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.8147 - accuracy: 0.2682 - val_loss: 7.3205 - val_accuracy: 0.1723\n",
            "Epoch 44/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 3.7738 - accuracy: 0.2713 - val_loss: 7.3722 - val_accuracy: 0.1731\n",
            "Epoch 45/500\n",
            "73/73 [==============================] - 9s 123ms/step - loss: 3.7309 - accuracy: 0.2766 - val_loss: 7.4387 - val_accuracy: 0.1747\n",
            "Epoch 46/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.6942 - accuracy: 0.2808 - val_loss: 7.4803 - val_accuracy: 0.1741\n",
            "Epoch 47/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 3.6541 - accuracy: 0.2843 - val_loss: 7.5424 - val_accuracy: 0.1738\n",
            "Epoch 48/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.6158 - accuracy: 0.2894 - val_loss: 7.5760 - val_accuracy: 0.1725\n",
            "Epoch 49/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.5764 - accuracy: 0.2941 - val_loss: 7.6412 - val_accuracy: 0.1742\n",
            "Epoch 50/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 3.5374 - accuracy: 0.2981 - val_loss: 7.6892 - val_accuracy: 0.1736\n",
            "Epoch 51/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.5015 - accuracy: 0.3017 - val_loss: 7.7472 - val_accuracy: 0.1727\n",
            "Epoch 52/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.4674 - accuracy: 0.3058 - val_loss: 7.7948 - val_accuracy: 0.1729\n",
            "Epoch 53/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.4321 - accuracy: 0.3104 - val_loss: 7.8606 - val_accuracy: 0.1718\n",
            "Epoch 54/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.3970 - accuracy: 0.3138 - val_loss: 7.9203 - val_accuracy: 0.1730\n",
            "Epoch 55/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 3.3610 - accuracy: 0.3191 - val_loss: 8.0091 - val_accuracy: 0.1742\n",
            "Epoch 56/500\n",
            "73/73 [==============================] - 9s 123ms/step - loss: 3.3263 - accuracy: 0.3238 - val_loss: 8.0282 - val_accuracy: 0.1751\n",
            "Epoch 57/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.2962 - accuracy: 0.3276 - val_loss: 8.0955 - val_accuracy: 0.1742\n",
            "Epoch 58/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 3.2607 - accuracy: 0.3310 - val_loss: 8.1370 - val_accuracy: 0.1742\n",
            "Epoch 59/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 3.2335 - accuracy: 0.3341 - val_loss: 8.2228 - val_accuracy: 0.1754\n",
            "Epoch 60/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 3.1950 - accuracy: 0.3400 - val_loss: 8.2539 - val_accuracy: 0.1739\n",
            "Epoch 61/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.1658 - accuracy: 0.3455 - val_loss: 8.3458 - val_accuracy: 0.1726\n",
            "Epoch 62/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 3.1403 - accuracy: 0.3477 - val_loss: 8.3843 - val_accuracy: 0.1739\n",
            "Epoch 63/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.1033 - accuracy: 0.3522 - val_loss: 8.4856 - val_accuracy: 0.1742\n",
            "Epoch 64/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 3.0735 - accuracy: 0.3577 - val_loss: 8.5247 - val_accuracy: 0.1729\n",
            "Epoch 65/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.0480 - accuracy: 0.3611 - val_loss: 8.5620 - val_accuracy: 0.1732\n",
            "Epoch 66/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 3.0155 - accuracy: 0.3649 - val_loss: 8.6496 - val_accuracy: 0.1740\n",
            "Epoch 67/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.9874 - accuracy: 0.3691 - val_loss: 8.6674 - val_accuracy: 0.1718\n",
            "Epoch 68/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.9538 - accuracy: 0.3736 - val_loss: 8.7787 - val_accuracy: 0.1745\n",
            "Epoch 69/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.9288 - accuracy: 0.3768 - val_loss: 8.8506 - val_accuracy: 0.1752\n",
            "Epoch 70/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.9040 - accuracy: 0.3809 - val_loss: 8.8487 - val_accuracy: 0.1744\n",
            "Epoch 71/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 2.8759 - accuracy: 0.3861 - val_loss: 8.9484 - val_accuracy: 0.1761\n",
            "Epoch 72/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.8455 - accuracy: 0.3883 - val_loss: 9.0101 - val_accuracy: 0.1742\n",
            "Epoch 73/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.8215 - accuracy: 0.3925 - val_loss: 9.0783 - val_accuracy: 0.1754\n",
            "Epoch 74/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.7941 - accuracy: 0.3972 - val_loss: 9.1193 - val_accuracy: 0.1755\n",
            "Epoch 75/500\n",
            "73/73 [==============================] - 9s 124ms/step - loss: 2.7669 - accuracy: 0.4010 - val_loss: 9.1840 - val_accuracy: 0.1777\n",
            "Epoch 76/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.7386 - accuracy: 0.4040 - val_loss: 9.2560 - val_accuracy: 0.1747\n",
            "Epoch 77/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.7136 - accuracy: 0.4093 - val_loss: 9.3385 - val_accuracy: 0.1735\n",
            "Epoch 78/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 2.6891 - accuracy: 0.4115 - val_loss: 9.4021 - val_accuracy: 0.1742\n",
            "Epoch 79/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 2.6610 - accuracy: 0.4180 - val_loss: 9.4828 - val_accuracy: 0.1757\n",
            "Epoch 80/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.6387 - accuracy: 0.4209 - val_loss: 9.5061 - val_accuracy: 0.1740\n",
            "Epoch 81/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.6162 - accuracy: 0.4235 - val_loss: 9.5772 - val_accuracy: 0.1751\n",
            "Epoch 82/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.5941 - accuracy: 0.4276 - val_loss: 9.6499 - val_accuracy: 0.1752\n",
            "Epoch 83/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.5669 - accuracy: 0.4307 - val_loss: 9.7342 - val_accuracy: 0.1740\n",
            "Epoch 84/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 2.5466 - accuracy: 0.4340 - val_loss: 9.7898 - val_accuracy: 0.1749\n",
            "Epoch 85/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 2.5215 - accuracy: 0.4394 - val_loss: 9.8891 - val_accuracy: 0.1780\n",
            "Epoch 86/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.4982 - accuracy: 0.4417 - val_loss: 9.9286 - val_accuracy: 0.1773\n",
            "Epoch 87/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.4756 - accuracy: 0.4470 - val_loss: 9.9686 - val_accuracy: 0.1725\n",
            "Epoch 88/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.4523 - accuracy: 0.4499 - val_loss: 9.9967 - val_accuracy: 0.1740\n",
            "Epoch 89/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.4310 - accuracy: 0.4540 - val_loss: 10.1599 - val_accuracy: 0.1767\n",
            "Epoch 90/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.4103 - accuracy: 0.4565 - val_loss: 10.1400 - val_accuracy: 0.1738\n",
            "Epoch 91/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 2.3831 - accuracy: 0.4618 - val_loss: 10.2608 - val_accuracy: 0.1751\n",
            "Epoch 92/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.3672 - accuracy: 0.4648 - val_loss: 10.3236 - val_accuracy: 0.1758\n",
            "Epoch 93/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.3423 - accuracy: 0.4690 - val_loss: 10.3763 - val_accuracy: 0.1743\n",
            "Epoch 94/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.3220 - accuracy: 0.4719 - val_loss: 10.4401 - val_accuracy: 0.1762\n",
            "Epoch 95/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.2987 - accuracy: 0.4763 - val_loss: 10.5411 - val_accuracy: 0.1777\n",
            "Epoch 96/500\n",
            "73/73 [==============================] - 9s 124ms/step - loss: 2.2804 - accuracy: 0.4793 - val_loss: 10.6055 - val_accuracy: 0.1783\n",
            "Epoch 97/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.2545 - accuracy: 0.4841 - val_loss: 10.6364 - val_accuracy: 0.1778\n",
            "Epoch 98/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.2338 - accuracy: 0.4883 - val_loss: 10.7320 - val_accuracy: 0.1768\n",
            "Epoch 99/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.2208 - accuracy: 0.4893 - val_loss: 10.7691 - val_accuracy: 0.1762\n",
            "Epoch 100/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.1948 - accuracy: 0.4936 - val_loss: 10.8207 - val_accuracy: 0.1768\n",
            "Epoch 101/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.1776 - accuracy: 0.4974 - val_loss: 10.9290 - val_accuracy: 0.1778\n",
            "Epoch 102/500\n",
            "73/73 [==============================] - 9s 123ms/step - loss: 2.1581 - accuracy: 0.5001 - val_loss: 10.9820 - val_accuracy: 0.1790\n",
            "Epoch 103/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 2.1353 - accuracy: 0.5052 - val_loss: 11.0998 - val_accuracy: 0.1782\n",
            "Epoch 104/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 2.1187 - accuracy: 0.5073 - val_loss: 11.0839 - val_accuracy: 0.1761\n",
            "Epoch 105/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.0925 - accuracy: 0.5144 - val_loss: 11.1921 - val_accuracy: 0.1775\n",
            "Epoch 106/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.0791 - accuracy: 0.5160 - val_loss: 11.2098 - val_accuracy: 0.1764\n",
            "Epoch 107/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 2.0573 - accuracy: 0.5182 - val_loss: 11.3305 - val_accuracy: 0.1758\n",
            "Epoch 108/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 2.0412 - accuracy: 0.5212 - val_loss: 11.3705 - val_accuracy: 0.1774\n",
            "Epoch 109/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.0230 - accuracy: 0.5257 - val_loss: 11.4851 - val_accuracy: 0.1776\n",
            "Epoch 110/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 2.0031 - accuracy: 0.5288 - val_loss: 11.4940 - val_accuracy: 0.1770\n",
            "Epoch 111/500\n",
            "73/73 [==============================] - 9s 125ms/step - loss: 1.9833 - accuracy: 0.5338 - val_loss: 11.6089 - val_accuracy: 0.1791\n",
            "Epoch 112/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.9651 - accuracy: 0.5359 - val_loss: 11.6537 - val_accuracy: 0.1756\n",
            "Epoch 113/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.9454 - accuracy: 0.5397 - val_loss: 11.7663 - val_accuracy: 0.1762\n",
            "Epoch 114/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.9324 - accuracy: 0.5425 - val_loss: 11.8014 - val_accuracy: 0.1774\n",
            "Epoch 115/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.9162 - accuracy: 0.5461 - val_loss: 11.9282 - val_accuracy: 0.1769\n",
            "Epoch 116/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 1.9015 - accuracy: 0.5468 - val_loss: 11.9410 - val_accuracy: 0.1802\n",
            "Epoch 117/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.8816 - accuracy: 0.5505 - val_loss: 11.9911 - val_accuracy: 0.1770\n",
            "Epoch 118/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.8614 - accuracy: 0.5563 - val_loss: 12.0822 - val_accuracy: 0.1793\n",
            "Epoch 119/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.8445 - accuracy: 0.5587 - val_loss: 12.1506 - val_accuracy: 0.1782\n",
            "Epoch 120/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.8238 - accuracy: 0.5624 - val_loss: 12.2535 - val_accuracy: 0.1786\n",
            "Epoch 121/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.8055 - accuracy: 0.5659 - val_loss: 12.3280 - val_accuracy: 0.1786\n",
            "Epoch 122/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.7907 - accuracy: 0.5679 - val_loss: 12.3402 - val_accuracy: 0.1762\n",
            "Epoch 123/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.7765 - accuracy: 0.5714 - val_loss: 12.4760 - val_accuracy: 0.1786\n",
            "Epoch 124/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.7617 - accuracy: 0.5735 - val_loss: 12.5056 - val_accuracy: 0.1796\n",
            "Epoch 125/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 1.7459 - accuracy: 0.5782 - val_loss: 12.5467 - val_accuracy: 0.1809\n",
            "Epoch 126/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.7260 - accuracy: 0.5816 - val_loss: 12.6581 - val_accuracy: 0.1800\n",
            "Epoch 127/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.7130 - accuracy: 0.5842 - val_loss: 12.7209 - val_accuracy: 0.1806\n",
            "Epoch 128/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.6921 - accuracy: 0.5888 - val_loss: 12.7784 - val_accuracy: 0.1792\n",
            "Epoch 129/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.6784 - accuracy: 0.5909 - val_loss: 12.8038 - val_accuracy: 0.1797\n",
            "Epoch 130/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 1.6659 - accuracy: 0.5923 - val_loss: 12.9115 - val_accuracy: 0.1810\n",
            "Epoch 131/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.6498 - accuracy: 0.5965 - val_loss: 12.9493 - val_accuracy: 0.1784\n",
            "Epoch 132/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 1.6337 - accuracy: 0.5997 - val_loss: 13.0200 - val_accuracy: 0.1815\n",
            "Epoch 133/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.6198 - accuracy: 0.6014 - val_loss: 13.1447 - val_accuracy: 0.1814\n",
            "Epoch 134/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.6065 - accuracy: 0.6057 - val_loss: 13.1206 - val_accuracy: 0.1806\n",
            "Epoch 135/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.5907 - accuracy: 0.6081 - val_loss: 13.1819 - val_accuracy: 0.1782\n",
            "Epoch 136/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 1.5739 - accuracy: 0.6112 - val_loss: 13.2910 - val_accuracy: 0.1822\n",
            "Epoch 137/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.5586 - accuracy: 0.6147 - val_loss: 13.3492 - val_accuracy: 0.1808\n",
            "Epoch 138/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.5410 - accuracy: 0.6188 - val_loss: 13.4933 - val_accuracy: 0.1821\n",
            "Epoch 139/500\n",
            "73/73 [==============================] - 9s 123ms/step - loss: 1.5302 - accuracy: 0.6195 - val_loss: 13.5801 - val_accuracy: 0.1835\n",
            "Epoch 140/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.5174 - accuracy: 0.6224 - val_loss: 13.6351 - val_accuracy: 0.1804\n",
            "Epoch 141/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.5077 - accuracy: 0.6250 - val_loss: 13.6887 - val_accuracy: 0.1826\n",
            "Epoch 142/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 1.4921 - accuracy: 0.6284 - val_loss: 13.7255 - val_accuracy: 0.1837\n",
            "Epoch 143/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.4755 - accuracy: 0.6308 - val_loss: 13.8374 - val_accuracy: 0.1828\n",
            "Epoch 144/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 1.4596 - accuracy: 0.6332 - val_loss: 13.8790 - val_accuracy: 0.1848\n",
            "Epoch 145/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.4473 - accuracy: 0.6374 - val_loss: 13.8880 - val_accuracy: 0.1835\n",
            "Epoch 146/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.4293 - accuracy: 0.6412 - val_loss: 14.0806 - val_accuracy: 0.1828\n",
            "Epoch 147/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.4220 - accuracy: 0.6426 - val_loss: 14.0500 - val_accuracy: 0.1820\n",
            "Epoch 148/500\n",
            "73/73 [==============================] - 9s 123ms/step - loss: 1.4083 - accuracy: 0.6442 - val_loss: 14.1569 - val_accuracy: 0.1851\n",
            "Epoch 149/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.3953 - accuracy: 0.6471 - val_loss: 14.1827 - val_accuracy: 0.1838\n",
            "Epoch 150/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.3798 - accuracy: 0.6500 - val_loss: 14.2692 - val_accuracy: 0.1848\n",
            "Epoch 151/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.3680 - accuracy: 0.6535 - val_loss: 14.3831 - val_accuracy: 0.1833\n",
            "Epoch 152/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.3566 - accuracy: 0.6566 - val_loss: 14.3932 - val_accuracy: 0.1822\n",
            "Epoch 153/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 1.3449 - accuracy: 0.6591 - val_loss: 14.5358 - val_accuracy: 0.1861\n",
            "Epoch 154/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.3321 - accuracy: 0.6611 - val_loss: 14.5344 - val_accuracy: 0.1833\n",
            "Epoch 155/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.3239 - accuracy: 0.6638 - val_loss: 14.6892 - val_accuracy: 0.1835\n",
            "Epoch 156/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.3173 - accuracy: 0.6629 - val_loss: 14.6746 - val_accuracy: 0.1855\n",
            "Epoch 157/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.2944 - accuracy: 0.6692 - val_loss: 14.7177 - val_accuracy: 0.1848\n",
            "Epoch 158/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.2854 - accuracy: 0.6711 - val_loss: 14.8869 - val_accuracy: 0.1847\n",
            "Epoch 159/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.2764 - accuracy: 0.6730 - val_loss: 14.8616 - val_accuracy: 0.1861\n",
            "Epoch 160/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.2605 - accuracy: 0.6769 - val_loss: 14.9346 - val_accuracy: 0.1860\n",
            "Epoch 161/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.2535 - accuracy: 0.6791 - val_loss: 15.0491 - val_accuracy: 0.1838\n",
            "Epoch 162/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.2426 - accuracy: 0.6803 - val_loss: 15.1370 - val_accuracy: 0.1858\n",
            "Epoch 163/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.2246 - accuracy: 0.6853 - val_loss: 15.1893 - val_accuracy: 0.1851\n",
            "Epoch 164/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 1.2201 - accuracy: 0.6865 - val_loss: 15.2782 - val_accuracy: 0.1882\n",
            "Epoch 165/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.2068 - accuracy: 0.6879 - val_loss: 15.3393 - val_accuracy: 0.1876\n",
            "Epoch 166/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.1952 - accuracy: 0.6907 - val_loss: 15.3982 - val_accuracy: 0.1870\n",
            "Epoch 167/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.1827 - accuracy: 0.6932 - val_loss: 15.3983 - val_accuracy: 0.1855\n",
            "Epoch 168/500\n",
            "73/73 [==============================] - 9s 125ms/step - loss: 1.1747 - accuracy: 0.6959 - val_loss: 15.4971 - val_accuracy: 0.1896\n",
            "Epoch 169/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.1679 - accuracy: 0.6962 - val_loss: 15.5388 - val_accuracy: 0.1850\n",
            "Epoch 170/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.1554 - accuracy: 0.7001 - val_loss: 15.6223 - val_accuracy: 0.1867\n",
            "Epoch 171/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.1459 - accuracy: 0.7020 - val_loss: 15.6884 - val_accuracy: 0.1855\n",
            "Epoch 172/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.1323 - accuracy: 0.7045 - val_loss: 15.8191 - val_accuracy: 0.1875\n",
            "Epoch 173/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.1267 - accuracy: 0.7073 - val_loss: 15.8345 - val_accuracy: 0.1858\n",
            "Epoch 174/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.1116 - accuracy: 0.7089 - val_loss: 15.8772 - val_accuracy: 0.1875\n",
            "Epoch 175/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.1022 - accuracy: 0.7112 - val_loss: 15.9897 - val_accuracy: 0.1895\n",
            "Epoch 176/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.0926 - accuracy: 0.7134 - val_loss: 16.0665 - val_accuracy: 0.1889\n",
            "Epoch 177/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.0876 - accuracy: 0.7149 - val_loss: 16.1217 - val_accuracy: 0.1880\n",
            "Epoch 178/500\n",
            "73/73 [==============================] - 9s 123ms/step - loss: 1.0761 - accuracy: 0.7173 - val_loss: 16.1083 - val_accuracy: 0.1897\n",
            "Epoch 179/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.0673 - accuracy: 0.7195 - val_loss: 16.2779 - val_accuracy: 0.1889\n",
            "Epoch 180/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.0543 - accuracy: 0.7218 - val_loss: 16.3626 - val_accuracy: 0.1889\n",
            "Epoch 181/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.0476 - accuracy: 0.7231 - val_loss: 16.3991 - val_accuracy: 0.1897\n",
            "Epoch 182/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.0392 - accuracy: 0.7255 - val_loss: 16.4545 - val_accuracy: 0.1887\n",
            "Epoch 183/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 1.0273 - accuracy: 0.7266 - val_loss: 16.5424 - val_accuracy: 0.1877\n",
            "Epoch 184/500\n",
            "73/73 [==============================] - 9s 125ms/step - loss: 1.0205 - accuracy: 0.7302 - val_loss: 16.5755 - val_accuracy: 0.1905\n",
            "Epoch 185/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 1.0081 - accuracy: 0.7318 - val_loss: 16.5687 - val_accuracy: 0.1893\n",
            "Epoch 186/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 1.0011 - accuracy: 0.7336 - val_loss: 16.7515 - val_accuracy: 0.1901\n",
            "Epoch 187/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.9944 - accuracy: 0.7347 - val_loss: 16.7606 - val_accuracy: 0.1884\n",
            "Epoch 188/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.9847 - accuracy: 0.7378 - val_loss: 16.8428 - val_accuracy: 0.1901\n",
            "Epoch 189/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 0.9746 - accuracy: 0.7398 - val_loss: 16.8477 - val_accuracy: 0.1907\n",
            "Epoch 190/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.9638 - accuracy: 0.7428 - val_loss: 16.9216 - val_accuracy: 0.1898\n",
            "Epoch 191/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.9538 - accuracy: 0.7440 - val_loss: 16.9906 - val_accuracy: 0.1900\n",
            "Epoch 192/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.9480 - accuracy: 0.7465 - val_loss: 17.0172 - val_accuracy: 0.1891\n",
            "Epoch 193/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 0.9441 - accuracy: 0.7478 - val_loss: 17.1377 - val_accuracy: 0.1907\n",
            "Epoch 194/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 0.9372 - accuracy: 0.7481 - val_loss: 17.2126 - val_accuracy: 0.1912\n",
            "Epoch 195/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.9273 - accuracy: 0.7507 - val_loss: 17.3420 - val_accuracy: 0.1902\n",
            "Epoch 196/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.9208 - accuracy: 0.7520 - val_loss: 17.3198 - val_accuracy: 0.1900\n",
            "Epoch 197/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.9066 - accuracy: 0.7568 - val_loss: 17.3537 - val_accuracy: 0.1902\n",
            "Epoch 198/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 0.9005 - accuracy: 0.7576 - val_loss: 17.5265 - val_accuracy: 0.1918\n",
            "Epoch 199/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.8962 - accuracy: 0.7581 - val_loss: 17.5444 - val_accuracy: 0.1896\n",
            "Epoch 200/500\n",
            "73/73 [==============================] - 11s 154ms/step - loss: 0.8896 - accuracy: 0.7589 - val_loss: 17.6291 - val_accuracy: 0.1919\n",
            "Epoch 201/500\n",
            "73/73 [==============================] - 8s 116ms/step - loss: 0.8758 - accuracy: 0.7636 - val_loss: 17.6870 - val_accuracy: 0.1928\n",
            "Epoch 202/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.8692 - accuracy: 0.7652 - val_loss: 17.6226 - val_accuracy: 0.1916\n",
            "Epoch 203/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.8649 - accuracy: 0.7650 - val_loss: 17.8913 - val_accuracy: 0.1918\n",
            "Epoch 204/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 0.8600 - accuracy: 0.7663 - val_loss: 17.8585 - val_accuracy: 0.1935\n",
            "Epoch 205/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.8552 - accuracy: 0.7662 - val_loss: 17.8817 - val_accuracy: 0.1907\n",
            "Epoch 206/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.8444 - accuracy: 0.7697 - val_loss: 17.9739 - val_accuracy: 0.1912\n",
            "Epoch 207/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.8316 - accuracy: 0.7726 - val_loss: 18.0286 - val_accuracy: 0.1916\n",
            "Epoch 208/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.8263 - accuracy: 0.7741 - val_loss: 18.1370 - val_accuracy: 0.1925\n",
            "Epoch 209/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.8275 - accuracy: 0.7744 - val_loss: 18.1692 - val_accuracy: 0.1920\n",
            "Epoch 210/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.8191 - accuracy: 0.7763 - val_loss: 18.2475 - val_accuracy: 0.1927\n",
            "Epoch 211/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.8073 - accuracy: 0.7793 - val_loss: 18.2879 - val_accuracy: 0.1912\n",
            "Epoch 212/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.8024 - accuracy: 0.7805 - val_loss: 18.2882 - val_accuracy: 0.1931\n",
            "Epoch 213/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.8027 - accuracy: 0.7795 - val_loss: 18.3824 - val_accuracy: 0.1913\n",
            "Epoch 214/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 0.7982 - accuracy: 0.7806 - val_loss: 18.3309 - val_accuracy: 0.1915\n",
            "Epoch 215/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.7876 - accuracy: 0.7833 - val_loss: 18.5674 - val_accuracy: 0.1934\n",
            "Epoch 216/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.7784 - accuracy: 0.7848 - val_loss: 18.6814 - val_accuracy: 0.1933\n",
            "Epoch 217/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.7705 - accuracy: 0.7865 - val_loss: 18.6963 - val_accuracy: 0.1934\n",
            "Epoch 218/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 0.7677 - accuracy: 0.7875 - val_loss: 18.6422 - val_accuracy: 0.1931\n",
            "Epoch 219/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 0.7582 - accuracy: 0.7915 - val_loss: 18.8161 - val_accuracy: 0.1938\n",
            "Epoch 220/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.7526 - accuracy: 0.7922 - val_loss: 18.8126 - val_accuracy: 0.1925\n",
            "Epoch 221/500\n",
            "73/73 [==============================] - 10s 138ms/step - loss: 0.7444 - accuracy: 0.7940 - val_loss: 18.9121 - val_accuracy: 0.1942\n",
            "Epoch 222/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.7402 - accuracy: 0.7955 - val_loss: 18.9570 - val_accuracy: 0.1933\n",
            "Epoch 223/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.7339 - accuracy: 0.7966 - val_loss: 18.9402 - val_accuracy: 0.1926\n",
            "Epoch 224/500\n",
            "73/73 [==============================] - 9s 122ms/step - loss: 0.7263 - accuracy: 0.7995 - val_loss: 19.0161 - val_accuracy: 0.1946\n",
            "Epoch 225/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.7213 - accuracy: 0.7994 - val_loss: 19.1180 - val_accuracy: 0.1927\n",
            "Epoch 226/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 0.7188 - accuracy: 0.8006 - val_loss: 19.1693 - val_accuracy: 0.1948\n",
            "Epoch 227/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.7100 - accuracy: 0.8030 - val_loss: 19.1964 - val_accuracy: 0.1946\n",
            "Epoch 228/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.7024 - accuracy: 0.8043 - val_loss: 19.2141 - val_accuracy: 0.1926\n",
            "Epoch 229/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.7052 - accuracy: 0.8037 - val_loss: 19.3174 - val_accuracy: 0.1938\n",
            "Epoch 230/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.6979 - accuracy: 0.8048 - val_loss: 19.3361 - val_accuracy: 0.1934\n",
            "Epoch 231/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 0.6901 - accuracy: 0.8071 - val_loss: 19.4299 - val_accuracy: 0.1930\n",
            "Epoch 232/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6869 - accuracy: 0.8082 - val_loss: 19.4960 - val_accuracy: 0.1944\n",
            "Epoch 233/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 0.6799 - accuracy: 0.8094 - val_loss: 19.6303 - val_accuracy: 0.1956\n",
            "Epoch 234/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6712 - accuracy: 0.8125 - val_loss: 19.5804 - val_accuracy: 0.1952\n",
            "Epoch 235/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6697 - accuracy: 0.8126 - val_loss: 19.5689 - val_accuracy: 0.1937\n",
            "Epoch 236/500\n",
            "73/73 [==============================] - 9s 120ms/step - loss: 0.6611 - accuracy: 0.8148 - val_loss: 19.7743 - val_accuracy: 0.1957\n",
            "Epoch 237/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.6618 - accuracy: 0.8139 - val_loss: 19.8650 - val_accuracy: 0.1937\n",
            "Epoch 238/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 0.6523 - accuracy: 0.8166 - val_loss: 19.9330 - val_accuracy: 0.1969\n",
            "Epoch 239/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6489 - accuracy: 0.8175 - val_loss: 19.8449 - val_accuracy: 0.1941\n",
            "Epoch 240/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6447 - accuracy: 0.8188 - val_loss: 20.0646 - val_accuracy: 0.1945\n",
            "Epoch 241/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6417 - accuracy: 0.8186 - val_loss: 20.0723 - val_accuracy: 0.1942\n",
            "Epoch 242/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6384 - accuracy: 0.8197 - val_loss: 20.1079 - val_accuracy: 0.1943\n",
            "Epoch 243/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6368 - accuracy: 0.8205 - val_loss: 20.0667 - val_accuracy: 0.1953\n",
            "Epoch 244/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.6282 - accuracy: 0.8225 - val_loss: 20.1635 - val_accuracy: 0.1944\n",
            "Epoch 245/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6242 - accuracy: 0.8235 - val_loss: 20.1779 - val_accuracy: 0.1954\n",
            "Epoch 246/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.6155 - accuracy: 0.8262 - val_loss: 20.3108 - val_accuracy: 0.1944\n",
            "Epoch 247/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.6118 - accuracy: 0.8264 - val_loss: 20.2136 - val_accuracy: 0.1944\n",
            "Epoch 248/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.6097 - accuracy: 0.8268 - val_loss: 20.3369 - val_accuracy: 0.1953\n",
            "Epoch 249/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.6009 - accuracy: 0.8292 - val_loss: 20.4475 - val_accuracy: 0.1960\n",
            "Epoch 250/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5984 - accuracy: 0.8302 - val_loss: 20.3981 - val_accuracy: 0.1946\n",
            "Epoch 251/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5919 - accuracy: 0.8322 - val_loss: 20.5336 - val_accuracy: 0.1947\n",
            "Epoch 252/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5889 - accuracy: 0.8315 - val_loss: 20.5247 - val_accuracy: 0.1963\n",
            "Epoch 253/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5867 - accuracy: 0.8330 - val_loss: 20.5885 - val_accuracy: 0.1941\n",
            "Epoch 254/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 0.5818 - accuracy: 0.8336 - val_loss: 20.7845 - val_accuracy: 0.1975\n",
            "Epoch 255/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.5768 - accuracy: 0.8355 - val_loss: 20.6639 - val_accuracy: 0.1942\n",
            "Epoch 256/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5765 - accuracy: 0.8343 - val_loss: 20.8422 - val_accuracy: 0.1965\n",
            "Epoch 257/500\n",
            "73/73 [==============================] - 8s 116ms/step - loss: 0.5721 - accuracy: 0.8369 - val_loss: 20.8509 - val_accuracy: 0.1965\n",
            "Epoch 258/500\n",
            "73/73 [==============================] - 8s 115ms/step - loss: 0.5680 - accuracy: 0.8366 - val_loss: 20.9992 - val_accuracy: 0.1963\n",
            "Epoch 259/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5667 - accuracy: 0.8375 - val_loss: 20.8391 - val_accuracy: 0.1952\n",
            "Epoch 260/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5623 - accuracy: 0.8386 - val_loss: 20.9830 - val_accuracy: 0.1949\n",
            "Epoch 261/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5541 - accuracy: 0.8405 - val_loss: 21.0995 - val_accuracy: 0.1972\n",
            "Epoch 262/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 0.5463 - accuracy: 0.8430 - val_loss: 21.0570 - val_accuracy: 0.1969\n",
            "Epoch 263/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5529 - accuracy: 0.8413 - val_loss: 21.1888 - val_accuracy: 0.1963\n",
            "Epoch 264/500\n",
            "73/73 [==============================] - 9s 121ms/step - loss: 0.5375 - accuracy: 0.8455 - val_loss: 21.2085 - val_accuracy: 0.1976\n",
            "Epoch 265/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5361 - accuracy: 0.8461 - val_loss: 21.1930 - val_accuracy: 0.1936\n",
            "Epoch 266/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.5410 - accuracy: 0.8446 - val_loss: 21.2889 - val_accuracy: 0.1958\n",
            "Epoch 267/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.5349 - accuracy: 0.8461 - val_loss: 21.3811 - val_accuracy: 0.1959\n",
            "Epoch 268/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5282 - accuracy: 0.8471 - val_loss: 21.3672 - val_accuracy: 0.1957\n",
            "Epoch 269/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5210 - accuracy: 0.8500 - val_loss: 21.4220 - val_accuracy: 0.1952\n",
            "Epoch 270/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 0.5241 - accuracy: 0.8487 - val_loss: 21.6032 - val_accuracy: 0.1962\n",
            "Epoch 271/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5200 - accuracy: 0.8501 - val_loss: 21.6382 - val_accuracy: 0.1958\n",
            "Epoch 272/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5144 - accuracy: 0.8516 - val_loss: 21.5664 - val_accuracy: 0.1948\n",
            "Epoch 273/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.5104 - accuracy: 0.8527 - val_loss: 21.7551 - val_accuracy: 0.1949\n",
            "Epoch 274/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.5079 - accuracy: 0.8528 - val_loss: 21.6510 - val_accuracy: 0.1974\n",
            "Epoch 275/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 0.5071 - accuracy: 0.8530 - val_loss: 21.7180 - val_accuracy: 0.1962\n",
            "Epoch 276/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 0.5041 - accuracy: 0.8536 - val_loss: 21.6819 - val_accuracy: 0.1968\n",
            "Epoch 277/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.4975 - accuracy: 0.8561 - val_loss: 21.6520 - val_accuracy: 0.1953\n",
            "Epoch 278/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.4961 - accuracy: 0.8566 - val_loss: 21.7450 - val_accuracy: 0.1943\n",
            "Epoch 279/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.4951 - accuracy: 0.8555 - val_loss: 21.9115 - val_accuracy: 0.1946\n",
            "Epoch 280/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 0.4882 - accuracy: 0.8570 - val_loss: 21.9403 - val_accuracy: 0.1967\n",
            "Epoch 281/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.4842 - accuracy: 0.8584 - val_loss: 21.9562 - val_accuracy: 0.1963\n",
            "Epoch 282/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.4847 - accuracy: 0.8586 - val_loss: 22.0740 - val_accuracy: 0.1970\n",
            "Epoch 283/500\n",
            "73/73 [==============================] - 8s 112ms/step - loss: 0.4785 - accuracy: 0.8609 - val_loss: 22.0663 - val_accuracy: 0.1962\n",
            "Epoch 284/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.4788 - accuracy: 0.8602 - val_loss: 22.0766 - val_accuracy: 0.1967\n",
            "Epoch 285/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.4783 - accuracy: 0.8604 - val_loss: 22.0540 - val_accuracy: 0.1950\n",
            "Epoch 286/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.4710 - accuracy: 0.8616 - val_loss: 22.1581 - val_accuracy: 0.1956\n",
            "Epoch 287/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.4701 - accuracy: 0.8630 - val_loss: 22.4308 - val_accuracy: 0.1957\n",
            "Epoch 288/500\n",
            "73/73 [==============================] - 8s 114ms/step - loss: 0.4688 - accuracy: 0.8639 - val_loss: 22.1809 - val_accuracy: 0.1959\n",
            "Epoch 289/500\n",
            "73/73 [==============================] - 8s 113ms/step - loss: 0.4632 - accuracy: 0.8645 - val_loss: 22.4565 - val_accuracy: 0.1963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66ffdb8240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoP-_Ot7AO4Z",
        "colab_type": "text"
      },
      "source": [
        "## V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK14bGSjeOwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZeVf9SAnCmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-3.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_8icpaa_4IP",
        "colab_type": "text"
      },
      "source": [
        "## V4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDo87F7AnUdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5c143a1a-05d5-4f4d-d9b8-1fa9eff074be"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(64))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaybHfIeq1ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-4.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y35orGI_7Mh",
        "colab_type": "text"
      },
      "source": [
        "## V5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNmL-cAkspbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "54a237cc-79e1-42c8-8043-34ed59bb3882"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7xulino73AY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbd4dd1b-6e87-4165-df29-191a4efee8f5"
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-5.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 7.3653 - accuracy: 0.0622 - val_loss: 6.9710 - val_accuracy: 0.0647\n",
            "Epoch 2/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 6.9159 - accuracy: 0.0648 - val_loss: 7.0083 - val_accuracy: 0.0647\n",
            "Epoch 3/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 6.9021 - accuracy: 0.0648 - val_loss: 7.0113 - val_accuracy: 0.0647\n",
            "Epoch 4/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 6.8891 - accuracy: 0.0648 - val_loss: 6.9736 - val_accuracy: 0.0647\n",
            "Epoch 5/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 6.7784 - accuracy: 0.0661 - val_loss: 6.8245 - val_accuracy: 0.0731\n",
            "Epoch 6/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 6.6285 - accuracy: 0.0735 - val_loss: 6.7089 - val_accuracy: 0.0756\n",
            "Epoch 7/500\n",
            "73/73 [==============================] - 17s 228ms/step - loss: 6.4880 - accuracy: 0.0827 - val_loss: 6.6267 - val_accuracy: 0.0867\n",
            "Epoch 8/500\n",
            "73/73 [==============================] - 17s 229ms/step - loss: 6.3800 - accuracy: 0.0908 - val_loss: 6.5682 - val_accuracy: 0.0921\n",
            "Epoch 9/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 6.2878 - accuracy: 0.0952 - val_loss: 6.5289 - val_accuracy: 0.0936\n",
            "Epoch 10/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 6.2043 - accuracy: 0.0992 - val_loss: 6.4889 - val_accuracy: 0.0971\n",
            "Epoch 11/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 6.1181 - accuracy: 0.1070 - val_loss: 6.4401 - val_accuracy: 0.1061\n",
            "Epoch 12/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 6.0349 - accuracy: 0.1131 - val_loss: 6.4107 - val_accuracy: 0.1091\n",
            "Epoch 13/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 5.9584 - accuracy: 0.1180 - val_loss: 6.3894 - val_accuracy: 0.1136\n",
            "Epoch 14/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 5.8926 - accuracy: 0.1227 - val_loss: 6.3702 - val_accuracy: 0.1161\n",
            "Epoch 15/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 5.8305 - accuracy: 0.1254 - val_loss: 6.3562 - val_accuracy: 0.1191\n",
            "Epoch 16/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 5.7755 - accuracy: 0.1279 - val_loss: 6.3414 - val_accuracy: 0.1207\n",
            "Epoch 17/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 5.7233 - accuracy: 0.1307 - val_loss: 6.3313 - val_accuracy: 0.1232\n",
            "Epoch 18/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 5.6729 - accuracy: 0.1332 - val_loss: 6.3247 - val_accuracy: 0.1241\n",
            "Epoch 19/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 5.6233 - accuracy: 0.1349 - val_loss: 6.3149 - val_accuracy: 0.1254\n",
            "Epoch 20/500\n",
            "73/73 [==============================] - 17s 239ms/step - loss: 5.5756 - accuracy: 0.1363 - val_loss: 6.3000 - val_accuracy: 0.1270\n",
            "Epoch 21/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 5.5283 - accuracy: 0.1389 - val_loss: 6.2942 - val_accuracy: 0.1279\n",
            "Epoch 22/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 5.4836 - accuracy: 0.1399 - val_loss: 6.2918 - val_accuracy: 0.1290\n",
            "Epoch 23/500\n",
            "73/73 [==============================] - 17s 229ms/step - loss: 5.4421 - accuracy: 0.1410 - val_loss: 6.2972 - val_accuracy: 0.1301\n",
            "Epoch 24/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 5.4024 - accuracy: 0.1424 - val_loss: 6.2956 - val_accuracy: 0.1325\n",
            "Epoch 25/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 5.3603 - accuracy: 0.1438 - val_loss: 6.3052 - val_accuracy: 0.1324\n",
            "Epoch 26/500\n",
            "73/73 [==============================] - 17s 228ms/step - loss: 5.3231 - accuracy: 0.1452 - val_loss: 6.3041 - val_accuracy: 0.1353\n",
            "Epoch 27/500\n",
            "73/73 [==============================] - 25s 337ms/step - loss: 5.2869 - accuracy: 0.1477 - val_loss: 6.3112 - val_accuracy: 0.1365\n",
            "Epoch 28/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 5.2474 - accuracy: 0.1490 - val_loss: 6.3118 - val_accuracy: 0.1376\n",
            "Epoch 29/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 5.2112 - accuracy: 0.1503 - val_loss: 6.3256 - val_accuracy: 0.1376\n",
            "Epoch 30/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 5.1763 - accuracy: 0.1525 - val_loss: 6.3309 - val_accuracy: 0.1390\n",
            "Epoch 31/500\n",
            "73/73 [==============================] - 17s 229ms/step - loss: 5.1398 - accuracy: 0.1542 - val_loss: 6.3428 - val_accuracy: 0.1415\n",
            "Epoch 32/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 5.1089 - accuracy: 0.1554 - val_loss: 6.3589 - val_accuracy: 0.1402\n",
            "Epoch 33/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 5.0763 - accuracy: 0.1569 - val_loss: 6.3603 - val_accuracy: 0.1432\n",
            "Epoch 34/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 5.0411 - accuracy: 0.1597 - val_loss: 6.3814 - val_accuracy: 0.1430\n",
            "Epoch 35/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 5.0121 - accuracy: 0.1607 - val_loss: 6.3897 - val_accuracy: 0.1445\n",
            "Epoch 36/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 4.9810 - accuracy: 0.1616 - val_loss: 6.4008 - val_accuracy: 0.1450\n",
            "Epoch 37/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 4.9516 - accuracy: 0.1635 - val_loss: 6.4188 - val_accuracy: 0.1420\n",
            "Epoch 38/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 4.9220 - accuracy: 0.1638 - val_loss: 6.4233 - val_accuracy: 0.1466\n",
            "Epoch 39/500\n",
            "73/73 [==============================] - 17s 235ms/step - loss: 4.8878 - accuracy: 0.1653 - val_loss: 6.4403 - val_accuracy: 0.1480\n",
            "Epoch 40/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 4.8584 - accuracy: 0.1677 - val_loss: 6.4445 - val_accuracy: 0.1495\n",
            "Epoch 41/500\n",
            "73/73 [==============================] - 17s 229ms/step - loss: 4.8349 - accuracy: 0.1686 - val_loss: 6.4553 - val_accuracy: 0.1498\n",
            "Epoch 42/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 4.8032 - accuracy: 0.1704 - val_loss: 6.4801 - val_accuracy: 0.1503\n",
            "Epoch 43/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 4.7768 - accuracy: 0.1713 - val_loss: 6.4928 - val_accuracy: 0.1496\n",
            "Epoch 44/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 4.7498 - accuracy: 0.1733 - val_loss: 6.5106 - val_accuracy: 0.1519\n",
            "Epoch 45/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 4.7241 - accuracy: 0.1747 - val_loss: 6.5298 - val_accuracy: 0.1520\n",
            "Epoch 46/500\n",
            "73/73 [==============================] - 17s 229ms/step - loss: 4.7008 - accuracy: 0.1765 - val_loss: 6.5382 - val_accuracy: 0.1527\n",
            "Epoch 47/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 4.6786 - accuracy: 0.1777 - val_loss: 6.5574 - val_accuracy: 0.1528\n",
            "Epoch 48/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 4.6516 - accuracy: 0.1782 - val_loss: 6.5672 - val_accuracy: 0.1531\n",
            "Epoch 49/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 4.6314 - accuracy: 0.1789 - val_loss: 6.5791 - val_accuracy: 0.1551\n",
            "Epoch 50/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 4.6047 - accuracy: 0.1816 - val_loss: 6.6145 - val_accuracy: 0.1524\n",
            "Epoch 51/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 4.5824 - accuracy: 0.1823 - val_loss: 6.6250 - val_accuracy: 0.1539\n",
            "Epoch 52/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 4.5636 - accuracy: 0.1830 - val_loss: 6.6383 - val_accuracy: 0.1560\n",
            "Epoch 53/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 4.5394 - accuracy: 0.1847 - val_loss: 6.6411 - val_accuracy: 0.1563\n",
            "Epoch 54/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 4.5135 - accuracy: 0.1867 - val_loss: 6.6738 - val_accuracy: 0.1554\n",
            "Epoch 55/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 4.4962 - accuracy: 0.1875 - val_loss: 6.6781 - val_accuracy: 0.1565\n",
            "Epoch 56/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 4.4725 - accuracy: 0.1885 - val_loss: 6.7171 - val_accuracy: 0.1555\n",
            "Epoch 57/500\n",
            "73/73 [==============================] - 17s 235ms/step - loss: 4.4510 - accuracy: 0.1905 - val_loss: 6.7258 - val_accuracy: 0.1576\n",
            "Epoch 58/500\n",
            "73/73 [==============================] - 17s 229ms/step - loss: 4.4322 - accuracy: 0.1923 - val_loss: 6.7482 - val_accuracy: 0.1579\n",
            "Epoch 59/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 4.4121 - accuracy: 0.1931 - val_loss: 6.7622 - val_accuracy: 0.1582\n",
            "Epoch 60/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 4.3843 - accuracy: 0.1946 - val_loss: 6.7804 - val_accuracy: 0.1580\n",
            "Epoch 61/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 4.3690 - accuracy: 0.1966 - val_loss: 6.8077 - val_accuracy: 0.1597\n",
            "Epoch 62/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 4.3470 - accuracy: 0.1966 - val_loss: 6.8087 - val_accuracy: 0.1581\n",
            "Epoch 63/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 4.3299 - accuracy: 0.1991 - val_loss: 6.8303 - val_accuracy: 0.1596\n",
            "Epoch 64/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 4.3099 - accuracy: 0.2003 - val_loss: 6.8510 - val_accuracy: 0.1598\n",
            "Epoch 65/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 4.2907 - accuracy: 0.2015 - val_loss: 6.8637 - val_accuracy: 0.1611\n",
            "Epoch 66/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 4.2762 - accuracy: 0.2023 - val_loss: 6.8895 - val_accuracy: 0.1601\n",
            "Epoch 67/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 4.2507 - accuracy: 0.2039 - val_loss: 6.9047 - val_accuracy: 0.1613\n",
            "Epoch 68/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 4.2390 - accuracy: 0.2056 - val_loss: 6.9341 - val_accuracy: 0.1596\n",
            "Epoch 69/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 4.2165 - accuracy: 0.2070 - val_loss: 6.9302 - val_accuracy: 0.1609\n",
            "Epoch 70/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 4.1971 - accuracy: 0.2087 - val_loss: 6.9690 - val_accuracy: 0.1639\n",
            "Epoch 71/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 4.1835 - accuracy: 0.2092 - val_loss: 6.9962 - val_accuracy: 0.1613\n",
            "Epoch 72/500\n",
            "73/73 [==============================] - 16s 219ms/step - loss: 4.1635 - accuracy: 0.2107 - val_loss: 6.9953 - val_accuracy: 0.1618\n",
            "Epoch 73/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 4.1443 - accuracy: 0.2123 - val_loss: 7.0285 - val_accuracy: 0.1609\n",
            "Epoch 74/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 4.1355 - accuracy: 0.2131 - val_loss: 7.0371 - val_accuracy: 0.1621\n",
            "Epoch 75/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 4.1136 - accuracy: 0.2152 - val_loss: 7.0583 - val_accuracy: 0.1624\n",
            "Epoch 76/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 4.0933 - accuracy: 0.2175 - val_loss: 7.0781 - val_accuracy: 0.1627\n",
            "Epoch 77/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 4.0826 - accuracy: 0.2175 - val_loss: 7.1138 - val_accuracy: 0.1610\n",
            "Epoch 78/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 4.0668 - accuracy: 0.2181 - val_loss: 7.0930 - val_accuracy: 0.1632\n",
            "Epoch 79/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 4.0488 - accuracy: 0.2200 - val_loss: 7.1165 - val_accuracy: 0.1616\n",
            "Epoch 80/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 4.0326 - accuracy: 0.2218 - val_loss: 7.1269 - val_accuracy: 0.1625\n",
            "Epoch 81/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 4.0196 - accuracy: 0.2221 - val_loss: 7.1679 - val_accuracy: 0.1643\n",
            "Epoch 82/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.9954 - accuracy: 0.2243 - val_loss: 7.1968 - val_accuracy: 0.1624\n",
            "Epoch 83/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.9838 - accuracy: 0.2249 - val_loss: 7.2182 - val_accuracy: 0.1632\n",
            "Epoch 84/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 3.9707 - accuracy: 0.2271 - val_loss: 7.2471 - val_accuracy: 0.1650\n",
            "Epoch 85/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.9527 - accuracy: 0.2279 - val_loss: 7.2622 - val_accuracy: 0.1633\n",
            "Epoch 86/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.9426 - accuracy: 0.2282 - val_loss: 7.2779 - val_accuracy: 0.1635\n",
            "Epoch 87/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 3.9257 - accuracy: 0.2307 - val_loss: 7.2864 - val_accuracy: 0.1653\n",
            "Epoch 88/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 3.9126 - accuracy: 0.2327 - val_loss: 7.3153 - val_accuracy: 0.1643\n",
            "Epoch 89/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.8994 - accuracy: 0.2337 - val_loss: 7.3254 - val_accuracy: 0.1633\n",
            "Epoch 90/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 3.8834 - accuracy: 0.2350 - val_loss: 7.3425 - val_accuracy: 0.1658\n",
            "Epoch 91/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 3.8723 - accuracy: 0.2362 - val_loss: 7.4029 - val_accuracy: 0.1647\n",
            "Epoch 92/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.8494 - accuracy: 0.2385 - val_loss: 7.3864 - val_accuracy: 0.1650\n",
            "Epoch 93/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 3.8408 - accuracy: 0.2383 - val_loss: 7.4265 - val_accuracy: 0.1650\n",
            "Epoch 94/500\n",
            "73/73 [==============================] - 17s 236ms/step - loss: 3.8292 - accuracy: 0.2397 - val_loss: 7.4304 - val_accuracy: 0.1660\n",
            "Epoch 95/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.8159 - accuracy: 0.2420 - val_loss: 7.4549 - val_accuracy: 0.1652\n",
            "Epoch 96/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.8024 - accuracy: 0.2428 - val_loss: 7.4651 - val_accuracy: 0.1646\n",
            "Epoch 97/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.7846 - accuracy: 0.2436 - val_loss: 7.4856 - val_accuracy: 0.1654\n",
            "Epoch 98/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.7743 - accuracy: 0.2460 - val_loss: 7.5216 - val_accuracy: 0.1649\n",
            "Epoch 99/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 3.7615 - accuracy: 0.2448 - val_loss: 7.5330 - val_accuracy: 0.1656\n",
            "Epoch 100/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.7475 - accuracy: 0.2473 - val_loss: 7.5435 - val_accuracy: 0.1650\n",
            "Epoch 101/500\n",
            "73/73 [==============================] - 16s 219ms/step - loss: 3.7400 - accuracy: 0.2482 - val_loss: 7.5698 - val_accuracy: 0.1642\n",
            "Epoch 102/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 3.7241 - accuracy: 0.2507 - val_loss: 7.5995 - val_accuracy: 0.1649\n",
            "Epoch 103/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 3.7142 - accuracy: 0.2519 - val_loss: 7.5817 - val_accuracy: 0.1663\n",
            "Epoch 104/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 3.7042 - accuracy: 0.2520 - val_loss: 7.6046 - val_accuracy: 0.1667\n",
            "Epoch 105/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 3.6931 - accuracy: 0.2538 - val_loss: 7.6365 - val_accuracy: 0.1671\n",
            "Epoch 106/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 3.6754 - accuracy: 0.2550 - val_loss: 7.6545 - val_accuracy: 0.1666\n",
            "Epoch 107/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 3.6625 - accuracy: 0.2568 - val_loss: 7.6875 - val_accuracy: 0.1680\n",
            "Epoch 108/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.6533 - accuracy: 0.2584 - val_loss: 7.6983 - val_accuracy: 0.1666\n",
            "Epoch 109/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.6418 - accuracy: 0.2584 - val_loss: 7.7304 - val_accuracy: 0.1678\n",
            "Epoch 110/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 3.6255 - accuracy: 0.2596 - val_loss: 7.7485 - val_accuracy: 0.1682\n",
            "Epoch 111/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.6181 - accuracy: 0.2619 - val_loss: 7.7558 - val_accuracy: 0.1669\n",
            "Epoch 112/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.6082 - accuracy: 0.2636 - val_loss: 7.7698 - val_accuracy: 0.1681\n",
            "Epoch 113/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.5914 - accuracy: 0.2645 - val_loss: 7.8036 - val_accuracy: 0.1680\n",
            "Epoch 114/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 3.5876 - accuracy: 0.2652 - val_loss: 7.8401 - val_accuracy: 0.1667\n",
            "Epoch 115/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.5769 - accuracy: 0.2666 - val_loss: 7.8394 - val_accuracy: 0.1674\n",
            "Epoch 116/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.5641 - accuracy: 0.2684 - val_loss: 7.8783 - val_accuracy: 0.1678\n",
            "Epoch 117/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 3.5517 - accuracy: 0.2684 - val_loss: 7.8796 - val_accuracy: 0.1675\n",
            "Epoch 118/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 3.5424 - accuracy: 0.2695 - val_loss: 7.9130 - val_accuracy: 0.1687\n",
            "Epoch 119/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 3.5298 - accuracy: 0.2727 - val_loss: 7.9139 - val_accuracy: 0.1699\n",
            "Epoch 120/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 3.5156 - accuracy: 0.2729 - val_loss: 7.9325 - val_accuracy: 0.1696\n",
            "Epoch 121/500\n",
            "73/73 [==============================] - 17s 235ms/step - loss: 3.5061 - accuracy: 0.2743 - val_loss: 7.9346 - val_accuracy: 0.1709\n",
            "Epoch 122/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 3.4979 - accuracy: 0.2766 - val_loss: 7.9809 - val_accuracy: 0.1702\n",
            "Epoch 123/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.4886 - accuracy: 0.2751 - val_loss: 7.9755 - val_accuracy: 0.1695\n",
            "Epoch 124/500\n",
            "73/73 [==============================] - 17s 230ms/step - loss: 3.4780 - accuracy: 0.2767 - val_loss: 7.9907 - val_accuracy: 0.1689\n",
            "Epoch 125/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 3.4660 - accuracy: 0.2774 - val_loss: 8.0541 - val_accuracy: 0.1694\n",
            "Epoch 126/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.4558 - accuracy: 0.2787 - val_loss: 8.0251 - val_accuracy: 0.1694\n",
            "Epoch 127/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 3.4495 - accuracy: 0.2806 - val_loss: 8.0664 - val_accuracy: 0.1700\n",
            "Epoch 128/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.4359 - accuracy: 0.2814 - val_loss: 8.1024 - val_accuracy: 0.1697\n",
            "Epoch 129/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 3.4250 - accuracy: 0.2834 - val_loss: 8.1011 - val_accuracy: 0.1687\n",
            "Epoch 130/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.4213 - accuracy: 0.2845 - val_loss: 8.1064 - val_accuracy: 0.1701\n",
            "Epoch 131/500\n",
            "73/73 [==============================] - 17s 239ms/step - loss: 3.4047 - accuracy: 0.2852 - val_loss: 8.1464 - val_accuracy: 0.1713\n",
            "Epoch 132/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 3.3973 - accuracy: 0.2875 - val_loss: 8.1462 - val_accuracy: 0.1713\n",
            "Epoch 133/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.3855 - accuracy: 0.2884 - val_loss: 8.1856 - val_accuracy: 0.1696\n",
            "Epoch 134/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.3783 - accuracy: 0.2905 - val_loss: 8.1886 - val_accuracy: 0.1702\n",
            "Epoch 135/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.3660 - accuracy: 0.2910 - val_loss: 8.2072 - val_accuracy: 0.1709\n",
            "Epoch 136/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.3622 - accuracy: 0.2910 - val_loss: 8.2363 - val_accuracy: 0.1693\n",
            "Epoch 137/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.3473 - accuracy: 0.2932 - val_loss: 8.2210 - val_accuracy: 0.1698\n",
            "Epoch 138/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 3.3406 - accuracy: 0.2941 - val_loss: 8.2876 - val_accuracy: 0.1728\n",
            "Epoch 139/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 3.3332 - accuracy: 0.2946 - val_loss: 8.2824 - val_accuracy: 0.1709\n",
            "Epoch 140/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.3194 - accuracy: 0.2965 - val_loss: 8.3333 - val_accuracy: 0.1697\n",
            "Epoch 141/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 3.3150 - accuracy: 0.2982 - val_loss: 8.3531 - val_accuracy: 0.1705\n",
            "Epoch 142/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.3026 - accuracy: 0.2996 - val_loss: 8.3339 - val_accuracy: 0.1717\n",
            "Epoch 143/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.2974 - accuracy: 0.2990 - val_loss: 8.3524 - val_accuracy: 0.1719\n",
            "Epoch 144/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.2900 - accuracy: 0.2997 - val_loss: 8.3649 - val_accuracy: 0.1712\n",
            "Epoch 145/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 3.2819 - accuracy: 0.3002 - val_loss: 8.3803 - val_accuracy: 0.1727\n",
            "Epoch 146/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 3.2746 - accuracy: 0.3020 - val_loss: 8.4159 - val_accuracy: 0.1695\n",
            "Epoch 147/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 3.2632 - accuracy: 0.3032 - val_loss: 8.4105 - val_accuracy: 0.1729\n",
            "Epoch 148/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 3.2546 - accuracy: 0.3044 - val_loss: 8.4539 - val_accuracy: 0.1710\n",
            "Epoch 149/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.2460 - accuracy: 0.3065 - val_loss: 8.4517 - val_accuracy: 0.1714\n",
            "Epoch 150/500\n",
            "73/73 [==============================] - 17s 228ms/step - loss: 3.2364 - accuracy: 0.3069 - val_loss: 8.4805 - val_accuracy: 0.1727\n",
            "Epoch 151/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 3.2258 - accuracy: 0.3086 - val_loss: 8.4939 - val_accuracy: 0.1710\n",
            "Epoch 152/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.2168 - accuracy: 0.3093 - val_loss: 8.5214 - val_accuracy: 0.1721\n",
            "Epoch 153/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.2019 - accuracy: 0.3125 - val_loss: 8.5262 - val_accuracy: 0.1704\n",
            "Epoch 154/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.1982 - accuracy: 0.3120 - val_loss: 8.5452 - val_accuracy: 0.1708\n",
            "Epoch 155/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 3.1914 - accuracy: 0.3144 - val_loss: 8.5578 - val_accuracy: 0.1716\n",
            "Epoch 156/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 3.1807 - accuracy: 0.3151 - val_loss: 8.6070 - val_accuracy: 0.1714\n",
            "Epoch 157/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.1790 - accuracy: 0.3155 - val_loss: 8.6356 - val_accuracy: 0.1715\n",
            "Epoch 158/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 3.1735 - accuracy: 0.3161 - val_loss: 8.6129 - val_accuracy: 0.1732\n",
            "Epoch 159/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.1560 - accuracy: 0.3177 - val_loss: 8.6547 - val_accuracy: 0.1726\n",
            "Epoch 160/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.1540 - accuracy: 0.3183 - val_loss: 8.6590 - val_accuracy: 0.1706\n",
            "Epoch 161/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.1401 - accuracy: 0.3192 - val_loss: 8.7011 - val_accuracy: 0.1710\n",
            "Epoch 162/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.1321 - accuracy: 0.3210 - val_loss: 8.6566 - val_accuracy: 0.1714\n",
            "Epoch 163/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 3.1257 - accuracy: 0.3211 - val_loss: 8.7223 - val_accuracy: 0.1736\n",
            "Epoch 164/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.1212 - accuracy: 0.3233 - val_loss: 8.7344 - val_accuracy: 0.1717\n",
            "Epoch 165/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.1084 - accuracy: 0.3239 - val_loss: 8.7382 - val_accuracy: 0.1721\n",
            "Epoch 166/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.1058 - accuracy: 0.3245 - val_loss: 8.7546 - val_accuracy: 0.1718\n",
            "Epoch 167/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 3.0984 - accuracy: 0.3252 - val_loss: 8.8002 - val_accuracy: 0.1715\n",
            "Epoch 168/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 3.0885 - accuracy: 0.3259 - val_loss: 8.7816 - val_accuracy: 0.1737\n",
            "Epoch 169/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 3.0793 - accuracy: 0.3287 - val_loss: 8.7986 - val_accuracy: 0.1716\n",
            "Epoch 170/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 3.0730 - accuracy: 0.3294 - val_loss: 8.8346 - val_accuracy: 0.1732\n",
            "Epoch 171/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.0654 - accuracy: 0.3309 - val_loss: 8.8592 - val_accuracy: 0.1730\n",
            "Epoch 172/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.0556 - accuracy: 0.3312 - val_loss: 8.8246 - val_accuracy: 0.1730\n",
            "Epoch 173/500\n",
            "73/73 [==============================] - 17s 235ms/step - loss: 3.0500 - accuracy: 0.3310 - val_loss: 8.8800 - val_accuracy: 0.1741\n",
            "Epoch 174/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.0411 - accuracy: 0.3344 - val_loss: 8.9028 - val_accuracy: 0.1740\n",
            "Epoch 175/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 3.0380 - accuracy: 0.3344 - val_loss: 8.9041 - val_accuracy: 0.1728\n",
            "Epoch 176/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 3.0316 - accuracy: 0.3342 - val_loss: 8.9189 - val_accuracy: 0.1738\n",
            "Epoch 177/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 3.0243 - accuracy: 0.3357 - val_loss: 8.9129 - val_accuracy: 0.1726\n",
            "Epoch 178/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 3.0164 - accuracy: 0.3367 - val_loss: 8.9573 - val_accuracy: 0.1735\n",
            "Epoch 179/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 3.0082 - accuracy: 0.3375 - val_loss: 8.9335 - val_accuracy: 0.1741\n",
            "Epoch 180/500\n",
            "73/73 [==============================] - 17s 228ms/step - loss: 2.9994 - accuracy: 0.3393 - val_loss: 8.9675 - val_accuracy: 0.1730\n",
            "Epoch 181/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.9940 - accuracy: 0.3397 - val_loss: 9.0189 - val_accuracy: 0.1731\n",
            "Epoch 182/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.9811 - accuracy: 0.3418 - val_loss: 9.0373 - val_accuracy: 0.1727\n",
            "Epoch 183/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.9822 - accuracy: 0.3419 - val_loss: 9.0651 - val_accuracy: 0.1735\n",
            "Epoch 184/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.9731 - accuracy: 0.3436 - val_loss: 9.0254 - val_accuracy: 0.1731\n",
            "Epoch 185/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 2.9678 - accuracy: 0.3434 - val_loss: 9.1030 - val_accuracy: 0.1752\n",
            "Epoch 186/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.9584 - accuracy: 0.3448 - val_loss: 9.0853 - val_accuracy: 0.1734\n",
            "Epoch 187/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.9518 - accuracy: 0.3455 - val_loss: 9.1246 - val_accuracy: 0.1738\n",
            "Epoch 188/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.9491 - accuracy: 0.3463 - val_loss: 9.1249 - val_accuracy: 0.1742\n",
            "Epoch 189/500\n",
            "73/73 [==============================] - 17s 237ms/step - loss: 2.9374 - accuracy: 0.3480 - val_loss: 9.1201 - val_accuracy: 0.1754\n",
            "Epoch 190/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.9314 - accuracy: 0.3491 - val_loss: 9.1178 - val_accuracy: 0.1753\n",
            "Epoch 191/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 2.9250 - accuracy: 0.3495 - val_loss: 9.1594 - val_accuracy: 0.1761\n",
            "Epoch 192/500\n",
            "73/73 [==============================] - 17s 228ms/step - loss: 2.9146 - accuracy: 0.3511 - val_loss: 9.1796 - val_accuracy: 0.1751\n",
            "Epoch 193/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.9132 - accuracy: 0.3504 - val_loss: 9.1652 - val_accuracy: 0.1744\n",
            "Epoch 194/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.9054 - accuracy: 0.3524 - val_loss: 9.1893 - val_accuracy: 0.1736\n",
            "Epoch 195/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.9030 - accuracy: 0.3534 - val_loss: 9.2200 - val_accuracy: 0.1745\n",
            "Epoch 196/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.8871 - accuracy: 0.3556 - val_loss: 9.2962 - val_accuracy: 0.1753\n",
            "Epoch 197/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.8822 - accuracy: 0.3577 - val_loss: 9.2175 - val_accuracy: 0.1758\n",
            "Epoch 198/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.8806 - accuracy: 0.3552 - val_loss: 9.2582 - val_accuracy: 0.1750\n",
            "Epoch 199/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.8747 - accuracy: 0.3577 - val_loss: 9.1994 - val_accuracy: 0.1749\n",
            "Epoch 200/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.8594 - accuracy: 0.3597 - val_loss: 9.2853 - val_accuracy: 0.1758\n",
            "Epoch 201/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.8649 - accuracy: 0.3589 - val_loss: 9.2707 - val_accuracy: 0.1743\n",
            "Epoch 202/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.8550 - accuracy: 0.3603 - val_loss: 9.3312 - val_accuracy: 0.1757\n",
            "Epoch 203/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 2.8530 - accuracy: 0.3603 - val_loss: 9.3290 - val_accuracy: 0.1766\n",
            "Epoch 204/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.8461 - accuracy: 0.3623 - val_loss: 9.3610 - val_accuracy: 0.1741\n",
            "Epoch 205/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.8347 - accuracy: 0.3637 - val_loss: 9.3390 - val_accuracy: 0.1751\n",
            "Epoch 206/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.8331 - accuracy: 0.3630 - val_loss: 9.3970 - val_accuracy: 0.1764\n",
            "Epoch 207/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.8236 - accuracy: 0.3639 - val_loss: 9.4345 - val_accuracy: 0.1754\n",
            "Epoch 208/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 2.8156 - accuracy: 0.3676 - val_loss: 9.3814 - val_accuracy: 0.1756\n",
            "Epoch 209/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.8133 - accuracy: 0.3661 - val_loss: 9.4368 - val_accuracy: 0.1762\n",
            "Epoch 210/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 2.8068 - accuracy: 0.3675 - val_loss: 9.3990 - val_accuracy: 0.1771\n",
            "Epoch 211/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.7954 - accuracy: 0.3696 - val_loss: 9.4632 - val_accuracy: 0.1762\n",
            "Epoch 212/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 2.7866 - accuracy: 0.3702 - val_loss: 9.4871 - val_accuracy: 0.1772\n",
            "Epoch 213/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.7884 - accuracy: 0.3691 - val_loss: 9.4548 - val_accuracy: 0.1768\n",
            "Epoch 214/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.7826 - accuracy: 0.3712 - val_loss: 9.4653 - val_accuracy: 0.1756\n",
            "Epoch 215/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.7765 - accuracy: 0.3708 - val_loss: 9.4803 - val_accuracy: 0.1755\n",
            "Epoch 216/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.7733 - accuracy: 0.3720 - val_loss: 9.5152 - val_accuracy: 0.1771\n",
            "Epoch 217/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 2.7648 - accuracy: 0.3740 - val_loss: 9.4970 - val_accuracy: 0.1773\n",
            "Epoch 218/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.7582 - accuracy: 0.3750 - val_loss: 9.5761 - val_accuracy: 0.1756\n",
            "Epoch 219/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.7514 - accuracy: 0.3754 - val_loss: 9.5864 - val_accuracy: 0.1771\n",
            "Epoch 220/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 2.7496 - accuracy: 0.3760 - val_loss: 9.5865 - val_accuracy: 0.1776\n",
            "Epoch 221/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.7390 - accuracy: 0.3775 - val_loss: 9.6431 - val_accuracy: 0.1758\n",
            "Epoch 222/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.7348 - accuracy: 0.3783 - val_loss: 9.5461 - val_accuracy: 0.1764\n",
            "Epoch 223/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.7315 - accuracy: 0.3777 - val_loss: 9.5874 - val_accuracy: 0.1766\n",
            "Epoch 224/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.7248 - accuracy: 0.3795 - val_loss: 9.5595 - val_accuracy: 0.1767\n",
            "Epoch 225/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.7204 - accuracy: 0.3803 - val_loss: 9.6508 - val_accuracy: 0.1766\n",
            "Epoch 226/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.7170 - accuracy: 0.3819 - val_loss: 9.6042 - val_accuracy: 0.1749\n",
            "Epoch 227/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.7045 - accuracy: 0.3842 - val_loss: 9.6470 - val_accuracy: 0.1764\n",
            "Epoch 228/500\n",
            "73/73 [==============================] - 17s 235ms/step - loss: 2.7041 - accuracy: 0.3838 - val_loss: 9.6893 - val_accuracy: 0.1782\n",
            "Epoch 229/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.6966 - accuracy: 0.3854 - val_loss: 9.6435 - val_accuracy: 0.1768\n",
            "Epoch 230/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.6913 - accuracy: 0.3849 - val_loss: 9.6833 - val_accuracy: 0.1771\n",
            "Epoch 231/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.6846 - accuracy: 0.3867 - val_loss: 9.6851 - val_accuracy: 0.1780\n",
            "Epoch 232/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 2.6842 - accuracy: 0.3852 - val_loss: 9.6663 - val_accuracy: 0.1798\n",
            "Epoch 233/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.6758 - accuracy: 0.3875 - val_loss: 9.6995 - val_accuracy: 0.1793\n",
            "Epoch 234/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.6727 - accuracy: 0.3881 - val_loss: 9.7791 - val_accuracy: 0.1798\n",
            "Epoch 235/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.6687 - accuracy: 0.3887 - val_loss: 9.7107 - val_accuracy: 0.1779\n",
            "Epoch 236/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.6610 - accuracy: 0.3900 - val_loss: 9.7352 - val_accuracy: 0.1779\n",
            "Epoch 237/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.6526 - accuracy: 0.3914 - val_loss: 9.7812 - val_accuracy: 0.1796\n",
            "Epoch 238/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.6465 - accuracy: 0.3923 - val_loss: 9.7636 - val_accuracy: 0.1769\n",
            "Epoch 239/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.6412 - accuracy: 0.3918 - val_loss: 9.7999 - val_accuracy: 0.1789\n",
            "Epoch 240/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.6434 - accuracy: 0.3917 - val_loss: 9.8483 - val_accuracy: 0.1790\n",
            "Epoch 241/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.6350 - accuracy: 0.3934 - val_loss: 9.8248 - val_accuracy: 0.1783\n",
            "Epoch 242/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.6270 - accuracy: 0.3952 - val_loss: 9.8815 - val_accuracy: 0.1774\n",
            "Epoch 243/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.6293 - accuracy: 0.3963 - val_loss: 9.8067 - val_accuracy: 0.1786\n",
            "Epoch 244/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.6205 - accuracy: 0.3954 - val_loss: 9.8992 - val_accuracy: 0.1787\n",
            "Epoch 245/500\n",
            "73/73 [==============================] - 17s 236ms/step - loss: 2.6165 - accuracy: 0.3971 - val_loss: 9.8585 - val_accuracy: 0.1806\n",
            "Epoch 246/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.6113 - accuracy: 0.3972 - val_loss: 9.8750 - val_accuracy: 0.1782\n",
            "Epoch 247/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.6006 - accuracy: 0.4016 - val_loss: 9.9110 - val_accuracy: 0.1793\n",
            "Epoch 248/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.6006 - accuracy: 0.3998 - val_loss: 9.9614 - val_accuracy: 0.1772\n",
            "Epoch 249/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.5981 - accuracy: 0.3984 - val_loss: 9.8669 - val_accuracy: 0.1800\n",
            "Epoch 250/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.5843 - accuracy: 0.4012 - val_loss: 9.9457 - val_accuracy: 0.1785\n",
            "Epoch 251/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.5849 - accuracy: 0.4027 - val_loss: 9.9128 - val_accuracy: 0.1789\n",
            "Epoch 252/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.5752 - accuracy: 0.4039 - val_loss: 9.9394 - val_accuracy: 0.1774\n",
            "Epoch 253/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.5761 - accuracy: 0.4042 - val_loss: 9.9311 - val_accuracy: 0.1804\n",
            "Epoch 254/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.5668 - accuracy: 0.4054 - val_loss: 9.9460 - val_accuracy: 0.1800\n",
            "Epoch 255/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.5664 - accuracy: 0.4057 - val_loss: 10.0490 - val_accuracy: 0.1799\n",
            "Epoch 256/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.5615 - accuracy: 0.4043 - val_loss: 9.9684 - val_accuracy: 0.1793\n",
            "Epoch 257/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.5591 - accuracy: 0.4070 - val_loss: 9.9820 - val_accuracy: 0.1788\n",
            "Epoch 258/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.5453 - accuracy: 0.4086 - val_loss: 10.0348 - val_accuracy: 0.1795\n",
            "Epoch 259/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.5443 - accuracy: 0.4090 - val_loss: 9.9873 - val_accuracy: 0.1778\n",
            "Epoch 260/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.5397 - accuracy: 0.4076 - val_loss: 10.0151 - val_accuracy: 0.1792\n",
            "Epoch 261/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.5369 - accuracy: 0.4089 - val_loss: 10.0732 - val_accuracy: 0.1796\n",
            "Epoch 262/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.5387 - accuracy: 0.4084 - val_loss: 10.0717 - val_accuracy: 0.1789\n",
            "Epoch 263/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.5260 - accuracy: 0.4110 - val_loss: 10.0942 - val_accuracy: 0.1800\n",
            "Epoch 264/500\n",
            "73/73 [==============================] - 17s 235ms/step - loss: 2.5219 - accuracy: 0.4128 - val_loss: 10.1072 - val_accuracy: 0.1819\n",
            "Epoch 265/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 2.5148 - accuracy: 0.4148 - val_loss: 10.1192 - val_accuracy: 0.1822\n",
            "Epoch 266/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.5112 - accuracy: 0.4141 - val_loss: 10.1100 - val_accuracy: 0.1794\n",
            "Epoch 267/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.5125 - accuracy: 0.4133 - val_loss: 10.0755 - val_accuracy: 0.1814\n",
            "Epoch 268/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.5048 - accuracy: 0.4150 - val_loss: 10.1614 - val_accuracy: 0.1790\n",
            "Epoch 269/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.5006 - accuracy: 0.4170 - val_loss: 10.1716 - val_accuracy: 0.1800\n",
            "Epoch 270/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.4944 - accuracy: 0.4161 - val_loss: 10.1136 - val_accuracy: 0.1803\n",
            "Epoch 271/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.4924 - accuracy: 0.4179 - val_loss: 10.1548 - val_accuracy: 0.1815\n",
            "Epoch 272/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 2.4931 - accuracy: 0.4171 - val_loss: 10.1796 - val_accuracy: 0.1826\n",
            "Epoch 273/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.4844 - accuracy: 0.4186 - val_loss: 10.2136 - val_accuracy: 0.1794\n",
            "Epoch 274/500\n",
            "73/73 [==============================] - 16s 218ms/step - loss: 2.4786 - accuracy: 0.4210 - val_loss: 10.1899 - val_accuracy: 0.1809\n",
            "Epoch 275/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.4761 - accuracy: 0.4198 - val_loss: 10.1937 - val_accuracy: 0.1800\n",
            "Epoch 276/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 2.4718 - accuracy: 0.4216 - val_loss: 10.2360 - val_accuracy: 0.1828\n",
            "Epoch 277/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.4668 - accuracy: 0.4217 - val_loss: 10.2342 - val_accuracy: 0.1801\n",
            "Epoch 278/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.4583 - accuracy: 0.4229 - val_loss: 10.2539 - val_accuracy: 0.1819\n",
            "Epoch 279/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.4588 - accuracy: 0.4233 - val_loss: 10.2168 - val_accuracy: 0.1802\n",
            "Epoch 280/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.4567 - accuracy: 0.4231 - val_loss: 10.3194 - val_accuracy: 0.1805\n",
            "Epoch 281/500\n",
            "73/73 [==============================] - 17s 236ms/step - loss: 2.4467 - accuracy: 0.4242 - val_loss: 10.2633 - val_accuracy: 0.1835\n",
            "Epoch 282/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.4549 - accuracy: 0.4239 - val_loss: 10.2774 - val_accuracy: 0.1802\n",
            "Epoch 283/500\n",
            "73/73 [==============================] - 16s 219ms/step - loss: 2.4433 - accuracy: 0.4256 - val_loss: 10.2448 - val_accuracy: 0.1811\n",
            "Epoch 284/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.4345 - accuracy: 0.4278 - val_loss: 10.2892 - val_accuracy: 0.1820\n",
            "Epoch 285/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.4323 - accuracy: 0.4281 - val_loss: 10.2745 - val_accuracy: 0.1821\n",
            "Epoch 286/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.4334 - accuracy: 0.4283 - val_loss: 10.3406 - val_accuracy: 0.1818\n",
            "Epoch 287/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.4257 - accuracy: 0.4296 - val_loss: 10.3179 - val_accuracy: 0.1829\n",
            "Epoch 288/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.4269 - accuracy: 0.4294 - val_loss: 10.2862 - val_accuracy: 0.1815\n",
            "Epoch 289/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.4179 - accuracy: 0.4294 - val_loss: 10.3821 - val_accuracy: 0.1820\n",
            "Epoch 290/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.4132 - accuracy: 0.4301 - val_loss: 10.3960 - val_accuracy: 0.1837\n",
            "Epoch 291/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.4064 - accuracy: 0.4330 - val_loss: 10.3940 - val_accuracy: 0.1814\n",
            "Epoch 292/500\n",
            "73/73 [==============================] - 16s 219ms/step - loss: 2.4088 - accuracy: 0.4315 - val_loss: 10.3307 - val_accuracy: 0.1830\n",
            "Epoch 293/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.3999 - accuracy: 0.4338 - val_loss: 10.4564 - val_accuracy: 0.1836\n",
            "Epoch 294/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.3949 - accuracy: 0.4341 - val_loss: 10.3841 - val_accuracy: 0.1813\n",
            "Epoch 295/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.3972 - accuracy: 0.4343 - val_loss: 10.4136 - val_accuracy: 0.1805\n",
            "Epoch 296/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.3945 - accuracy: 0.4335 - val_loss: 10.3187 - val_accuracy: 0.1807\n",
            "Epoch 297/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.3833 - accuracy: 0.4361 - val_loss: 10.4556 - val_accuracy: 0.1826\n",
            "Epoch 298/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 2.3804 - accuracy: 0.4372 - val_loss: 10.4482 - val_accuracy: 0.1828\n",
            "Epoch 299/500\n",
            "73/73 [==============================] - 16s 219ms/step - loss: 2.3784 - accuracy: 0.4372 - val_loss: 10.4009 - val_accuracy: 0.1824\n",
            "Epoch 300/500\n",
            "73/73 [==============================] - 16s 219ms/step - loss: 2.3805 - accuracy: 0.4363 - val_loss: 10.4429 - val_accuracy: 0.1826\n",
            "Epoch 301/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 2.3718 - accuracy: 0.4396 - val_loss: 10.4665 - val_accuracy: 0.1840\n",
            "Epoch 302/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 2.3624 - accuracy: 0.4402 - val_loss: 10.5496 - val_accuracy: 0.1840\n",
            "Epoch 303/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.3665 - accuracy: 0.4398 - val_loss: 10.4998 - val_accuracy: 0.1815\n",
            "Epoch 304/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.3577 - accuracy: 0.4401 - val_loss: 10.5014 - val_accuracy: 0.1825\n",
            "Epoch 305/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.3606 - accuracy: 0.4403 - val_loss: 10.5010 - val_accuracy: 0.1836\n",
            "Epoch 306/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.3472 - accuracy: 0.4425 - val_loss: 10.4914 - val_accuracy: 0.1832\n",
            "Epoch 307/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 2.3483 - accuracy: 0.4436 - val_loss: 10.5189 - val_accuracy: 0.1844\n",
            "Epoch 308/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.3438 - accuracy: 0.4425 - val_loss: 10.5246 - val_accuracy: 0.1838\n",
            "Epoch 309/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.3448 - accuracy: 0.4420 - val_loss: 10.5307 - val_accuracy: 0.1830\n",
            "Epoch 310/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 2.3361 - accuracy: 0.4445 - val_loss: 10.5501 - val_accuracy: 0.1850\n",
            "Epoch 311/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.3340 - accuracy: 0.4438 - val_loss: 10.5877 - val_accuracy: 0.1849\n",
            "Epoch 312/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.3281 - accuracy: 0.4458 - val_loss: 10.5913 - val_accuracy: 0.1842\n",
            "Epoch 313/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.3238 - accuracy: 0.4463 - val_loss: 10.5737 - val_accuracy: 0.1835\n",
            "Epoch 314/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.3211 - accuracy: 0.4473 - val_loss: 10.5926 - val_accuracy: 0.1840\n",
            "Epoch 315/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.3215 - accuracy: 0.4445 - val_loss: 10.5498 - val_accuracy: 0.1844\n",
            "Epoch 316/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 2.3143 - accuracy: 0.4491 - val_loss: 10.6369 - val_accuracy: 0.1853\n",
            "Epoch 317/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.3121 - accuracy: 0.4478 - val_loss: 10.5939 - val_accuracy: 0.1844\n",
            "Epoch 318/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.3143 - accuracy: 0.4477 - val_loss: 10.6441 - val_accuracy: 0.1844\n",
            "Epoch 319/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.3140 - accuracy: 0.4473 - val_loss: 10.6441 - val_accuracy: 0.1841\n",
            "Epoch 320/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.3051 - accuracy: 0.4510 - val_loss: 10.6717 - val_accuracy: 0.1853\n",
            "Epoch 321/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.3001 - accuracy: 0.4505 - val_loss: 10.5817 - val_accuracy: 0.1843\n",
            "Epoch 322/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.2914 - accuracy: 0.4526 - val_loss: 10.6292 - val_accuracy: 0.1841\n",
            "Epoch 323/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.2861 - accuracy: 0.4520 - val_loss: 10.6644 - val_accuracy: 0.1849\n",
            "Epoch 324/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.2892 - accuracy: 0.4525 - val_loss: 10.6954 - val_accuracy: 0.1840\n",
            "Epoch 325/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.2865 - accuracy: 0.4527 - val_loss: 10.7114 - val_accuracy: 0.1837\n",
            "Epoch 326/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.2794 - accuracy: 0.4541 - val_loss: 10.7815 - val_accuracy: 0.1843\n",
            "Epoch 327/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.2803 - accuracy: 0.4537 - val_loss: 10.6786 - val_accuracy: 0.1847\n",
            "Epoch 328/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.2738 - accuracy: 0.4565 - val_loss: 10.7601 - val_accuracy: 0.1846\n",
            "Epoch 329/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.2771 - accuracy: 0.4548 - val_loss: 10.7024 - val_accuracy: 0.1840\n",
            "Epoch 330/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 2.2661 - accuracy: 0.4559 - val_loss: 10.7049 - val_accuracy: 0.1834\n",
            "Epoch 331/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.2667 - accuracy: 0.4577 - val_loss: 10.7019 - val_accuracy: 0.1852\n",
            "Epoch 332/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.2587 - accuracy: 0.4588 - val_loss: 10.7400 - val_accuracy: 0.1842\n",
            "Epoch 333/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 2.2654 - accuracy: 0.4563 - val_loss: 10.7390 - val_accuracy: 0.1857\n",
            "Epoch 334/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.2549 - accuracy: 0.4581 - val_loss: 10.8031 - val_accuracy: 0.1849\n",
            "Epoch 335/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.2513 - accuracy: 0.4588 - val_loss: 10.7247 - val_accuracy: 0.1840\n",
            "Epoch 336/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.2573 - accuracy: 0.4578 - val_loss: 10.7497 - val_accuracy: 0.1848\n",
            "Epoch 337/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.2503 - accuracy: 0.4585 - val_loss: 10.7808 - val_accuracy: 0.1855\n",
            "Epoch 338/500\n",
            "73/73 [==============================] - 17s 228ms/step - loss: 2.2425 - accuracy: 0.4620 - val_loss: 10.8122 - val_accuracy: 0.1847\n",
            "Epoch 339/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 2.2420 - accuracy: 0.4609 - val_loss: 10.8357 - val_accuracy: 0.1869\n",
            "Epoch 340/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.2364 - accuracy: 0.4627 - val_loss: 10.8107 - val_accuracy: 0.1851\n",
            "Epoch 341/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.2290 - accuracy: 0.4636 - val_loss: 10.7972 - val_accuracy: 0.1850\n",
            "Epoch 342/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.2318 - accuracy: 0.4634 - val_loss: 10.7698 - val_accuracy: 0.1858\n",
            "Epoch 343/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.2228 - accuracy: 0.4652 - val_loss: 10.8463 - val_accuracy: 0.1833\n",
            "Epoch 344/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.2275 - accuracy: 0.4639 - val_loss: 10.8242 - val_accuracy: 0.1855\n",
            "Epoch 345/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.2208 - accuracy: 0.4658 - val_loss: 10.8291 - val_accuracy: 0.1848\n",
            "Epoch 346/500\n",
            "73/73 [==============================] - 18s 243ms/step - loss: 2.2202 - accuracy: 0.4648 - val_loss: 10.8385 - val_accuracy: 0.1874\n",
            "Epoch 347/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 2.2153 - accuracy: 0.4668 - val_loss: 10.8670 - val_accuracy: 0.1881\n",
            "Epoch 348/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.2180 - accuracy: 0.4644 - val_loss: 10.8403 - val_accuracy: 0.1858\n",
            "Epoch 349/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.2086 - accuracy: 0.4674 - val_loss: 10.9003 - val_accuracy: 0.1852\n",
            "Epoch 350/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.2075 - accuracy: 0.4678 - val_loss: 10.8530 - val_accuracy: 0.1862\n",
            "Epoch 351/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.1950 - accuracy: 0.4701 - val_loss: 10.8892 - val_accuracy: 0.1870\n",
            "Epoch 352/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.1963 - accuracy: 0.4694 - val_loss: 10.8620 - val_accuracy: 0.1857\n",
            "Epoch 353/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.1941 - accuracy: 0.4696 - val_loss: 10.9997 - val_accuracy: 0.1866\n",
            "Epoch 354/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.1920 - accuracy: 0.4711 - val_loss: 10.9242 - val_accuracy: 0.1842\n",
            "Epoch 355/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.1917 - accuracy: 0.4676 - val_loss: 10.8852 - val_accuracy: 0.1861\n",
            "Epoch 356/500\n",
            "73/73 [==============================] - 17s 229ms/step - loss: 2.1838 - accuracy: 0.4721 - val_loss: 10.9810 - val_accuracy: 0.1873\n",
            "Epoch 357/500\n",
            "73/73 [==============================] - 17s 229ms/step - loss: 2.1833 - accuracy: 0.4733 - val_loss: 10.9901 - val_accuracy: 0.1866\n",
            "Epoch 358/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 2.1757 - accuracy: 0.4730 - val_loss: 10.9339 - val_accuracy: 0.1886\n",
            "Epoch 359/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.1787 - accuracy: 0.4709 - val_loss: 11.0360 - val_accuracy: 0.1876\n",
            "Epoch 360/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.1741 - accuracy: 0.4737 - val_loss: 10.9147 - val_accuracy: 0.1863\n",
            "Epoch 361/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.1695 - accuracy: 0.4757 - val_loss: 10.9346 - val_accuracy: 0.1869\n",
            "Epoch 362/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.1675 - accuracy: 0.4747 - val_loss: 10.9298 - val_accuracy: 0.1865\n",
            "Epoch 363/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.1711 - accuracy: 0.4736 - val_loss: 10.9743 - val_accuracy: 0.1853\n",
            "Epoch 364/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 2.1637 - accuracy: 0.4766 - val_loss: 11.0221 - val_accuracy: 0.1889\n",
            "Epoch 365/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.1609 - accuracy: 0.4769 - val_loss: 10.9525 - val_accuracy: 0.1870\n",
            "Epoch 366/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.1575 - accuracy: 0.4784 - val_loss: 10.9783 - val_accuracy: 0.1879\n",
            "Epoch 367/500\n",
            "73/73 [==============================] - 17s 236ms/step - loss: 2.1520 - accuracy: 0.4778 - val_loss: 10.9817 - val_accuracy: 0.1889\n",
            "Epoch 368/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.1528 - accuracy: 0.4782 - val_loss: 11.0905 - val_accuracy: 0.1884\n",
            "Epoch 369/500\n",
            "73/73 [==============================] - 16s 219ms/step - loss: 2.1506 - accuracy: 0.4777 - val_loss: 10.9483 - val_accuracy: 0.1872\n",
            "Epoch 370/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.1464 - accuracy: 0.4780 - val_loss: 11.0076 - val_accuracy: 0.1849\n",
            "Epoch 371/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.1462 - accuracy: 0.4794 - val_loss: 11.0282 - val_accuracy: 0.1876\n",
            "Epoch 372/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.1426 - accuracy: 0.4798 - val_loss: 11.0540 - val_accuracy: 0.1869\n",
            "Epoch 373/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.1419 - accuracy: 0.4811 - val_loss: 11.0679 - val_accuracy: 0.1869\n",
            "Epoch 374/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.1398 - accuracy: 0.4814 - val_loss: 11.1197 - val_accuracy: 0.1854\n",
            "Epoch 375/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 2.1330 - accuracy: 0.4807 - val_loss: 11.0955 - val_accuracy: 0.1871\n",
            "Epoch 376/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 2.1259 - accuracy: 0.4830 - val_loss: 11.0325 - val_accuracy: 0.1890\n",
            "Epoch 377/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 2.1224 - accuracy: 0.4837 - val_loss: 11.1480 - val_accuracy: 0.1895\n",
            "Epoch 378/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.1195 - accuracy: 0.4830 - val_loss: 11.0306 - val_accuracy: 0.1878\n",
            "Epoch 379/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.1186 - accuracy: 0.4834 - val_loss: 11.1398 - val_accuracy: 0.1880\n",
            "Epoch 380/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.1173 - accuracy: 0.4852 - val_loss: 11.1194 - val_accuracy: 0.1888\n",
            "Epoch 381/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.1129 - accuracy: 0.4856 - val_loss: 11.0512 - val_accuracy: 0.1874\n",
            "Epoch 382/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.1119 - accuracy: 0.4850 - val_loss: 11.1152 - val_accuracy: 0.1888\n",
            "Epoch 383/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.1031 - accuracy: 0.4869 - val_loss: 11.1561 - val_accuracy: 0.1872\n",
            "Epoch 384/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.1082 - accuracy: 0.4860 - val_loss: 11.1066 - val_accuracy: 0.1883\n",
            "Epoch 385/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.1051 - accuracy: 0.4874 - val_loss: 11.2096 - val_accuracy: 0.1882\n",
            "Epoch 386/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.0992 - accuracy: 0.4876 - val_loss: 11.1707 - val_accuracy: 0.1891\n",
            "Epoch 387/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.0993 - accuracy: 0.4872 - val_loss: 11.1577 - val_accuracy: 0.1873\n",
            "Epoch 388/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0954 - accuracy: 0.4886 - val_loss: 11.1636 - val_accuracy: 0.1879\n",
            "Epoch 389/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0903 - accuracy: 0.4896 - val_loss: 11.2105 - val_accuracy: 0.1880\n",
            "Epoch 390/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.0926 - accuracy: 0.4885 - val_loss: 11.1673 - val_accuracy: 0.1874\n",
            "Epoch 391/500\n",
            "73/73 [==============================] - 17s 228ms/step - loss: 2.0890 - accuracy: 0.4891 - val_loss: 11.2180 - val_accuracy: 0.1887\n",
            "Epoch 392/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.0905 - accuracy: 0.4886 - val_loss: 11.1987 - val_accuracy: 0.1878\n",
            "Epoch 393/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0805 - accuracy: 0.4905 - val_loss: 11.2602 - val_accuracy: 0.1883\n",
            "Epoch 394/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0827 - accuracy: 0.4910 - val_loss: 11.2287 - val_accuracy: 0.1877\n",
            "Epoch 395/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0811 - accuracy: 0.4906 - val_loss: 11.1583 - val_accuracy: 0.1875\n",
            "Epoch 396/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0800 - accuracy: 0.4913 - val_loss: 11.2145 - val_accuracy: 0.1879\n",
            "Epoch 397/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 2.0818 - accuracy: 0.4919 - val_loss: 11.2272 - val_accuracy: 0.1909\n",
            "Epoch 398/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.0717 - accuracy: 0.4938 - val_loss: 11.1819 - val_accuracy: 0.1902\n",
            "Epoch 399/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.0703 - accuracy: 0.4924 - val_loss: 11.1945 - val_accuracy: 0.1894\n",
            "Epoch 400/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.0665 - accuracy: 0.4942 - val_loss: 11.2720 - val_accuracy: 0.1904\n",
            "Epoch 401/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.0705 - accuracy: 0.4920 - val_loss: 11.2868 - val_accuracy: 0.1902\n",
            "Epoch 402/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.0560 - accuracy: 0.4970 - val_loss: 11.2425 - val_accuracy: 0.1879\n",
            "Epoch 403/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 2.0527 - accuracy: 0.4975 - val_loss: 11.3106 - val_accuracy: 0.1898\n",
            "Epoch 404/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.0504 - accuracy: 0.4973 - val_loss: 11.2835 - val_accuracy: 0.1901\n",
            "Epoch 405/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.0481 - accuracy: 0.4977 - val_loss: 11.3343 - val_accuracy: 0.1892\n",
            "Epoch 406/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.0489 - accuracy: 0.4981 - val_loss: 11.2618 - val_accuracy: 0.1895\n",
            "Epoch 407/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 2.0480 - accuracy: 0.4979 - val_loss: 11.3974 - val_accuracy: 0.1900\n",
            "Epoch 408/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0458 - accuracy: 0.4967 - val_loss: 11.3285 - val_accuracy: 0.1878\n",
            "Epoch 409/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.0431 - accuracy: 0.4982 - val_loss: 11.3239 - val_accuracy: 0.1904\n",
            "Epoch 410/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0411 - accuracy: 0.4987 - val_loss: 11.3598 - val_accuracy: 0.1893\n",
            "Epoch 411/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.0330 - accuracy: 0.4990 - val_loss: 11.2880 - val_accuracy: 0.1890\n",
            "Epoch 412/500\n",
            "73/73 [==============================] - 17s 237ms/step - loss: 2.0325 - accuracy: 0.5002 - val_loss: 11.3203 - val_accuracy: 0.1910\n",
            "Epoch 413/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.0309 - accuracy: 0.5005 - val_loss: 11.3688 - val_accuracy: 0.1907\n",
            "Epoch 414/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.0391 - accuracy: 0.4984 - val_loss: 11.3281 - val_accuracy: 0.1908\n",
            "Epoch 415/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0241 - accuracy: 0.5022 - val_loss: 11.3892 - val_accuracy: 0.1890\n",
            "Epoch 416/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 2.0277 - accuracy: 0.5005 - val_loss: 11.3550 - val_accuracy: 0.1912\n",
            "Epoch 417/500\n",
            "73/73 [==============================] - 17s 234ms/step - loss: 2.0183 - accuracy: 0.5047 - val_loss: 11.4298 - val_accuracy: 0.1918\n",
            "Epoch 418/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 2.0245 - accuracy: 0.5015 - val_loss: 11.4068 - val_accuracy: 0.1897\n",
            "Epoch 419/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.0203 - accuracy: 0.5031 - val_loss: 11.3414 - val_accuracy: 0.1896\n",
            "Epoch 420/500\n",
            "73/73 [==============================] - 17s 233ms/step - loss: 2.0141 - accuracy: 0.5038 - val_loss: 11.3427 - val_accuracy: 0.1923\n",
            "Epoch 421/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 2.0202 - accuracy: 0.5016 - val_loss: 11.3840 - val_accuracy: 0.1907\n",
            "Epoch 422/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.0086 - accuracy: 0.5056 - val_loss: 11.3289 - val_accuracy: 0.1910\n",
            "Epoch 423/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 2.0094 - accuracy: 0.5039 - val_loss: 11.4626 - val_accuracy: 0.1902\n",
            "Epoch 424/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 2.0029 - accuracy: 0.5047 - val_loss: 11.3554 - val_accuracy: 0.1917\n",
            "Epoch 425/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 2.0041 - accuracy: 0.5064 - val_loss: 11.4085 - val_accuracy: 0.1910\n",
            "Epoch 426/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 2.0036 - accuracy: 0.5072 - val_loss: 11.4199 - val_accuracy: 0.1914\n",
            "Epoch 427/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 1.9935 - accuracy: 0.5079 - val_loss: 11.5090 - val_accuracy: 0.1905\n",
            "Epoch 428/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9982 - accuracy: 0.5069 - val_loss: 11.4806 - val_accuracy: 0.1914\n",
            "Epoch 429/500\n",
            "73/73 [==============================] - 17s 228ms/step - loss: 1.9930 - accuracy: 0.5069 - val_loss: 11.4751 - val_accuracy: 0.1904\n",
            "Epoch 430/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 1.9932 - accuracy: 0.5081 - val_loss: 11.4161 - val_accuracy: 0.1915\n",
            "Epoch 431/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 1.9973 - accuracy: 0.5073 - val_loss: 11.4344 - val_accuracy: 0.1906\n",
            "Epoch 432/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 1.9849 - accuracy: 0.5095 - val_loss: 11.4846 - val_accuracy: 0.1892\n",
            "Epoch 433/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 1.9795 - accuracy: 0.5103 - val_loss: 11.6009 - val_accuracy: 0.1893\n",
            "Epoch 434/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9860 - accuracy: 0.5092 - val_loss: 11.4726 - val_accuracy: 0.1922\n",
            "Epoch 435/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9803 - accuracy: 0.5103 - val_loss: 11.4719 - val_accuracy: 0.1912\n",
            "Epoch 436/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 1.9820 - accuracy: 0.5099 - val_loss: 11.5235 - val_accuracy: 0.1900\n",
            "Epoch 437/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 1.9744 - accuracy: 0.5104 - val_loss: 11.4898 - val_accuracy: 0.1901\n",
            "Epoch 438/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 1.9680 - accuracy: 0.5128 - val_loss: 11.4531 - val_accuracy: 0.1914\n",
            "Epoch 439/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 1.9766 - accuracy: 0.5117 - val_loss: 11.4304 - val_accuracy: 0.1924\n",
            "Epoch 440/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 1.9682 - accuracy: 0.5118 - val_loss: 11.4860 - val_accuracy: 0.1908\n",
            "Epoch 441/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 1.9631 - accuracy: 0.5121 - val_loss: 11.5383 - val_accuracy: 0.1918\n",
            "Epoch 442/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 1.9680 - accuracy: 0.5126 - val_loss: 11.5360 - val_accuracy: 0.1909\n",
            "Epoch 443/500\n",
            "73/73 [==============================] - 16s 219ms/step - loss: 1.9576 - accuracy: 0.5150 - val_loss: 11.5470 - val_accuracy: 0.1913\n",
            "Epoch 444/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9562 - accuracy: 0.5146 - val_loss: 11.5431 - val_accuracy: 0.1917\n",
            "Epoch 445/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9616 - accuracy: 0.5139 - val_loss: 11.5055 - val_accuracy: 0.1918\n",
            "Epoch 446/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9565 - accuracy: 0.5155 - val_loss: 11.5766 - val_accuracy: 0.1899\n",
            "Epoch 447/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 1.9548 - accuracy: 0.5152 - val_loss: 11.5216 - val_accuracy: 0.1901\n",
            "Epoch 448/500\n",
            "73/73 [==============================] - 17s 226ms/step - loss: 1.9526 - accuracy: 0.5168 - val_loss: 11.5459 - val_accuracy: 0.1913\n",
            "Epoch 449/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 1.9484 - accuracy: 0.5167 - val_loss: 11.5161 - val_accuracy: 0.1908\n",
            "Epoch 450/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 1.9486 - accuracy: 0.5160 - val_loss: 11.5526 - val_accuracy: 0.1905\n",
            "Epoch 451/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 1.9470 - accuracy: 0.5176 - val_loss: 11.4758 - val_accuracy: 0.1898\n",
            "Epoch 452/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9424 - accuracy: 0.5168 - val_loss: 11.5950 - val_accuracy: 0.1916\n",
            "Epoch 453/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 1.9455 - accuracy: 0.5164 - val_loss: 11.5487 - val_accuracy: 0.1905\n",
            "Epoch 454/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9347 - accuracy: 0.5202 - val_loss: 11.6325 - val_accuracy: 0.1915\n",
            "Epoch 455/500\n",
            "73/73 [==============================] - 17s 232ms/step - loss: 1.9445 - accuracy: 0.5164 - val_loss: 11.6196 - val_accuracy: 0.1928\n",
            "Epoch 456/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 1.9373 - accuracy: 0.5184 - val_loss: 11.5753 - val_accuracy: 0.1908\n",
            "Epoch 457/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 1.9361 - accuracy: 0.5185 - val_loss: 11.5525 - val_accuracy: 0.1919\n",
            "Epoch 458/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 1.9316 - accuracy: 0.5199 - val_loss: 11.5514 - val_accuracy: 0.1922\n",
            "Epoch 459/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 1.9292 - accuracy: 0.5197 - val_loss: 11.6151 - val_accuracy: 0.1928\n",
            "Epoch 460/500\n",
            "73/73 [==============================] - 17s 231ms/step - loss: 1.9232 - accuracy: 0.5210 - val_loss: 11.6318 - val_accuracy: 0.1938\n",
            "Epoch 461/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9190 - accuracy: 0.5214 - val_loss: 11.6534 - val_accuracy: 0.1911\n",
            "Epoch 462/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9188 - accuracy: 0.5218 - val_loss: 11.6873 - val_accuracy: 0.1923\n",
            "Epoch 463/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9207 - accuracy: 0.5218 - val_loss: 11.6201 - val_accuracy: 0.1909\n",
            "Epoch 464/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 1.9213 - accuracy: 0.5235 - val_loss: 11.6166 - val_accuracy: 0.1920\n",
            "Epoch 465/500\n",
            "73/73 [==============================] - 16s 226ms/step - loss: 1.9158 - accuracy: 0.5240 - val_loss: 11.6162 - val_accuracy: 0.1922\n",
            "Epoch 466/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 1.9107 - accuracy: 0.5242 - val_loss: 11.6218 - val_accuracy: 0.1921\n",
            "Epoch 467/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 1.9094 - accuracy: 0.5244 - val_loss: 11.6433 - val_accuracy: 0.1926\n",
            "Epoch 468/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 1.9079 - accuracy: 0.5247 - val_loss: 11.6218 - val_accuracy: 0.1918\n",
            "Epoch 469/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 1.9069 - accuracy: 0.5247 - val_loss: 11.6812 - val_accuracy: 0.1919\n",
            "Epoch 470/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 1.9063 - accuracy: 0.5240 - val_loss: 11.6078 - val_accuracy: 0.1917\n",
            "Epoch 471/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 1.8979 - accuracy: 0.5269 - val_loss: 11.7495 - val_accuracy: 0.1936\n",
            "Epoch 472/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 1.8954 - accuracy: 0.5261 - val_loss: 11.6455 - val_accuracy: 0.1933\n",
            "Epoch 473/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9027 - accuracy: 0.5266 - val_loss: 11.6629 - val_accuracy: 0.1934\n",
            "Epoch 474/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.9016 - accuracy: 0.5247 - val_loss: 11.6076 - val_accuracy: 0.1938\n",
            "Epoch 475/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 1.8951 - accuracy: 0.5286 - val_loss: 11.7439 - val_accuracy: 0.1922\n",
            "Epoch 476/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 1.8977 - accuracy: 0.5253 - val_loss: 11.7458 - val_accuracy: 0.1912\n",
            "Epoch 477/500\n",
            "73/73 [==============================] - 16s 219ms/step - loss: 1.8969 - accuracy: 0.5261 - val_loss: 11.7800 - val_accuracy: 0.1926\n",
            "Epoch 478/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 1.8860 - accuracy: 0.5296 - val_loss: 11.6535 - val_accuracy: 0.1906\n",
            "Epoch 479/500\n",
            "73/73 [==============================] - 17s 227ms/step - loss: 1.8913 - accuracy: 0.5298 - val_loss: 11.7057 - val_accuracy: 0.1923\n",
            "Epoch 480/500\n",
            "73/73 [==============================] - 16s 222ms/step - loss: 1.8905 - accuracy: 0.5283 - val_loss: 11.7028 - val_accuracy: 0.1922\n",
            "Epoch 481/500\n",
            "73/73 [==============================] - 16s 220ms/step - loss: 1.8886 - accuracy: 0.5285 - val_loss: 11.6841 - val_accuracy: 0.1923\n",
            "Epoch 482/500\n",
            "73/73 [==============================] - 16s 223ms/step - loss: 1.8865 - accuracy: 0.5298 - val_loss: 11.5852 - val_accuracy: 0.1932\n",
            "Epoch 483/500\n",
            "73/73 [==============================] - 16s 221ms/step - loss: 1.8811 - accuracy: 0.5302 - val_loss: 11.6906 - val_accuracy: 0.1929\n",
            "Epoch 484/500\n",
            "73/73 [==============================] - 16s 225ms/step - loss: 1.8740 - accuracy: 0.5308 - val_loss: 11.7582 - val_accuracy: 0.1930\n",
            "Epoch 485/500\n",
            "73/73 [==============================] - 16s 224ms/step - loss: 1.8788 - accuracy: 0.5317 - val_loss: 11.7624 - val_accuracy: 0.1925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f671da050f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JOIgTU_-O-",
        "colab_type": "text"
      },
      "source": [
        "## V6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf8Xngj_75dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXlm2KmaA17M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-6.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD93EE9F7qMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "4077e2eb-12cb-4c30-9910-e601ccec4b56"
      },
      "source": [
        "train_utils.plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"dc2c2089-688c-4027-84aa-30c456c68a31\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"dc2c2089-688c-4027-84aa-30c456c68a31\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'dc2c2089-688c-4027-84aa-30c456c68a31',\n",
              "                        [{\"line\": {\"color\": \"royalblue\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"train_loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [7.458602428436279, 7.021402359008789, 7.009311676025391, 6.955985069274902, 6.8343939781188965, 6.712810039520264, 6.608242511749268, 6.514761447906494, 6.441170692443848, 6.371826171875, 6.300393581390381, 6.232043743133545, 6.170290470123291, 6.113407135009766, 6.059351444244385, 6.004700183868408, 5.949937343597412, 5.893865585327148, 5.840144634246826, 5.785127639770508, 5.73313570022583, 5.686136245727539, 5.643167018890381, 5.59910249710083, 5.556097507476807, 5.515835762023926, 5.475383281707764, 5.43580961227417, 5.395748615264893, 5.358993053436279, 5.318986415863037, 5.285747528076172, 5.251445293426514, 5.212973117828369, 5.178022384643555, 5.146390438079834, 5.111725807189941, 5.081413269042969, 5.049098491668701, 5.016397953033447, 4.985947608947754, 4.956323623657227, 4.93022346496582, 4.901480197906494, 4.873708724975586, 4.841719627380371, 4.813971996307373, 4.785449981689453, 4.76080846786499, 4.736237525939941, 4.708621501922607, 4.686616897583008, 4.657742977142334, 4.632593631744385, 4.607674598693848, 4.585359573364258, 4.560201168060303, 4.540639877319336, 4.51220703125, 4.492542266845703, 4.4723005294799805, 4.4511494636535645, 4.42878532409668, 4.413455963134766, 4.391733646392822, 4.369042873382568, 4.3505353927612305, 4.326739311218262, 4.311563968658447, 4.292220115661621, 4.269426345825195, 4.256741523742676, 4.238198757171631, 4.219151973724365, 4.20266056060791, 4.18494176864624, 4.169332027435303, 4.149455547332764, 4.137803554534912, 4.116767406463623, 4.100502967834473, 4.083449840545654, 4.0702009201049805, 4.059648036956787, 4.039373874664307, 4.027568340301514, 4.011825084686279, 3.9942076206207275, 3.9786579608917236, 3.9630703926086426, 3.953157663345337, 3.94500732421875, 3.930150032043457, 3.9062442779541016, 3.8975884914398193, 3.8874988555908203, 3.8704326152801514, 3.8597898483276367, 3.844069242477417, 3.8322761058807373, 3.821240186691284, 3.8114266395568848, 3.7916510105133057, 3.784296989440918, 3.768200397491455, 3.7584316730499268, 3.750084161758423, 3.733917236328125, 3.724353075027466, 3.716841220855713, 3.7006072998046875, 3.686579465866089, 3.67802095413208, 3.669113874435425, 3.6539947986602783, 3.643953323364258, 3.639620542526245, 3.631702423095703, 3.616957664489746, 3.609309196472168, 3.5965981483459473, 3.5904407501220703, 3.5754475593566895, 3.564770221710205, 3.5559475421905518, 3.5410876274108887, 3.54119610786438, 3.526963472366333, 3.5164248943328857, 3.504305601119995, 3.5012247562408447, 3.4923555850982666, 3.47900652885437, 3.474431037902832, 3.457871198654175, 3.4513890743255615, 3.4426355361938477, 3.437422752380371, 3.4223594665527344, 3.420081615447998, 3.404766321182251, 3.398977518081665, 3.389678716659546, 3.3810465335845947, 3.3723390102386475, 3.3669545650482178, 3.3583598136901855, 3.3477187156677246, 3.338785409927368, 3.33193302154541, 3.3255183696746826, 3.3179781436920166, 3.3103854656219482, 3.30106520652771, 3.2952380180358887, 3.2856810092926025, 3.2768495082855225, 3.268112897872925, 3.2588319778442383, 3.2528700828552246, 3.250032663345337, 3.2422804832458496, 3.231519937515259, 3.2216625213623047, 3.219912528991699, 3.2107439041137695, 3.1988728046417236, 3.204078197479248, 3.1879794597625732, 3.179272413253784, 3.1763107776641846, 3.1669631004333496, 3.1663520336151123, 3.156172275543213, 3.1521947383880615, 3.1415719985961914, 3.1325693130493164, 3.128201484680176, 3.1239748001098633, 3.1095027923583984, 3.107022762298584, 3.109335422515869, 3.0960733890533447, 3.0828840732574463, 3.0855002403259277, 3.0722806453704834, 3.0718817710876465, 3.060788631439209, 3.0544981956481934, 3.0475826263427734, 3.041982889175415, 3.0337958335876465, 3.0354974269866943, 3.026292562484741, 3.0212693214416504, 3.012688398361206, 3.0056071281433105, 2.999680995941162, 2.9940104484558105, 2.9915013313293457, 2.9830503463745117, 2.9763174057006836, 2.9720370769500732, 2.9659273624420166, 2.957529067993164, 2.9552230834960938, 2.952772617340088, 2.943516492843628, 2.943812370300293, 2.927532434463501, 2.9208385944366455, 2.9204330444335938, 2.920297622680664, 2.9069883823394775, 2.9014029502868652, 2.899669885635376, 2.887300729751587, 2.887101411819458, 2.878067970275879, 2.8732335567474365, 2.872183322906494, 2.869094133377075, 2.8626792430877686, 2.8570947647094727, 2.8543055057525635, 2.8468871116638184, 2.835191249847412, 2.8365352153778076, 2.8314614295959473, 2.8236448764801025, 2.818066120147705, 2.8167624473571777, 2.8123903274536133, 2.8090639114379883, 2.8043978214263916, 2.798945665359497, 2.796839714050293, 2.7929646968841553, 2.7829573154449463, 2.778717517852783, 2.7744243144989014, 2.7651422023773193, 2.7624804973602295, 2.75708270072937, 2.7479960918426514, 2.7508907318115234, 2.742372751235962, 2.739006280899048, 2.7318272590637207, 2.73349928855896, 2.7258694171905518, 2.720759868621826, 2.7181174755096436, 2.71685528755188, 2.7080912590026855, 2.6990468502044678, 2.6948909759521484, 2.6980807781219482, 2.6961491107940674, 2.685594081878662, 2.6833837032318115, 2.67678165435791, 2.669595956802368, 2.6717209815979004, 2.666579008102417, 2.6554994583129883, 2.6524298191070557, 2.6517951488494873, 2.6420364379882812, 2.6392295360565186, 2.6405091285705566, 2.6317214965820312, 2.6298704147338867, 2.6316287517547607, 2.6189308166503906, 2.6266093254089355, 2.6110169887542725, 2.613149881362915, 2.6061320304870605, 2.6067609786987305, 2.5941669940948486, 2.596419095993042, 2.593614339828491, 2.584463596343994, 2.580822229385376, 2.580596446990967, 2.5706098079681396, 2.5698089599609375, 2.5620675086975098, 2.563743829727173, 2.5594749450683594, 2.553781270980835, 2.5495808124542236, 2.5503041744232178, 2.5479135513305664, 2.541914701461792, 2.540370225906372, 2.5333597660064697, 2.5280609130859375, 2.527148723602295, 2.5240111351013184, 2.5221266746520996, 2.517972469329834, 2.510897636413574, 2.5106399059295654, 2.4961678981781006, 2.50069260597229, 2.4992992877960205, 2.4951581954956055, 2.4896724224090576, 2.4896066188812256, 2.484933614730835, 2.4841721057891846, 2.4836010932922363, 2.480484962463379, 2.4681806564331055, 2.461735725402832, 2.4697868824005127, 2.4554059505462646, 2.4540064334869385, 2.447173595428467, 2.451136350631714, 2.4495129585266113, 2.447692632675171, 2.440633535385132, 2.43415904045105, 2.432251214981079, 2.4319276809692383, 2.4283690452575684, 2.428084135055542, 2.423860788345337, 2.4220213890075684, 2.4219110012054443, 2.41408634185791, 2.407965660095215, 2.404655694961548, 2.408109188079834, 2.4001846313476562, 2.4002151489257812, 2.394381523132324, 2.3932816982269287, 2.3838207721710205, 2.3845086097717285, 2.381629228591919, 2.3772876262664795, 2.380197286605835, 2.3700618743896484, 2.361698865890503, 2.3691210746765137, 2.354217290878296, 2.3582186698913574, 2.3483285903930664, 2.34909725189209, 2.3508801460266113, 2.352296829223633, 2.3456146717071533, 2.343899726867676, 2.3429369926452637, 2.3420512676239014, 2.334465503692627, 2.3293683528900146, 2.332944631576538, 2.3179967403411865, 2.322411298751831, 2.3163552284240723, 2.31400465965271, 2.31575345993042, 2.308544397354126, 2.3045167922973633, 2.304391860961914, 2.3031654357910156, 2.2948250770568848, 2.3007946014404297, 2.2943413257598877, 2.299194097518921, 2.2900660037994385, 2.2869091033935547, 2.280423879623413, 2.2785377502441406, 2.276789665222168, 2.279853582382202, 2.275507688522339, 2.2724356651306152, 2.269101142883301, 2.2687745094299316, 2.2627599239349365, 2.2564404010772705, 2.2533669471740723, 2.256253242492676, 2.2519936561584473, 2.252096652984619, 2.25284481048584, 2.2436461448669434, 2.2367539405822754, 2.241771936416626, 2.2350378036499023, 2.2347187995910645, 2.2325217723846436, 2.234553813934326, 2.228029251098633, 2.220820188522339, 2.2201356887817383, 2.2250750064849854, 2.2170023918151855, 2.214937210083008, 2.2161030769348145, 2.2099266052246094, 2.2059361934661865, 2.2041614055633545, 2.197805643081665, 2.1955502033233643, 2.201632022857666, 2.2009034156799316, 2.1934211254119873, 2.1948812007904053, 2.184741497039795, 2.1895227432250977, 2.1845178604125977, 2.1759610176086426, 2.176597833633423, 2.1796576976776123, 2.1687614917755127, 2.1743369102478027, 2.1618545055389404, 2.1677160263061523, 2.1629109382629395, 2.157944679260254, 2.1609809398651123, 2.1588261127471924, 2.1591691970825195, 2.1612415313720703, 2.148361921310425, 2.1509604454040527, 2.1545166969299316, 2.143122673034668, 2.1410958766937256, 2.141697883605957, 2.1300461292266846, 2.1318600177764893, 2.128767251968384, 2.1309731006622314, 2.1296863555908203, 2.127523422241211, 2.1259565353393555, 2.1223254203796387, 2.115030527114868, 2.119023561477661, 2.116718292236328, 2.1130118370056152, 2.114192247390747, 2.108396530151367, 2.10859751701355, 2.106961965560913, 2.0974416732788086, 2.1060073375701904, 2.0964243412017822, 2.0923948287963867, 2.0999016761779785, 2.093358278274536, 2.0858376026153564, 2.087390422821045, 2.084538698196411, 2.0864334106445312, 2.079694986343384, 2.081512928009033, 2.0752129554748535, 2.08119797706604, 2.0702297687530518, 2.0719404220581055, 2.0688765048980713, 2.0731663703918457, 2.0700695514678955, 2.0657315254211426, 2.058476686477661, 2.0574235916137695, 2.0610194206237793, 2.0544745922088623, 2.052564859390259, 2.058309555053711, 2.048804759979248, 2.053652286529541, 2.041518449783325, 2.0463902950286865, 2.039344310760498, 2.0366008281707764, 2.0353167057037354, 2.038710117340088, 2.0290632247924805, 2.034236431121826, 2.033118486404419, 2.0306506156921387, 2.027523994445801, 2.029937982559204, 2.0225539207458496, 2.024111270904541, 2.0212676525115967, 2.018556594848633, 2.0204620361328125, 2.017644166946411, 2.0152645111083984]}, {\"line\": {\"color\": \"firebrick\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"train_accuracy\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"xaxis\": \"x\", \"y\": [0.0541599839925766, 0.05779228359460831, 0.05779852718114853, 0.0578172504901886, 0.0645139142870903, 0.07083611935377121, 0.07786355912685394, 0.08414207398891449, 0.08962173014879227, 0.09450848400592804, 0.09875865280628204, 0.10367037355899811, 0.10773954540491104, 0.11044817417860031, 0.1136186346411705, 0.11635845899581909, 0.11837432533502579, 0.12124522030353546, 0.12284917384386063, 0.12506474554538727, 0.1273365020751953, 0.12904655933380127, 0.13121220469474792, 0.1330907642841339, 0.13485074043273926, 0.13576817512512207, 0.13764050602912903, 0.13923820853233337, 0.1411791890859604, 0.14272072911262512, 0.14338228106498718, 0.14515474438667297, 0.14648409187793732, 0.1483376920223236, 0.14953598380088806, 0.15018504858016968, 0.15170162916183472, 0.15447266399860382, 0.15454131364822388, 0.155702143907547, 0.15698781609535217, 0.1587165892124176, 0.15924707055091858, 0.16103202104568481, 0.16219286620616913, 0.1639029085636139, 0.16659905016422272, 0.16744160652160645, 0.16827790439128876, 0.16945746541023254, 0.1713859587907791, 0.17171673476696014, 0.17364521324634552, 0.1752803772687912, 0.1763538420200348, 0.1779453158378601, 0.17909367382526398, 0.18139038980007172, 0.18241392076015472, 0.18423007428646088, 0.18559686839580536, 0.18609614670276642, 0.18806833028793335, 0.1887548416852951, 0.19060219824314117, 0.1924121081829071, 0.19377265870571136, 0.19562625885009766, 0.1970929056406021, 0.19883416593074799, 0.19952069222927094, 0.20134307444095612, 0.20247894525527954, 0.20446985960006714, 0.20503778755664825, 0.20734699070453644, 0.20800229907035828, 0.20974980294704437, 0.21161587536334991, 0.21383145451545715, 0.21460534632205963, 0.216696098446846, 0.21679596602916718, 0.21816898882389069, 0.22108358144760132, 0.22125832736492157, 0.2232055366039276, 0.22429148852825165, 0.22633855044841766, 0.22724974155426025, 0.22884121537208557, 0.22905965149402618, 0.23065736889839172, 0.23375917971134186, 0.23399010300636292, 0.2360808551311493, 0.2375599890947342, 0.2381778508424759, 0.24095512926578522, 0.2411423623561859, 0.24287737905979156, 0.2432643324136734, 0.24540501832962036, 0.24626627564430237, 0.2474333643913269, 0.24871277809143066, 0.2504790127277374, 0.2525697648525238, 0.25258225202560425, 0.25417372584342957, 0.25564661622047424, 0.2575688660144806, 0.25863608717918396, 0.260520875453949, 0.2619188725948334, 0.26328566670417786, 0.26449644565582275, 0.2626241147518158, 0.26522040367126465, 0.26653727889060974, 0.2687341272830963, 0.2685968279838562, 0.27151140570640564, 0.27336499094963074, 0.27242258191108704, 0.2752123475074768, 0.27509376406669617, 0.2755680978298187, 0.27858254313468933, 0.27923783659935, 0.2786199748516083, 0.28127244114875793, 0.2823334038257599, 0.2830074429512024, 0.28568485379219055, 0.28665223717689514, 0.28694555163383484, 0.2884184420108795, 0.29057785868644714, 0.29015347361564636, 0.29294323921203613, 0.29165756702423096, 0.29429754614830017, 0.295545756816864, 0.2984790503978729, 0.29732444882392883, 0.29741182923316956, 0.29980841279029846, 0.30133122205734253, 0.30106285214424133, 0.3028790056705475, 0.3036404252052307, 0.30362793803215027, 0.3058185577392578, 0.3073538541793823, 0.3087456226348877, 0.30862703919410706, 0.30980658531188965, 0.3106616139411926, 0.31209081411361694, 0.3127586245536804, 0.3134576082229614, 0.3164907693862915, 0.3174331784248352, 0.3164595663547516, 0.31737077236175537, 0.32045385241508484, 0.31871259212493896, 0.3201417922973633, 0.3216833472251892, 0.3225383758544922, 0.32280048727989197, 0.3240424692630768, 0.32656386494636536, 0.32553407549858093, 0.3271068334579468, 0.3286171555519104, 0.32924750447273254, 0.3300651013851166, 0.33253031969070435, 0.3334040641784668, 0.33323556184768677, 0.33427780866622925, 0.3356570899486542, 0.33639976382255554, 0.33715495467185974, 0.3374670147895813, 0.3392644226551056, 0.34051263332366943, 0.3404689431190491, 0.34261587262153625, 0.3427157402038574, 0.3416547477245331, 0.34538692235946655, 0.3444632291793823, 0.3474963903427124, 0.34747767448425293, 0.3484575152397156, 0.34944984316825867, 0.3494373559951782, 0.35025495290756226, 0.35143449902534485, 0.353737473487854, 0.3531508147716522, 0.3542117774486542, 0.3554038405418396, 0.3552727699279785, 0.35581573843955994, 0.3579813838005066, 0.3592483401298523, 0.3606463372707367, 0.3612954020500183, 0.36030930280685425, 0.36278077960014343, 0.36332374811172485, 0.3632800579071045, 0.36574527621269226, 0.36542072892189026, 0.3663007318973541, 0.3664068281650543, 0.36787348985671997, 0.3679858148097992, 0.3693276643753052, 0.3680731952190399, 0.3709503412246704, 0.3725043535232544, 0.3720799684524536, 0.3717554211616516, 0.37443283200263977, 0.3751380741596222, 0.3767233192920685, 0.37574347853660583, 0.37730371952056885, 0.37679195404052734, 0.37720388174057007, 0.37975022196769714, 0.3793882429599762, 0.38088610768318176, 0.3824900686740875, 0.3819658160209656, 0.38318905234336853, 0.3838880658149719, 0.3844497501850128, 0.3857167065143585, 0.3868962526321411, 0.38655298948287964, 0.388637512922287, 0.3876951038837433, 0.3900729715824127, 0.3896111249923706, 0.388806015253067, 0.39050358533859253, 0.3926505148410797, 0.3917580544948578, 0.3934306502342224, 0.3935367465019226, 0.3959770202636719, 0.3956587016582489, 0.39658862352371216, 0.3974062204360962, 0.397144079208374, 0.3978118896484375, 0.39940959215164185, 0.39943456649780273, 0.39968419075012207, 0.4017874300479889, 0.4022991955280304, 0.4012756645679474, 0.4031604826450348, 0.40432754158973694, 0.4050390422344208, 0.40538230538368225, 0.40599390864372253, 0.4057754874229431, 0.4077601432800293, 0.4071485102176666, 0.40780383348464966, 0.4092455208301544, 0.409401535987854, 0.4097760021686554, 0.4134270250797272, 0.41277173161506653, 0.4110679030418396, 0.4142758250236511, 0.41382020711898804, 0.4131337106227875, 0.4146253168582916, 0.4154990613460541, 0.41732147336006165, 0.41847604513168335, 0.4173027276992798, 0.4189004600048065, 0.4181889593601227, 0.4202422797679901, 0.4192998707294464, 0.4219648241996765, 0.4206417202949524, 0.4223829507827759, 0.42361870408058167, 0.42216452956199646, 0.42401811480522156, 0.4249979853630066, 0.4239557087421417, 0.4256220757961273, 0.42725101113319397, 0.4295476973056793, 0.42767539620399475, 0.4278251826763153, 0.429628849029541, 0.42915451526641846, 0.43006572127342224, 0.43087083101272583, 0.42972245812416077, 0.4296225905418396, 0.43103307485580444, 0.4329615831375122, 0.43375417590141296, 0.43163222074508667, 0.4365564286708832, 0.4353768527507782, 0.4367998242378235, 0.43699952960014343, 0.4363442361354828, 0.4356764256954193, 0.4375612437725067, 0.43915271759033203, 0.4390091598033905, 0.4396519958972931, 0.4407878816127777, 0.4402698576450348, 0.43995779752731323, 0.4418051540851593, 0.44232940673828125, 0.44324684143066406, 0.4441767632961273, 0.4453750550746918, 0.44461989402770996, 0.4444701075553894, 0.4459554851055145, 0.44568711519241333, 0.4464547634124756, 0.44802752137184143, 0.44768425822257996, 0.4482022523880005, 0.4484955966472626, 0.4489574432373047, 0.45026180148124695, 0.4520280361175537, 0.4498249292373657, 0.4541187882423401, 0.45279568433761597, 0.4532824754714966, 0.4544932544231415, 0.45383793115615845, 0.4532325565814972, 0.4546118378639221, 0.45664018392562866, 0.45631563663482666, 0.4550924003124237, 0.4574265480041504, 0.4574640095233917, 0.45537325739860535, 0.4590242803096771, 0.4579695165157318, 0.4593987464904785, 0.46039730310440063, 0.4611150324344635, 0.4620511829853058, 0.46137091517448425, 0.46206989884376526, 0.46265658736228943, 0.4647223651409149, 0.46357402205467224, 0.4628937244415283, 0.46300607919692993, 0.46443527936935425, 0.46357402205467224, 0.46822986006736755, 0.46717509627342224, 0.466731995344162, 0.46645113825798035, 0.46657595038414, 0.4687977731227875, 0.46779295802116394, 0.46949678659439087, 0.46954047679901123, 0.4705016016960144, 0.46989619731903076, 0.4707637131214142, 0.47185590863227844, 0.46979010105133057, 0.47167491912841797, 0.473022997379303, 0.47487035393714905, 0.47252368927001953, 0.4749452471733093, 0.4740215539932251, 0.47412142157554626, 0.47397786378860474, 0.4761996865272522, 0.4772981107234955, 0.477279394865036, 0.4765491783618927, 0.47761017084121704, 0.4777037799358368, 0.47756025195121765, 0.47973838448524475, 0.4799380898475647, 0.4807431995868683, 0.4823783338069916, 0.48210999369621277, 0.4807494282722473, 0.4811488687992096, 0.48229098320007324, 0.4826467037200928, 0.48360782861709595, 0.483245849609375, 0.48328953981399536, 0.48498088121414185, 0.4841570556163788, 0.483938604593277, 0.48630398511886597, 0.48556753993034363, 0.4873899221420288, 0.4865598678588867, 0.48796409368515015, 0.48780182003974915, 0.4879578649997711, 0.488519549369812, 0.48809516429901123, 0.4881700575351715, 0.4912032186985016, 0.48931217193603516, 0.4886007010936737, 0.49066025018692017, 0.4916151165962219, 0.49162137508392334, 0.49328774213790894, 0.4924951195716858, 0.4955906867980957, 0.4936809241771698, 0.493793249130249, 0.494442343711853, 0.49484801292419434, 0.49582159519195557, 0.49737563729286194, 0.4948916733264923, 0.4962584674358368, 0.4972071349620819, 0.49522244930267334, 0.4976939260959625, 0.49799349904060364, 0.4980309307575226, 0.4987673759460449, 0.49928539991378784, 0.5002776980400085, 0.5001591444015503, 0.4989047050476074, 0.5019253492355347, 0.500933051109314, 0.5020189881324768, 0.5015820860862732, 0.5023372769355774, 0.5038164258003235, 0.5035542845726013, 0.5042282938957214, 0.5025744438171387, 0.5066685676574707, 0.5043219327926636, 0.5047088861465454, 0.5030050873756409, 0.5049210786819458, 0.505233108997345, 0.5060444474220276, 0.5073738098144531, 0.507604718208313, 0.5079230070114136, 0.5082350969314575, 0.5075298547744751, 0.5096330642700195, 0.5079042911529541, 0.5109749436378479, 0.5108688473701477, 0.5107439756393433, 0.5106441378593445, 0.5122168660163879, 0.5119172930717468, 0.5130032896995544, 0.5115865468978882, 0.5120670795440674, 0.5143575668334961, 0.5131655335426331, 0.5122855305671692, 0.5125726461410522, 0.513359010219574, 0.5153873562812805, 0.5146009922027588, 0.5140954256057739, 0.5155371427536011, 0.51540607213974], \"yaxis\": \"y2\"}, {\"line\": {\"color\": \"royalblue\", \"dash\": \"dot\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"validation_loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [7.0811052322387695, 7.101848602294922, 7.090435981750488, 7.001238822937012, 6.886741638183594, 6.814002513885498, 6.741083145141602, 6.700465679168701, 6.6743316650390625, 6.641850471496582, 6.620259761810303, 6.593625068664551, 6.589068412780762, 6.578842639923096, 6.558661937713623, 6.549444675445557, 6.537961959838867, 6.5289506912231445, 6.517361640930176, 6.5008158683776855, 6.494589328765869, 6.5000691413879395, 6.493002414703369, 6.504294395446777, 6.5040154457092285, 6.504066467285156, 6.505837440490723, 6.513614177703857, 6.524117946624756, 6.528754711151123, 6.533158779144287, 6.53879976272583, 6.550942420959473, 6.569360256195068, 6.568551540374756, 6.577001571655273, 6.596731662750244, 6.602609157562256, 6.616481304168701, 6.630683898925781, 6.641595363616943, 6.642998218536377, 6.65492057800293, 6.673959255218506, 6.6845855712890625, 6.693271160125732, 6.723233222961426, 6.723042964935303, 6.735398292541504, 6.756357192993164, 6.756956100463867, 6.7870869636535645, 6.791988372802734, 6.813998699188232, 6.819335460662842, 6.841073989868164, 6.853143215179443, 6.858806133270264, 6.877065658569336, 6.890848159790039, 6.906373977661133, 6.9317545890808105, 6.93049430847168, 6.944567680358887, 6.974958896636963, 6.98774528503418, 7.0010857582092285, 7.020697116851807, 7.033700466156006, 7.0398454666137695, 7.05448579788208, 7.063262939453125, 7.078580379486084, 7.105457305908203, 7.130711078643799, 7.128904342651367, 7.146759986877441, 7.168460369110107, 7.1587371826171875, 7.198734283447266, 7.221553802490234, 7.219066143035889, 7.239317893981934, 7.244234085083008, 7.274664878845215, 7.272515773773193, 7.283438205718994, 7.2960357666015625, 7.329097747802734, 7.3549299240112305, 7.370049953460693, 7.3714599609375, 7.3922014236450195, 7.388037204742432, 7.429225921630859, 7.4100661277771, 7.434469223022461, 7.465261936187744, 7.485400199890137, 7.483192443847656, 7.49373722076416, 7.506464004516602, 7.536664962768555, 7.545938014984131, 7.562707424163818, 7.571969032287598, 7.596616268157959, 7.625345706939697, 7.601678371429443, 7.63006591796875, 7.651721477508545, 7.6534743309021, 7.692101001739502, 7.675325393676758, 7.701205730438232, 7.708262920379639, 7.715651988983154, 7.722330570220947, 7.752479076385498, 7.760960578918457, 7.776294708251953, 7.795327186584473, 7.798788547515869, 7.835936546325684, 7.829690933227539, 7.852116584777832, 7.875899314880371, 7.889072418212891, 7.887316703796387, 7.883529186248779, 7.9167962074279785, 7.944774627685547, 7.943746566772461, 7.945460319519043, 7.948983192443848, 7.991881370544434, 7.969258785247803, 8.028912544250488, 8.011883735656738, 8.017423629760742, 8.041860580444336, 8.071242332458496, 8.081646919250488, 8.058266639709473, 8.115095138549805, 8.113784790039062, 8.12731647491455, 8.14806842803955, 8.153298377990723, 8.143675804138184, 8.154038429260254, 8.157585144042969, 8.223134994506836, 8.203485488891602, 8.198715209960938, 8.22368049621582, 8.26600456237793, 8.288798332214355, 8.26154899597168, 8.259553909301758, 8.280402183532715, 8.290225982666016, 8.315468788146973, 8.329297065734863, 8.334005355834961, 8.346994400024414, 8.327754974365234, 8.3567533493042, 8.384794235229492, 8.396097183227539, 8.423657417297363, 8.40749454498291, 8.436782836914062, 8.436640739440918, 8.455733299255371, 8.460367202758789, 8.4896879196167, 8.473045349121094, 8.5108642578125, 8.54369831085205, 8.488730430603027, 8.53065299987793, 8.525118827819824, 8.557458877563477, 8.58045482635498, 8.57884693145752, 8.579798698425293, 8.604087829589844, 8.601811408996582, 8.62378978729248, 8.622810363769531, 8.641777038574219, 8.64533805847168, 8.65832233428955, 8.683515548706055, 8.691615104675293, 8.754294395446777, 8.668593406677246, 8.712483406066895, 8.71349811553955, 8.722867965698242, 8.726900100708008, 8.746829986572266, 8.767716407775879, 8.787223815917969, 8.769930839538574, 8.798505783081055, 8.807829856872559, 8.799479484558105, 8.810611724853516, 8.834456443786621, 8.814477920532227, 8.830406188964844, 8.826130867004395, 8.867487907409668, 8.923562049865723, 8.8707914352417, 8.890682220458984, 8.918107032775879, 8.932426452636719, 8.932161331176758, 8.956990242004395, 8.923675537109375, 8.926551818847656, 8.928370475769043, 8.962406158447266, 9.01309871673584, 8.975807189941406, 9.013347625732422, 9.0341157913208, 8.972091674804688, 9.007434844970703, 9.026371955871582, 9.042062759399414, 9.048633575439453, 9.028782844543457, 9.02031421661377, 9.03646469116211, 9.08127498626709, 9.08030891418457, 9.09367561340332, 9.134988784790039, 9.103424072265625, 9.138724327087402, 9.099093437194824, 9.148544311523438, 9.121636390686035, 9.139052391052246, 9.174615859985352, 9.154459953308105, 9.182940483093262, 9.19161319732666, 9.176226615905762, 9.191561698913574, 9.189057350158691, 9.243419647216797, 9.255827903747559, 9.227087020874023, 9.200124740600586, 9.278923034667969, 9.262886047363281, 9.245074272155762, 9.294384956359863, 9.316097259521484, 9.320127487182617, 9.271126747131348, 9.268879890441895, 9.312228202819824, 9.32699966430664, 9.30872631072998, 9.277027130126953, 9.315185546875, 9.353362083435059, 9.345064163208008, 9.292256355285645, 9.399405479431152, 9.392840385437012, 9.387420654296875, 9.384929656982422, 9.436405181884766, 9.38337230682373, 9.37002182006836, 9.40088939666748, 9.454127311706543, 9.420952796936035, 9.450101852416992, 9.475909233093262, 9.432161331176758, 9.438759803771973, 9.441463470458984, 9.47204303741455, 9.403733253479004, 9.456913948059082, 9.489250183105469, 9.472453117370605, 9.467790603637695, 9.447869300842285, 9.50535774230957, 9.495697975158691, 9.550504684448242, 9.494595527648926, 9.526615142822266, 9.571975708007812, 9.5604248046875, 9.520313262939453, 9.625659942626953, 9.599746704101562, 9.614375114440918, 9.619553565979004, 9.54300594329834, 9.58000659942627, 9.659143447875977, 9.65058422088623, 9.589821815490723, 9.686781883239746, 9.678272247314453, 9.641669273376465, 9.632264137268066, 9.678268432617188, 9.668156623840332, 9.700486183166504, 9.670654296875, 9.736562728881836, 9.714948654174805, 9.747123718261719, 9.710671424865723, 9.726539611816406, 9.767770767211914, 9.706463813781738, 9.687151908874512, 9.706415176391602, 9.693958282470703, 9.683018684387207, 9.801871299743652, 9.735254287719727, 9.724715232849121, 9.698816299438477, 9.753783226013184, 9.760106086730957, 9.839076042175293, 9.845850944519043, 9.855257987976074, 9.79184341430664, 9.765809059143066, 9.850480079650879, 9.797479629516602, 9.804856300354004, 9.857549667358398, 9.81493091583252, 9.823090553283691, 9.786590576171875, 9.7866849899292, 9.8911714553833, 9.788925170898438, 9.90186595916748, 9.879353523254395, 9.860163688659668, 9.89746379852295, 9.779236793518066, 9.85682201385498, 9.872174263000488, 9.904027938842773, 9.97659969329834, 9.994986534118652, 9.93973159790039, 9.892644882202148, 9.907636642456055, 9.947927474975586, 9.991455078125, 9.984703063964844, 9.985127449035645, 9.975567817687988, 9.97638988494873, 9.952582359313965, 9.976996421813965, 9.938711166381836, 10.002985000610352, 10.014431953430176, 10.014984130859375, 10.060877799987793, 10.088844299316406, 10.02536392211914, 9.989370346069336, 10.029144287109375, 9.94666576385498, 10.048295974731445, 10.043638229370117, 10.055829048156738, 10.002419471740723, 10.03606128692627, 9.985846519470215, 10.056924819946289, 10.046748161315918, 10.043429374694824, 10.057329177856445, 10.089051246643066, 10.093863487243652, 10.153517723083496, 10.104842185974121, 10.064675331115723, 10.149765968322754, 10.174967765808105, 10.1535005569458, 10.148859977722168, 10.098540306091309, 10.18799114227295, 10.151997566223145, 10.110013008117676, 10.152444839477539, 10.204248428344727, 10.182126998901367, 10.177501678466797, 10.136330604553223, 10.192605018615723, 10.156577110290527, 10.183003425598145, 10.133132934570312, 10.110381126403809, 10.160709381103516, 10.199335098266602, 10.191929817199707, 10.230559349060059, 10.198878288269043, 10.20158863067627, 10.221324920654297, 10.235447883605957, 10.189408302307129, 10.219768524169922, 10.25086498260498, 10.29871654510498, 10.25256061553955, 10.304192543029785, 10.194900512695312, 10.247417449951172, 10.246251106262207, 10.29559326171875, 10.261749267578125, 10.271471977233887, 10.33713436126709, 10.273933410644531, 10.226823806762695, 10.266448974609375, 10.23887825012207, 10.33549976348877, 10.311206817626953, 10.280656814575195, 10.298513412475586, 10.356525421142578, 10.356785774230957, 10.358695983886719, 10.318659782409668, 10.338214874267578, 10.333951950073242, 10.359245300292969, 10.304974555969238, 10.414581298828125, 10.360187530517578, 10.422396659851074, 10.383631706237793, 10.391356468200684, 10.337711334228516, 10.399136543273926, 10.380802154541016, 10.424531936645508, 10.395012855529785, 10.465195655822754, 10.338091850280762, 10.4320068359375, 10.43752384185791, 10.413241386413574, 10.444828987121582, 10.409032821655273, 10.471036911010742, 10.342068672180176, 10.471902847290039, 10.459872245788574, 10.41877269744873, 10.499322891235352, 10.444331169128418, 10.44308853149414, 10.461743354797363, 10.463170051574707, 10.444615364074707, 10.432865142822266, 10.487117767333984, 10.523528099060059, 10.545230865478516, 10.504417419433594, 10.468966484069824, 10.44719123840332, 10.427950859069824, 10.499178886413574, 10.495201110839844, 10.548486709594727, 10.526596069335938, 10.525449752807617, 10.437758445739746, 10.4793119430542, 10.520360946655273, 10.438840866088867]}, {\"line\": {\"color\": \"firebrick\", \"dash\": \"dot\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"validation_accuracy\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"xaxis\": \"x\", \"y\": [0.05711717903614044, 0.05711717903614044, 0.05711717903614044, 0.05711717903614044, 0.06867542117834091, 0.0748165175318718, 0.08243047446012497, 0.08530130982398987, 0.09149233251810074, 0.0948624461889267, 0.0980827808380127, 0.10175246000289917, 0.10320036113262177, 0.1059214174747467, 0.10819312185049057, 0.10959109663963318, 0.11243696510791779, 0.11338558793067932, 0.11515802145004272, 0.11842828243970871, 0.11912726610898972, 0.11875280737876892, 0.12055020034313202, 0.12147386372089386, 0.1216985359787941, 0.12402017414569855, 0.12509360909461975, 0.12649159133434296, 0.12626691162586212, 0.1270158290863037, 0.12721553444862366, 0.1295870989561081, 0.1320335566997528, 0.13081032037734985, 0.13368116319179535, 0.13223326206207275, 0.13413050770759583, 0.13512906432151794, 0.13575315475463867, 0.1353038102388382, 0.1381496787071228, 0.13877378404140472, 0.13944779336452484, 0.13972240686416626, 0.1399720460176468, 0.1403215378522873, 0.14094562828540802, 0.14189425110816956, 0.14184433221817017, 0.143466979265213, 0.14449049532413483, 0.14518947899341583, 0.1459883153438568, 0.14611314237117767, 0.14696191251277924, 0.14796045422554016, 0.14791053533554077, 0.1489090770483017, 0.15008237957954407, 0.14908382296562195, 0.15083129703998566, 0.1503569781780243, 0.1499325931072235, 0.15220430493354797, 0.14950820803642273, 0.1517299860715866, 0.15220430493354797, 0.152104452252388, 0.15267860889434814, 0.15312796831130981, 0.1549253612756729, 0.15417644381523132, 0.1553747057914734, 0.1549253612756729, 0.1560736894607544, 0.1570972055196762, 0.1564231812953949, 0.1574966311454773, 0.1588696390390396, 0.15879474580287933, 0.15772131085395813, 0.158220574259758, 0.15809576213359833, 0.16054221987724304, 0.15944381058216095, 0.16059213876724243, 0.16169054806232452, 0.16061709821224213, 0.16064207255840302, 0.16079185903072357, 0.16418692469596863, 0.1635378748178482, 0.16411203145980835, 0.16383743286132812, 0.16483598947525024, 0.1661091446876526, 0.16481103003025055, 0.16590942442417145, 0.16735732555389404, 0.1653851866722107, 0.16673323512077332, 0.1692545861005783, 0.16673323512077332, 0.1682560294866562, 0.1682060956954956, 0.1678066849708557, 0.16942933201789856, 0.1703280210494995, 0.1689799726009369, 0.17087723314762115, 0.17025312781333923, 0.1728493720293045, 0.17349842190742493, 0.1710519790649414, 0.17257477343082428, 0.17377303540706635, 0.17384791374206543, 0.17449697852134705, 0.17459683120250702, 0.17552049458026886, 0.17621947824954987, 0.17479655146598816, 0.1764940768480301, 0.17761746048927307, 0.1761196255683899, 0.17584502696990967, 0.1761445850133896, 0.1775175929069519, 0.17844125628471375, 0.17869089543819427, 0.17661890387535095, 0.1778670996427536, 0.1785660833120346, 0.18066303431987762, 0.1807878613471985, 0.18023864924907684, 0.1821608692407608, 0.18131209909915924, 0.18305955827236176, 0.18410804867744446, 0.18445754051208496, 0.18498177826404572, 0.18378351628780365, 0.18600529432296753, 0.18488192558288574, 0.18498177826404572, 0.1843077540397644, 0.185755655169487, 0.18568076193332672, 0.18627989292144775, 0.1868790239095688, 0.18655449151992798, 0.18750311434268951, 0.18979978561401367, 0.18872635066509247, 0.1882520318031311, 0.18977482616901398, 0.1886514574289322, 0.188975989818573, 0.19027410447597504, 0.1904488503932953, 0.19172200560569763, 0.18977482616901398, 0.18984971940517426, 0.19029906392097473, 0.19349443912506104, 0.1917968988418579, 0.1925957351922989, 0.1936691850423813, 0.19334465265274048, 0.19481751322746277, 0.19496729969978333, 0.19461780786514282, 0.19314493238925934, 0.19411852955818176, 0.1957411766052246, 0.19496729969978333, 0.19671475887298584, 0.19678965210914612, 0.19621548056602478, 0.19693943858146667, 0.19676469266414642, 0.1968895047903061, 0.19691447913646698, 0.1975885033607483, 0.19918617606163025, 0.19873683154582977, 0.19803784787654877, 0.1996854543685913, 0.19943581521511078, 0.199710413813591, 0.19996005296707153, 0.20060911774635315, 0.2007589042186737, 0.2004842907190323, 0.20065905153751373, 0.20303060114383698, 0.20195716619491577, 0.20280593633651733, 0.20375455915927887, 0.202930748462677, 0.2036047726869583, 0.20323032140731812, 0.2043287307024002, 0.2043786495923996, 0.20457835495471954, 0.20642568171024323, 0.20712466537952423, 0.2054770588874817, 0.20592640340328217, 0.20487792789936066, 0.2065255343914032, 0.20709970593452454, 0.20729941129684448, 0.20879724621772766, 0.20802336931228638, 0.20804832875728607, 0.20907184481620789, 0.20912177860736847, 0.20949622988700867, 0.2090219110250473, 0.2097209095954895, 0.21051974594593048, 0.2115432620048523, 0.2108442783355713, 0.210769385099411, 0.21174296736717224, 0.21209245920181274, 0.2111688107252121, 0.21246692538261414, 0.2125917375087738, 0.2147136628627777, 0.21391482651233673, 0.2129662036895752, 0.21458885073661804, 0.2147386223077774, 0.21319086849689484, 0.21411453187465668, 0.21271656453609467, 0.2158370316028595, 0.21641120314598083, 0.21683558821678162, 0.21641120314598083, 0.21701033413410187, 0.2165110558271408, 0.2187328338623047, 0.21698537468910217, 0.21743471920490265, 0.21628639101982117, 0.21960656344890594, 0.21890757977962494, 0.21883268654346466, 0.21985620260238647, 0.21980628371238708, 0.21930700540542603, 0.22053022682666779, 0.22055520117282867, 0.2208547592163086, 0.22205302119255066, 0.22397524118423462, 0.2222527265548706, 0.22160367667675018, 0.2233511358499527, 0.22275200486183167, 0.22360077500343323, 0.22355085611343384, 0.22177842259407043, 0.2241000533103943, 0.22340106964111328, 0.2237256020307541, 0.22392530739307404, 0.2233012169599533, 0.22492386400699615, 0.22599729895591736, 0.2262219786643982, 0.22529831528663635, 0.22697089612483978, 0.22524839639663696, 0.2265964299440384, 0.2265215367078781, 0.22642168402671814, 0.22709570825099945, 0.22604723274707794, 0.22664636373519897, 0.22776973247528076, 0.22724549472332, 0.2290928214788437, 0.22891807556152344, 0.22854360938072205, 0.22931748628616333, 0.22999151051044464, 0.23006640374660492, 0.22829397022724152, 0.23061560094356537, 0.23006640374660492, 0.23034100234508514, 0.23143941164016724, 0.23099006712436676, 0.2322881817817688, 0.23039093613624573, 0.23223824799060822, 0.2316141575574875, 0.23178890347480774, 0.23103998601436615, 0.23293724656105042, 0.23391082882881165, 0.23293724656105042, 0.23308701813220978, 0.23276250064373016, 0.23321184515953064, 0.2337111234664917, 0.2326127141714096, 0.23308701813220978, 0.23426032066345215, 0.23416046798229218, 0.234385147690773, 0.23493434488773346, 0.23530879616737366, 0.23383593559265137, 0.23448500037193298, 0.23388586938381195, 0.23533377051353455, 0.23323680460453033, 0.23595786094665527, 0.23685656487941742, 0.23650705814361572, 0.2354835420846939, 0.23783014714717865, 0.23638224601745605, 0.23758050799369812, 0.23788008093833923, 0.23608267307281494, 0.23720605671405792, 0.2376554012298584, 0.23827949166297913, 0.23795495927333832, 0.23920315504074097, 0.23815467953681946, 0.2390533685684204, 0.2409256547689438, 0.23758050799369812, 0.24005192518234253, 0.24080084264278412, 0.24032652378082275, 0.2401517778635025, 0.23817963898181915, 0.24112537503242493, 0.23967747390270233, 0.239802286028862, 0.24070098996162415, 0.24115033447742462, 0.24212391674518585, 0.24152478575706482, 0.23985221982002258, 0.2416246384382248, 0.24284787476062775, 0.24137499928474426, 0.24257327616214752, 0.24359677731990814, 0.2426231950521469, 0.2441210299730301, 0.24147486686706543, 0.24169953167438507, 0.243022620677948, 0.24437066912651062, 0.24312247335910797, 0.24357181787490845, 0.2433970719575882, 0.24422088265419006, 0.24344700574874878, 0.24462029337882996, 0.24579359591007233, 0.24486993253231049, 0.2448200136423111, 0.24634280800819397, 0.24529431760311127, 0.24651755392551422, 0.2462679147720337, 0.24828997254371643, 0.24631783366203308, 0.2465674728155136, 0.2455189973115921, 0.24779070913791656, 0.24566878378391266, 0.24853961169719696, 0.24794048070907593, 0.246941938996315, 0.24789056181907654, 0.24809026718139648, 0.24736632406711578, 0.24601827561855316, 0.2483898401260376, 0.24958810210227966, 0.24973787367343903, 0.24918867647647858, 0.24796545505523682, 0.2498627007007599, 0.24901393055915833, 0.2509361505508423, 0.25076138973236084, 0.2501123249530792, 0.2512606680393219, 0.24828997254371643, 0.25138548016548157, 0.25036197900772095, 0.25243398547172546, 0.25255879759788513, 0.25111088156700134, 0.25041189789772034, 0.2519596517086029, 0.25255879759788513, 0.254181444644928, 0.25315791368484497, 0.2512107491493225, 0.2512856423854828, 0.254181444644928, 0.2534574866294861, 0.2533825933933258, 0.25223425030708313, 0.25353237986564636, 0.25508013367652893, 0.2544809877872467, 0.2548554539680481, 0.2540316581726074, 0.2552049458026886, 0.25353237986564636, 0.2541564702987671, 0.2541564702987671, 0.25530481338500977, 0.2556293308734894, 0.25605371594429016, 0.25537970662117004, 0.2547306418418884, 0.2549053728580475, 0.25602877140045166, 0.25652801990509033, 0.2566278874874115, 0.25607869029045105, 0.2566778063774109, 0.2573268711566925, 0.2577013373374939, 0.2573518455028534, 0.2563033699989319, 0.25650307536125183, 0.25787606835365295, 0.25782614946365356, 0.25707724690437317, 0.2583504021167755, 0.25790104269981384, 0.2572270333766937, 0.25902441143989563, 0.2580508291721344, 0.2602476477622986, 0.25967347621917725, 0.25862500071525574, 0.2597982883453369, 0.2594238221645355, 0.2590493857860565, 0.25972339510917664, 0.258000910282135, 0.26007288694381714, 0.26137101650238037, 0.260097861289978, 0.26034748554229736, 0.2612212300300598, 0.26004794239997864, 0.261321097612381, 0.2611713111400604, 0.262020081281662, 0.259773313999176, 0.26137101650238037, 0.260996550321579, 0.2620450258255005, 0.2627440094947815, 0.2612462043762207, 0.26291877031326294, 0.2625942528247833, 0.263093501329422, 0.26319336891174316, 0.26366767287254333, 0.2624194920063019, 0.2626441717147827, 0.2641420066356659, 0.26329323649406433, 0.26286885142326355, 0.26394227147102356, 0.26359277963638306, 0.2640671133995056, 0.2644166052341461, 0.26549002528190613, 0.26651355624198914, 0.26501572132110596, 0.2655898928642273, 0.26466622948646545, 0.26509061455726624, 0.2665884494781494, 0.265839546918869, 0.2656398117542267, 0.2661890387535095, 0.2666383683681488], \"yaxis\": \"y2\"}],\n",
              "                        {\"legend\": {\"orientation\": \"h\"}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Training Metrics\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.94]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"tickfont\": {\"color\": \"royalblue\"}, \"title\": {\"font\": {\"color\": \"royalblue\"}, \"text\": \"Loss\"}}, \"yaxis2\": {\"anchor\": \"x\", \"overlaying\": \"y\", \"side\": \"right\", \"tickfont\": {\"color\": \"firebrick\"}, \"title\": {\"font\": {\"color\": \"firebrick\"}, \"text\": \"Accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dc2c2089-688c-4027-84aa-30c456c68a31');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMTPlfM8I_Wd",
        "colab_type": "text"
      },
      "source": [
        "# Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W8epfdOM9vm",
        "colab_type": "text"
      },
      "source": [
        "### 100 Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn7qrsCu-LI9",
        "colab_type": "text"
      },
      "source": [
        "<tr>\n",
        "    <th>LSTM Layers</th>\n",
        "    <th>LSTM Cells per Layer</th>\n",
        "    <th>Dropout %</th>\n",
        "    <th>Validation Loss</th>\n",
        "    <th>Validation Accuracy</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2</td>\n",
        "    <td>8.9918</td>\n",
        "    <td>0.2271</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2</td>\n",
        "    <td>10.7950</td>\n",
        "    <td>0.2354</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5</td>\n",
        "    <td>8.9682</td>\n",
        "    <td>0.2153</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>6.9549</td>\n",
        "    <td>0.1490</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>7.4581</td>\n",
        "    <td>0.1683</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5, 0.5</td>\n",
        "    <td>7.3286</td>\n",
        "    <td>0.1650</td>\n",
        "</tr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXjdqe2PNKFv",
        "colab_type": "text"
      },
      "source": [
        "### 500 Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnZhzWmNGKs",
        "colab_type": "text"
      },
      "source": [
        "<tr>\n",
        "    <th>LSTM Layers</th>\n",
        "    <th>LSTM Cells per Layer</th>\n",
        "    <th>Dropout %</th>\n",
        "    <th>Validation Loss</th>\n",
        "    <th>Validation Accuracy</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2</td>\n",
        "    <td>16.4351</td>\n",
        "    <td>0.3229</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2</td>\n",
        "    <td>22.5922</td>\n",
        "    <td>0.3373</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5</td>\n",
        "    <td>18.8511</td>\n",
        "    <td>0.3329</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>8.8127</td>\n",
        "    <td>0.2230</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>10.8240</td>\n",
        "    <td>0.2854</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5, 0.5</td>\n",
        "    <td>10.4388</td>\n",
        "    <td>0.2666</td>\n",
        "</tr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TPnMMNvDAXC",
        "colab_type": "text"
      },
      "source": [
        "# Generate Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rmhEm6UESld",
        "colab_type": "text"
      },
      "source": [
        "### Load Objects To Infer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbsosSoSDBVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-2.h5')\n",
        "model = load_model(model_filepath)\n",
        "TRAINING_LENGTH = 10"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j56FokGJQUKc",
        "colab_type": "text"
      },
      "source": [
        "## Existing Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68jLzLENLLOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fd0a9f5a-1345-4c8c-8595-5fd68359452e"
      },
      "source": [
        "original_sequence, gen_list, a = predict_utils.generate_output(\n",
        "    model,\n",
        "    sequences,\n",
        "    idx_word,\n",
        "    seed_length=TRAINING_LENGTH,\n",
        "    new_words=20,\n",
        "    diversity=1,\n",
        "    n_gen=1\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py:42: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFkDMZOGLRKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bdd0e522-cb71-4cc7-c6ed-e60234a27059"
      },
      "source": [
        "' '.join(word for word in original_sequence)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'executive faces more than 1 000 similar claims for damages'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x2HCA1iLRMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7e260a9f-22ef-49c6-b032-6c20c00ddffc"
      },
      "source": [
        "' '.join(word for word in gen_list[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'< --- > from their final crisis with senior police officers and more with half their people across custody and then in many'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmsySNZULlku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b27e1953-21f4-4762-a27d-2c124f37eb21"
      },
      "source": [
        "' '.join(word for word in a)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'< --- > law firm tods murray where he is a partner mr mcletchie said he has taken advice from holyrood officials about'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6sCL1XKLlo7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "635ff198-292e-4c78-de48-2c39ddfa1722"
      },
      "source": [
        "original_sequence, gen_list, a = predict_utils.generate_output(\n",
        "    model,\n",
        "    sequences,\n",
        "    idx_word,\n",
        "    seed_length=TRAINING_LENGTH,\n",
        "    new_words=20,\n",
        "    diversity=0.9,\n",
        "    n_gen=1\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py:42: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX3GDUtvWsow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11bee062-3120-4b4f-f7a6-e76ce32fcb0c"
      },
      "source": [
        "' '.join(word for word in original_sequence)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'the new year in the meantime we will be studying'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi8iWEeYWssO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8702ab55-5150-4b86-ace9-f7d7f6bb405a"
      },
      "source": [
        "' '.join(word for word in gen_list[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"< --- > the announcement on his spending plans on the same after a meeting on labour's media media lord woolf for labour's\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86WEQEIoWw0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2695c567-037c-4f72-882e-340f78f4fcef"
      },
      "source": [
        "' '.join(word for word in a)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'< --- > the judgment carefully to see whether it is possible to modify our legislation to address the concerns raised by the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxudAXauQRiT",
        "colab_type": "text"
      },
      "source": [
        "## Custom Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvCuoZqgLls7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "875e9092-97c5-4101-d8ee-46ddf5c6a9e0"
      },
      "source": [
        "sentence = 'Stocks of major large technology firms are becoming even more fragile even though'\n",
        "predict_utils.generate_custom_sentence(sentence, word_idx, idx_word, model, new_words=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 3, 546, 490, 45, 126, 13, 518, 150, 24, 9544, 150, 456]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c04f970c9511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Stocks of major large technology firms are becoming even more fragile even though'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_custom_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py\u001b[0m in \u001b[0;36mgenerate_custom_sentence\u001b[0;34m(sentence, word_idx, idx_word, model, new_words, diversity)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Make a prediction from the seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \t\tpreds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n\u001b[0m\u001b[1;32m     93\u001b[0m \t\t\tnp.float64)\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8_8bcGDQaG7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "74b90cde-dcae-4035-ffc8-a9cb39a67f38"
      },
      "source": [
        "sentence = 'However, there have been many instances of'\n",
        "predict_utils.generate_custom_sentence(sentence, word_idx, idx_word, model, new_words=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 56, 18, 46, 67, 7424, 3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-833bdbd219b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'However, there have been many instances of'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_custom_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py\u001b[0m in \u001b[0;36mgenerate_custom_sentence\u001b[0;34m(sentence, word_idx, idx_word, model, new_words, diversity)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Make a prediction from the seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \t\tpreds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n\u001b[0m\u001b[1;32m     93\u001b[0m \t\t\tnp.float64)\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "240jLlPSQvDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}