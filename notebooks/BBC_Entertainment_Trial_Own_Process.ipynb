{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "autocomplete_me",
      "language": "python",
      "name": "autocomplete_me"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "BBC Entertainment - Trial Own Process.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvWcI8T2eOvw",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNXYErppeOvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "99d0b9f1-745f-4a7c-ac37-d1e1bff00058"
      },
      "source": [
        "# Google Only\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ROOT_FOLDER = '/content/drive/My Drive/Code/autocomplete_me/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9p9MmnceOv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Set Variables for Local and Cloud File Finding\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(ROOT_FOLDER)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W397RMzgCvy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8f983c37-e871-416c-b116-41786db65d7f"
      },
      "source": [
        "!ls -l '/content/drive/My Drive/Code/autocomplete_me/src'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 27\n",
            "-rw------- 1 root root 2516 Jul 14 22:20 predict_utils.py\n",
            "drwx------ 2 root root 4096 Jul 11 13:20 __pycache__\n",
            "-rw------- 1 root root 3340 Jul 14 22:20 reader.py\n",
            "-rw------- 1 root root 3580 Jul 14 22:20 train_model_baseline.py\n",
            "-rw------- 1 root root 3203 Jul 14 22:20 train_utils.py\n",
            "-rw------- 1 root root 9341 Jul 12 14:24 utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fiv0531eOv7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6def9b4-419b-4ec7-a557-b6f8d45bf7d9"
      },
      "source": [
        "from src import utils, reader, predict_utils, train_utils\n",
        "from importlib import reload\n",
        "reload(utils)\n",
        "reload(reader)\n",
        "reload(predict_utils)\n",
        "reload(train_utils)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'src.train_utils' from '/content/drive/My Drive/Code/autocomplete_me/src/train_utils.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpn1lT4IeOv_",
        "colab_type": "text"
      },
      "source": [
        "## Load Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhWMNtqceOv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = reader.read_bbc('entertainment')\n",
        "content_type = 'BBC-Entertainment'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YyHF8aQeOwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "b8286075-f04b-497b-997c-44078fc29706"
      },
      "source": [
        "text[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Blair buys copies of new Band Aid\\n\\nPrime Minister Tony Blair purchased two copies of the charity single Band Aid 20 in Edinburgh on Friday.\\n\\nStaff were surprised when the Prime Minister walked into HMV at 0900 GMT, accompanied by aides and local police. \"When Mr Blair came in unannounced, we were all pretty gobsmacked,\" said HMV manager Clive Smith. \"Our customer helper approached him... it was only then we realised he wanted to buy copies of the Band Aid single, rather than the latest Eminem album.\" Predicted chart-topper Do They Know it\\'s Christmas? is expected to sell at least 300,000 copies by the time the new chart is announced on Sunday. However, the new version of the 1984 single is not going to be released in the US, despite being sold in many countries around the world. US record shops are stocking an import version of Do They Know It\\'s Christmas, which is said to be selling very well in Los Angeles and New York. The original track was released in the US, and reached number 13 in the singles chart. British stars who appear on the current recording, such as Dido and Coldplay\\'s Chris Martin, are well-known to music fans across the Atlantic, along with U2 frontman Bono.\\n\\nRecord company Universal is responsible for the global distribution of the single, which will be available across Europe, Asia, South America and Canada. But music fans in the US are still able to access the song and download it on Band Aid 20\\'s official website. In 1985, a group of high-profile American stars known as USA For Africa came together to record their own fund-raising single, We Are The World. The song was written by Lionel Richie and Michael Jackson, with Quincy Jones as producer. It topped the US charts for three weeks and went on win Grammy awards for best record and song.\\n\\nDionne Warwick, Diana Ross and Tina Turner were among the line-up of performers. It is predicted that the Band Aid 20 song will sell 300,000 copies in the UK by the time the new chart is announced on Sunday. The record is also tipped to become this year\\'s Christmas number one, as the original version did in 1984. Proceeds from the sales are going towards relief for the Darfur region of Sudan and to combat HIV and Aids across Africa.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8y2Nv_MeOwE",
        "colab_type": "text"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Q8y2ILeOwH",
        "colab_type": "text"
      },
      "source": [
        "## Process Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtdMjwySG_ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences, num_words, word_idx, idx_word = train_utils.preprocess_text(text)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESiYIwBkHN7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef750052-8b96-43d3-989f-55acdf4fdaa4"
      },
      "source": [
        "features, labels = train_utils.pass_sliding_window(sequences, sequence_len=10)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 125015 sequences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gny329GmHSMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "475416e0-de9e-4f7e-ebeb-33d3acebc6d3"
      },
      "source": [
        "labels = train_utils.one_hot_labels_and_improve_efficiency(labels)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels matrix shape:  (125015, 12541)\n",
            "Labels matrix shape:  (125015, 12541)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eny8qjVzeOwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Test Train Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20, random_state=42, shuffle=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x5EeUMbkpPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82649ba4-8fbf-483a-cb1f-b17b6dfa2b3f"
      },
      "source": [
        "import gc\n",
        "gc.enable()\n",
        "del labels\n",
        "gc.collect()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8N7zK7IeOwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "68ae4867-37ee-4fec-d565-40b1f20660fd"
      },
      "source": [
        "print('X_train shape: ', X_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape:  (100012, 10)\n",
            "X_test shape:  (25003, 10)\n",
            "y_train shape:  (100012, 12541)\n",
            "y_test shape:  (25003, 12541)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOA6uzXKlu0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f83b92fa-3998-4463-ec9b-2bf136f7c02c"
      },
      "source": [
        "import sys\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "                         key= lambda x: -x[1])[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       y_train:  1.4 GiB\n",
            "                        y_test: 355.5 MiB\n",
            "                      features: 12.6 MiB\n",
            "                       X_train: 10.1 MiB\n",
            "                        X_test:  2.5 MiB\n",
            "                      word_idx: 576.1 KiB\n",
            "                      idx_word: 576.1 KiB\n",
            "                     sequences:  4.5 KiB\n",
            "                          text:  4.2 KiB\n",
            "                            __:  778.0 B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DROHQO32eOwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "a3a178d1-3e11-4813-cc61-ae92fa995458"
      },
      "source": [
        "# Embedding Matrix\n",
        "# embedding_matrix = utils.create_embedding_matrix(word_idx, num_words, '/Users/jaipancholi/data/glove.6B.100d.txt')\n",
        "embedding_matrix = utils.create_embedding_matrix(word_idx, num_words, '/content/drive/My Drive/Code/autocomplete_me/data/glove.6B.100d.txt')\n",
        "embedding_matrix"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glove Vectors loading with dimension 100\n",
            "There were 1186 words without pre-trained embeddings.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Code/autocomplete_me/src/utils.py:180: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.00656124, -0.04206555,  0.12508174, ..., -0.02506376,\n",
              "         0.14220549,  0.04648907],\n",
              "       [-0.01269503,  0.04080414,  0.00418698, ..., -0.12684801,\n",
              "         0.15331151,  0.03447365],\n",
              "       ...,\n",
              "       [ 0.03130877,  0.01514814,  0.06958524, ...,  0.02720818,\n",
              "         0.11345827,  0.06412452],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.05417296, -0.04813922,  0.13245429, ..., -0.20398643,\n",
              "         0.01218687,  0.03430193]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDWbIoaAcRbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7d520a6-515f-489a-f35b-a2412d318560"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12541, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IZp-EmFeOwY",
        "colab_type": "text"
      },
      "source": [
        "# Design Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gstEw4vgGsPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1eSpudQeOwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZOti2z7JJW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=False, epochs=100):\n",
        "  if not model and not use_pretrained_model:\n",
        "    print('Provide one of either model or use_pretrained_model.')\n",
        "  elif model and use_pretrained_model:\n",
        "      print('Provide one of either model or use_pretrained_model.')\n",
        "  elif use_pretrained_model:\n",
        "    model = load_model(model_filepath)\n",
        "  \n",
        "  callbacks = [\n",
        "      EarlyStopping(monitor='val_accuracy', patience=25),\n",
        "      ModelCheckpoint(f'{model_filepath}', save_best_only=True, save_weights_only=False, monitor='val_accuracy')\n",
        "  ]\n",
        "\n",
        "  history = model.fit(\n",
        "      X_train, \n",
        "      y_train, \n",
        "      epochs=epochs, \n",
        "      batch_size=2048, \n",
        "      validation_data=(X_test, y_test), \n",
        "      verbose=1,\n",
        "      callbacks=callbacks\n",
        "  )\n",
        "\n",
        "  return history"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHNsyM7H_qkC",
        "colab_type": "text"
      },
      "source": [
        "##V1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoT44K4meOwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(64))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBtAHhA1eOwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-1.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3pRn4k_2Jx",
        "colab_type": "text"
      },
      "source": [
        "## V2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSI6BxH5ALw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfRMwq-rALzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2831a9c3-d76f-4914-88e0-6fa9727f5268"
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-2.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 7.8605 - accuracy: 0.0609 - val_loss: 7.2590 - val_accuracy: 0.0643\n",
            "Epoch 2/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 7.1513 - accuracy: 0.0653 - val_loss: 7.2325 - val_accuracy: 0.0643\n",
            "Epoch 3/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 7.0817 - accuracy: 0.0653 - val_loss: 7.1817 - val_accuracy: 0.0643\n",
            "Epoch 4/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 7.0136 - accuracy: 0.0653 - val_loss: 7.1765 - val_accuracy: 0.0643\n",
            "Epoch 5/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 6.9652 - accuracy: 0.0653 - val_loss: 7.1617 - val_accuracy: 0.0643\n",
            "Epoch 6/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.8910 - accuracy: 0.0687 - val_loss: 7.0955 - val_accuracy: 0.0721\n",
            "Epoch 7/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.7755 - accuracy: 0.0746 - val_loss: 7.0222 - val_accuracy: 0.0777\n",
            "Epoch 8/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.6501 - accuracy: 0.0828 - val_loss: 6.9680 - val_accuracy: 0.0825\n",
            "Epoch 9/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 6.5421 - accuracy: 0.0888 - val_loss: 6.9489 - val_accuracy: 0.0854\n",
            "Epoch 10/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.4550 - accuracy: 0.0926 - val_loss: 6.9228 - val_accuracy: 0.0893\n",
            "Epoch 11/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.3687 - accuracy: 0.0965 - val_loss: 6.9108 - val_accuracy: 0.0905\n",
            "Epoch 12/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 6.2801 - accuracy: 0.1028 - val_loss: 6.8931 - val_accuracy: 0.0953\n",
            "Epoch 13/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.1836 - accuracy: 0.1074 - val_loss: 6.8637 - val_accuracy: 0.0979\n",
            "Epoch 14/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.0793 - accuracy: 0.1128 - val_loss: 6.8382 - val_accuracy: 0.1011\n",
            "Epoch 15/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.9761 - accuracy: 0.1180 - val_loss: 6.8298 - val_accuracy: 0.1051\n",
            "Epoch 16/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.8793 - accuracy: 0.1218 - val_loss: 6.8236 - val_accuracy: 0.1090\n",
            "Epoch 17/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.7825 - accuracy: 0.1279 - val_loss: 6.8380 - val_accuracy: 0.1116\n",
            "Epoch 18/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.6890 - accuracy: 0.1328 - val_loss: 6.8561 - val_accuracy: 0.1140\n",
            "Epoch 19/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.6002 - accuracy: 0.1366 - val_loss: 6.8658 - val_accuracy: 0.1191\n",
            "Epoch 20/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.5137 - accuracy: 0.1427 - val_loss: 6.8902 - val_accuracy: 0.1225\n",
            "Epoch 21/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.4306 - accuracy: 0.1475 - val_loss: 6.9118 - val_accuracy: 0.1226\n",
            "Epoch 22/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.3447 - accuracy: 0.1510 - val_loss: 6.9711 - val_accuracy: 0.1248\n",
            "Epoch 23/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.2651 - accuracy: 0.1569 - val_loss: 6.9925 - val_accuracy: 0.1287\n",
            "Epoch 24/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.1891 - accuracy: 0.1605 - val_loss: 7.0395 - val_accuracy: 0.1307\n",
            "Epoch 25/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.1142 - accuracy: 0.1648 - val_loss: 7.0852 - val_accuracy: 0.1307\n",
            "Epoch 26/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.0405 - accuracy: 0.1692 - val_loss: 7.1344 - val_accuracy: 0.1327\n",
            "Epoch 27/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.9711 - accuracy: 0.1732 - val_loss: 7.2099 - val_accuracy: 0.1352\n",
            "Epoch 28/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.9011 - accuracy: 0.1771 - val_loss: 7.2661 - val_accuracy: 0.1368\n",
            "Epoch 29/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 4.8331 - accuracy: 0.1819 - val_loss: 7.3287 - val_accuracy: 0.1375\n",
            "Epoch 30/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 4.7732 - accuracy: 0.1846 - val_loss: 7.3743 - val_accuracy: 0.1383\n",
            "Epoch 31/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 4.7085 - accuracy: 0.1894 - val_loss: 7.4446 - val_accuracy: 0.1377\n",
            "Epoch 32/500\n",
            "49/49 [==============================] - 6s 127ms/step - loss: 4.6463 - accuracy: 0.1942 - val_loss: 7.5157 - val_accuracy: 0.1403\n",
            "Epoch 33/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.5837 - accuracy: 0.1986 - val_loss: 7.5778 - val_accuracy: 0.1409\n",
            "Epoch 34/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.5270 - accuracy: 0.2048 - val_loss: 7.6278 - val_accuracy: 0.1424\n",
            "Epoch 35/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 4.4638 - accuracy: 0.2088 - val_loss: 7.7080 - val_accuracy: 0.1445\n",
            "Epoch 36/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.4073 - accuracy: 0.2138 - val_loss: 7.8146 - val_accuracy: 0.1445\n",
            "Epoch 37/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 4.3538 - accuracy: 0.2184 - val_loss: 7.8803 - val_accuracy: 0.1442\n",
            "Epoch 38/500\n",
            "49/49 [==============================] - 6s 126ms/step - loss: 4.2935 - accuracy: 0.2242 - val_loss: 7.9453 - val_accuracy: 0.1449\n",
            "Epoch 39/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 4.2365 - accuracy: 0.2301 - val_loss: 8.0327 - val_accuracy: 0.1455\n",
            "Epoch 40/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 4.1787 - accuracy: 0.2346 - val_loss: 8.0822 - val_accuracy: 0.1477\n",
            "Epoch 41/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 4.1246 - accuracy: 0.2406 - val_loss: 8.1979 - val_accuracy: 0.1481\n",
            "Epoch 42/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 4.0726 - accuracy: 0.2460 - val_loss: 8.2335 - val_accuracy: 0.1474\n",
            "Epoch 43/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.0195 - accuracy: 0.2502 - val_loss: 8.3101 - val_accuracy: 0.1460\n",
            "Epoch 44/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 3.9651 - accuracy: 0.2558 - val_loss: 8.4165 - val_accuracy: 0.1489\n",
            "Epoch 45/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.9154 - accuracy: 0.2604 - val_loss: 8.4940 - val_accuracy: 0.1474\n",
            "Epoch 46/500\n",
            "49/49 [==============================] - 6s 127ms/step - loss: 3.8684 - accuracy: 0.2654 - val_loss: 8.5662 - val_accuracy: 0.1497\n",
            "Epoch 47/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.8183 - accuracy: 0.2711 - val_loss: 8.6219 - val_accuracy: 0.1467\n",
            "Epoch 48/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 3.7723 - accuracy: 0.2756 - val_loss: 8.7371 - val_accuracy: 0.1498\n",
            "Epoch 49/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.7278 - accuracy: 0.2809 - val_loss: 8.8237 - val_accuracy: 0.1480\n",
            "Epoch 50/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.6811 - accuracy: 0.2850 - val_loss: 8.8957 - val_accuracy: 0.1478\n",
            "Epoch 51/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.6345 - accuracy: 0.2920 - val_loss: 8.9703 - val_accuracy: 0.1484\n",
            "Epoch 52/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 3.5928 - accuracy: 0.2967 - val_loss: 9.0418 - val_accuracy: 0.1474\n",
            "Epoch 53/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.5528 - accuracy: 0.2999 - val_loss: 9.1365 - val_accuracy: 0.1472\n",
            "Epoch 54/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.5063 - accuracy: 0.3063 - val_loss: 9.2339 - val_accuracy: 0.1486\n",
            "Epoch 55/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.4683 - accuracy: 0.3101 - val_loss: 9.2970 - val_accuracy: 0.1483\n",
            "Epoch 56/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.4284 - accuracy: 0.3146 - val_loss: 9.3892 - val_accuracy: 0.1465\n",
            "Epoch 57/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 3.3848 - accuracy: 0.3204 - val_loss: 9.4537 - val_accuracy: 0.1501\n",
            "Epoch 58/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.3419 - accuracy: 0.3253 - val_loss: 9.5516 - val_accuracy: 0.1483\n",
            "Epoch 59/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 3.3073 - accuracy: 0.3291 - val_loss: 9.6315 - val_accuracy: 0.1480\n",
            "Epoch 60/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 3.2684 - accuracy: 0.3342 - val_loss: 9.7095 - val_accuracy: 0.1491\n",
            "Epoch 61/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.2321 - accuracy: 0.3393 - val_loss: 9.7657 - val_accuracy: 0.1482\n",
            "Epoch 62/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.1981 - accuracy: 0.3429 - val_loss: 9.8770 - val_accuracy: 0.1481\n",
            "Epoch 63/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 3.1621 - accuracy: 0.3483 - val_loss: 9.9412 - val_accuracy: 0.1507\n",
            "Epoch 64/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.1400 - accuracy: 0.3502 - val_loss: 9.9561 - val_accuracy: 0.1477\n",
            "Epoch 65/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.0962 - accuracy: 0.3565 - val_loss: 10.0658 - val_accuracy: 0.1486\n",
            "Epoch 66/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.0526 - accuracy: 0.3631 - val_loss: 10.1874 - val_accuracy: 0.1478\n",
            "Epoch 67/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 3.0214 - accuracy: 0.3666 - val_loss: 10.2621 - val_accuracy: 0.1481\n",
            "Epoch 68/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.9893 - accuracy: 0.3717 - val_loss: 10.2984 - val_accuracy: 0.1503\n",
            "Epoch 69/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.9557 - accuracy: 0.3764 - val_loss: 10.4091 - val_accuracy: 0.1502\n",
            "Epoch 70/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.9215 - accuracy: 0.3797 - val_loss: 10.4850 - val_accuracy: 0.1502\n",
            "Epoch 71/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.8893 - accuracy: 0.3867 - val_loss: 10.5554 - val_accuracy: 0.1491\n",
            "Epoch 72/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 2.8615 - accuracy: 0.3913 - val_loss: 10.6713 - val_accuracy: 0.1515\n",
            "Epoch 73/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 2.8295 - accuracy: 0.3943 - val_loss: 10.7614 - val_accuracy: 0.1518\n",
            "Epoch 74/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.7998 - accuracy: 0.3982 - val_loss: 10.8525 - val_accuracy: 0.1496\n",
            "Epoch 75/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 2.7645 - accuracy: 0.4062 - val_loss: 10.9412 - val_accuracy: 0.1539\n",
            "Epoch 76/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.7320 - accuracy: 0.4092 - val_loss: 11.0293 - val_accuracy: 0.1526\n",
            "Epoch 77/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.7096 - accuracy: 0.4147 - val_loss: 11.0947 - val_accuracy: 0.1517\n",
            "Epoch 78/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.6781 - accuracy: 0.4182 - val_loss: 11.1481 - val_accuracy: 0.1527\n",
            "Epoch 79/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 2.6457 - accuracy: 0.4248 - val_loss: 11.2132 - val_accuracy: 0.1497\n",
            "Epoch 80/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 2.6180 - accuracy: 0.4280 - val_loss: 11.3383 - val_accuracy: 0.1519\n",
            "Epoch 81/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 2.5917 - accuracy: 0.4320 - val_loss: 11.4135 - val_accuracy: 0.1513\n",
            "Epoch 82/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.5679 - accuracy: 0.4361 - val_loss: 11.4742 - val_accuracy: 0.1503\n",
            "Epoch 83/500\n",
            "49/49 [==============================] - 6s 114ms/step - loss: 2.5358 - accuracy: 0.4417 - val_loss: 11.5461 - val_accuracy: 0.1509\n",
            "Epoch 84/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.5065 - accuracy: 0.4449 - val_loss: 11.6109 - val_accuracy: 0.1525\n",
            "Epoch 85/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 2.4859 - accuracy: 0.4489 - val_loss: 11.6804 - val_accuracy: 0.1522\n",
            "Epoch 86/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 2.4511 - accuracy: 0.4554 - val_loss: 11.8552 - val_accuracy: 0.1532\n",
            "Epoch 87/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.4291 - accuracy: 0.4575 - val_loss: 11.8641 - val_accuracy: 0.1528\n",
            "Epoch 88/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.4052 - accuracy: 0.4632 - val_loss: 11.9652 - val_accuracy: 0.1512\n",
            "Epoch 89/500\n",
            "49/49 [==============================] - 6s 130ms/step - loss: 2.3763 - accuracy: 0.4682 - val_loss: 12.0551 - val_accuracy: 0.1541\n",
            "Epoch 90/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.3557 - accuracy: 0.4708 - val_loss: 12.1207 - val_accuracy: 0.1515\n",
            "Epoch 91/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.3253 - accuracy: 0.4766 - val_loss: 12.2095 - val_accuracy: 0.1541\n",
            "Epoch 92/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.3038 - accuracy: 0.4815 - val_loss: 12.2691 - val_accuracy: 0.1501\n",
            "Epoch 93/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 2.2780 - accuracy: 0.4842 - val_loss: 12.3760 - val_accuracy: 0.1538\n",
            "Epoch 94/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 2.2511 - accuracy: 0.4875 - val_loss: 12.4411 - val_accuracy: 0.1542\n",
            "Epoch 95/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 2.2335 - accuracy: 0.4924 - val_loss: 12.5611 - val_accuracy: 0.1555\n",
            "Epoch 96/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.2064 - accuracy: 0.4959 - val_loss: 12.5963 - val_accuracy: 0.1525\n",
            "Epoch 97/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.1875 - accuracy: 0.5003 - val_loss: 12.6880 - val_accuracy: 0.1542\n",
            "Epoch 98/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.1596 - accuracy: 0.5058 - val_loss: 12.8106 - val_accuracy: 0.1551\n",
            "Epoch 99/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 2.1380 - accuracy: 0.5078 - val_loss: 12.8787 - val_accuracy: 0.1528\n",
            "Epoch 100/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.1162 - accuracy: 0.5128 - val_loss: 12.9710 - val_accuracy: 0.1532\n",
            "Epoch 101/500\n",
            "49/49 [==============================] - 6s 127ms/step - loss: 2.0954 - accuracy: 0.5169 - val_loss: 12.9791 - val_accuracy: 0.1559\n",
            "Epoch 102/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.0758 - accuracy: 0.5203 - val_loss: 13.0951 - val_accuracy: 0.1547\n",
            "Epoch 103/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.0468 - accuracy: 0.5260 - val_loss: 13.1740 - val_accuracy: 0.1516\n",
            "Epoch 104/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 2.0251 - accuracy: 0.5292 - val_loss: 13.3063 - val_accuracy: 0.1558\n",
            "Epoch 105/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 2.0083 - accuracy: 0.5336 - val_loss: 13.3328 - val_accuracy: 0.1529\n",
            "Epoch 106/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.9891 - accuracy: 0.5345 - val_loss: 13.3903 - val_accuracy: 0.1544\n",
            "Epoch 107/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.9705 - accuracy: 0.5382 - val_loss: 13.4689 - val_accuracy: 0.1550\n",
            "Epoch 108/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.9455 - accuracy: 0.5434 - val_loss: 13.5859 - val_accuracy: 0.1553\n",
            "Epoch 109/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.9251 - accuracy: 0.5485 - val_loss: 13.6865 - val_accuracy: 0.1537\n",
            "Epoch 110/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.9107 - accuracy: 0.5504 - val_loss: 13.7731 - val_accuracy: 0.1537\n",
            "Epoch 111/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.8810 - accuracy: 0.5577 - val_loss: 13.8298 - val_accuracy: 0.1541\n",
            "Epoch 112/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.8681 - accuracy: 0.5576 - val_loss: 13.9014 - val_accuracy: 0.1523\n",
            "Epoch 113/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.8493 - accuracy: 0.5617 - val_loss: 13.9587 - val_accuracy: 0.1521\n",
            "Epoch 114/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.8271 - accuracy: 0.5677 - val_loss: 14.1148 - val_accuracy: 0.1553\n",
            "Epoch 115/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 1.8082 - accuracy: 0.5697 - val_loss: 14.1275 - val_accuracy: 0.1565\n",
            "Epoch 116/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.7889 - accuracy: 0.5744 - val_loss: 14.2408 - val_accuracy: 0.1561\n",
            "Epoch 117/500\n",
            "49/49 [==============================] - 6s 132ms/step - loss: 1.7726 - accuracy: 0.5778 - val_loss: 14.3066 - val_accuracy: 0.1569\n",
            "Epoch 118/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.7553 - accuracy: 0.5805 - val_loss: 14.4026 - val_accuracy: 0.1543\n",
            "Epoch 119/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.7390 - accuracy: 0.5837 - val_loss: 14.4657 - val_accuracy: 0.1543\n",
            "Epoch 120/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.7171 - accuracy: 0.5896 - val_loss: 14.5693 - val_accuracy: 0.1564\n",
            "Epoch 121/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 1.6931 - accuracy: 0.5936 - val_loss: 14.6043 - val_accuracy: 0.1562\n",
            "Epoch 122/500\n",
            "49/49 [==============================] - 6s 127ms/step - loss: 1.6780 - accuracy: 0.5964 - val_loss: 14.7499 - val_accuracy: 0.1576\n",
            "Epoch 123/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.6613 - accuracy: 0.5989 - val_loss: 14.7911 - val_accuracy: 0.1547\n",
            "Epoch 124/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 1.6488 - accuracy: 0.6012 - val_loss: 14.8621 - val_accuracy: 0.1553\n",
            "Epoch 125/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.6266 - accuracy: 0.6056 - val_loss: 14.9723 - val_accuracy: 0.1561\n",
            "Epoch 126/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.6159 - accuracy: 0.6070 - val_loss: 15.0494 - val_accuracy: 0.1544\n",
            "Epoch 127/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.5945 - accuracy: 0.6132 - val_loss: 15.1533 - val_accuracy: 0.1567\n",
            "Epoch 128/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.5808 - accuracy: 0.6162 - val_loss: 15.2010 - val_accuracy: 0.1542\n",
            "Epoch 129/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.5633 - accuracy: 0.6170 - val_loss: 15.2901 - val_accuracy: 0.1556\n",
            "Epoch 130/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.5503 - accuracy: 0.6210 - val_loss: 15.3684 - val_accuracy: 0.1571\n",
            "Epoch 131/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.5305 - accuracy: 0.6245 - val_loss: 15.5012 - val_accuracy: 0.1555\n",
            "Epoch 132/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.5129 - accuracy: 0.6299 - val_loss: 15.4648 - val_accuracy: 0.1543\n",
            "Epoch 133/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.4962 - accuracy: 0.6334 - val_loss: 15.5894 - val_accuracy: 0.1541\n",
            "Epoch 134/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.4830 - accuracy: 0.6349 - val_loss: 15.6469 - val_accuracy: 0.1572\n",
            "Epoch 135/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.4666 - accuracy: 0.6390 - val_loss: 15.8395 - val_accuracy: 0.1552\n",
            "Epoch 136/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 1.4598 - accuracy: 0.6396 - val_loss: 15.8693 - val_accuracy: 0.1579\n",
            "Epoch 137/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 1.4399 - accuracy: 0.6451 - val_loss: 15.9180 - val_accuracy: 0.1584\n",
            "Epoch 138/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 1.4181 - accuracy: 0.6487 - val_loss: 15.9841 - val_accuracy: 0.1589\n",
            "Epoch 139/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 1.4047 - accuracy: 0.6518 - val_loss: 16.0760 - val_accuracy: 0.1593\n",
            "Epoch 140/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.3929 - accuracy: 0.6539 - val_loss: 16.1744 - val_accuracy: 0.1563\n",
            "Epoch 141/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 1.3786 - accuracy: 0.6572 - val_loss: 16.2662 - val_accuracy: 0.1571\n",
            "Epoch 142/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.3623 - accuracy: 0.6598 - val_loss: 16.2996 - val_accuracy: 0.1569\n",
            "Epoch 143/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.3500 - accuracy: 0.6633 - val_loss: 16.4113 - val_accuracy: 0.1584\n",
            "Epoch 144/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.3407 - accuracy: 0.6654 - val_loss: 16.4309 - val_accuracy: 0.1557\n",
            "Epoch 145/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.3246 - accuracy: 0.6691 - val_loss: 16.5291 - val_accuracy: 0.1578\n",
            "Epoch 146/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.3123 - accuracy: 0.6693 - val_loss: 16.6322 - val_accuracy: 0.1546\n",
            "Epoch 147/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.2965 - accuracy: 0.6742 - val_loss: 16.7601 - val_accuracy: 0.1564\n",
            "Epoch 148/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.2819 - accuracy: 0.6790 - val_loss: 16.7976 - val_accuracy: 0.1574\n",
            "Epoch 149/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.2701 - accuracy: 0.6809 - val_loss: 16.8730 - val_accuracy: 0.1577\n",
            "Epoch 150/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.2655 - accuracy: 0.6819 - val_loss: 17.0050 - val_accuracy: 0.1579\n",
            "Epoch 151/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.2477 - accuracy: 0.6845 - val_loss: 17.0287 - val_accuracy: 0.1585\n",
            "Epoch 152/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.2342 - accuracy: 0.6882 - val_loss: 17.1032 - val_accuracy: 0.1578\n",
            "Epoch 153/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.2186 - accuracy: 0.6915 - val_loss: 17.2078 - val_accuracy: 0.1588\n",
            "Epoch 154/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 1.2065 - accuracy: 0.6931 - val_loss: 17.3245 - val_accuracy: 0.1595\n",
            "Epoch 155/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 1.1995 - accuracy: 0.6948 - val_loss: 17.3783 - val_accuracy: 0.1587\n",
            "Epoch 156/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.1846 - accuracy: 0.6982 - val_loss: 17.4648 - val_accuracy: 0.1569\n",
            "Epoch 157/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.1715 - accuracy: 0.7024 - val_loss: 17.5684 - val_accuracy: 0.1584\n",
            "Epoch 158/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 1.1581 - accuracy: 0.7035 - val_loss: 17.6322 - val_accuracy: 0.1588\n",
            "Epoch 159/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.1425 - accuracy: 0.7076 - val_loss: 17.6747 - val_accuracy: 0.1591\n",
            "Epoch 160/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 1.1378 - accuracy: 0.7101 - val_loss: 17.7885 - val_accuracy: 0.1603\n",
            "Epoch 161/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 1.1211 - accuracy: 0.7129 - val_loss: 17.8205 - val_accuracy: 0.1600\n",
            "Epoch 162/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 1.1141 - accuracy: 0.7158 - val_loss: 17.8503 - val_accuracy: 0.1593\n",
            "Epoch 163/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.1038 - accuracy: 0.7140 - val_loss: 17.9808 - val_accuracy: 0.1595\n",
            "Epoch 164/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 1.0873 - accuracy: 0.7193 - val_loss: 17.9805 - val_accuracy: 0.1577\n",
            "Epoch 165/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.0792 - accuracy: 0.7217 - val_loss: 18.1035 - val_accuracy: 0.1596\n",
            "Epoch 166/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 1.0723 - accuracy: 0.7227 - val_loss: 18.1294 - val_accuracy: 0.1600\n",
            "Epoch 167/500\n",
            "49/49 [==============================] - 6s 114ms/step - loss: 1.0635 - accuracy: 0.7250 - val_loss: 18.2851 - val_accuracy: 0.1595\n",
            "Epoch 168/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.0498 - accuracy: 0.7283 - val_loss: 18.1948 - val_accuracy: 0.1599\n",
            "Epoch 169/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.0368 - accuracy: 0.7317 - val_loss: 18.4714 - val_accuracy: 0.1602\n",
            "Epoch 170/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.0226 - accuracy: 0.7337 - val_loss: 18.4944 - val_accuracy: 0.1594\n",
            "Epoch 171/500\n",
            "49/49 [==============================] - 6s 127ms/step - loss: 1.0133 - accuracy: 0.7370 - val_loss: 18.5611 - val_accuracy: 0.1614\n",
            "Epoch 172/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 1.0009 - accuracy: 0.7390 - val_loss: 18.6763 - val_accuracy: 0.1608\n",
            "Epoch 173/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 0.9874 - accuracy: 0.7424 - val_loss: 18.6878 - val_accuracy: 0.1629\n",
            "Epoch 174/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.9829 - accuracy: 0.7440 - val_loss: 18.7507 - val_accuracy: 0.1611\n",
            "Epoch 175/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.9731 - accuracy: 0.7467 - val_loss: 18.8897 - val_accuracy: 0.1603\n",
            "Epoch 176/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.9633 - accuracy: 0.7472 - val_loss: 18.9476 - val_accuracy: 0.1605\n",
            "Epoch 177/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.9500 - accuracy: 0.7517 - val_loss: 19.1040 - val_accuracy: 0.1619\n",
            "Epoch 178/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.9459 - accuracy: 0.7511 - val_loss: 19.1491 - val_accuracy: 0.1614\n",
            "Epoch 179/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 0.9303 - accuracy: 0.7548 - val_loss: 19.2218 - val_accuracy: 0.1611\n",
            "Epoch 180/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.9230 - accuracy: 0.7572 - val_loss: 19.2563 - val_accuracy: 0.1609\n",
            "Epoch 181/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.9180 - accuracy: 0.7587 - val_loss: 19.4002 - val_accuracy: 0.1621\n",
            "Epoch 182/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 0.9030 - accuracy: 0.7628 - val_loss: 19.3996 - val_accuracy: 0.1613\n",
            "Epoch 183/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.8960 - accuracy: 0.7640 - val_loss: 19.4596 - val_accuracy: 0.1610\n",
            "Epoch 184/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 0.8910 - accuracy: 0.7635 - val_loss: 19.5183 - val_accuracy: 0.1636\n",
            "Epoch 185/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.8783 - accuracy: 0.7687 - val_loss: 19.6795 - val_accuracy: 0.1626\n",
            "Epoch 186/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.8816 - accuracy: 0.7649 - val_loss: 19.6979 - val_accuracy: 0.1607\n",
            "Epoch 187/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.8655 - accuracy: 0.7705 - val_loss: 19.7333 - val_accuracy: 0.1629\n",
            "Epoch 188/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.8561 - accuracy: 0.7730 - val_loss: 19.8698 - val_accuracy: 0.1611\n",
            "Epoch 189/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.8390 - accuracy: 0.7768 - val_loss: 19.9302 - val_accuracy: 0.1633\n",
            "Epoch 190/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.8400 - accuracy: 0.7771 - val_loss: 19.9287 - val_accuracy: 0.1611\n",
            "Epoch 191/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.8286 - accuracy: 0.7776 - val_loss: 20.0608 - val_accuracy: 0.1611\n",
            "Epoch 192/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.8180 - accuracy: 0.7812 - val_loss: 20.1531 - val_accuracy: 0.1628\n",
            "Epoch 193/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.8115 - accuracy: 0.7835 - val_loss: 20.2731 - val_accuracy: 0.1631\n",
            "Epoch 194/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.7978 - accuracy: 0.7869 - val_loss: 20.3528 - val_accuracy: 0.1619\n",
            "Epoch 195/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.7905 - accuracy: 0.7870 - val_loss: 20.3721 - val_accuracy: 0.1625\n",
            "Epoch 196/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.7785 - accuracy: 0.7925 - val_loss: 20.4516 - val_accuracy: 0.1631\n",
            "Epoch 197/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.7771 - accuracy: 0.7914 - val_loss: 20.4891 - val_accuracy: 0.1632\n",
            "Epoch 198/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.7696 - accuracy: 0.7927 - val_loss: 20.5936 - val_accuracy: 0.1619\n",
            "Epoch 199/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 0.7656 - accuracy: 0.7932 - val_loss: 20.6800 - val_accuracy: 0.1638\n",
            "Epoch 200/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.7562 - accuracy: 0.7967 - val_loss: 20.7331 - val_accuracy: 0.1637\n",
            "Epoch 201/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.7511 - accuracy: 0.7979 - val_loss: 20.8114 - val_accuracy: 0.1629\n",
            "Epoch 202/500\n",
            "49/49 [==============================] - 6s 126ms/step - loss: 0.7419 - accuracy: 0.7991 - val_loss: 20.8923 - val_accuracy: 0.1643\n",
            "Epoch 203/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 0.7348 - accuracy: 0.8012 - val_loss: 20.9506 - val_accuracy: 0.1652\n",
            "Epoch 204/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.7225 - accuracy: 0.8048 - val_loss: 20.9912 - val_accuracy: 0.1643\n",
            "Epoch 205/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.7206 - accuracy: 0.8058 - val_loss: 21.1011 - val_accuracy: 0.1638\n",
            "Epoch 206/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.7159 - accuracy: 0.8052 - val_loss: 21.1264 - val_accuracy: 0.1626\n",
            "Epoch 207/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.7063 - accuracy: 0.8090 - val_loss: 21.1657 - val_accuracy: 0.1639\n",
            "Epoch 208/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.6956 - accuracy: 0.8114 - val_loss: 21.3179 - val_accuracy: 0.1635\n",
            "Epoch 209/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 0.6879 - accuracy: 0.8135 - val_loss: 21.4286 - val_accuracy: 0.1636\n",
            "Epoch 210/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.6829 - accuracy: 0.8152 - val_loss: 21.4671 - val_accuracy: 0.1626\n",
            "Epoch 211/500\n",
            "49/49 [==============================] - 6s 115ms/step - loss: 0.6807 - accuracy: 0.8147 - val_loss: 21.4572 - val_accuracy: 0.1632\n",
            "Epoch 212/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.6701 - accuracy: 0.8173 - val_loss: 21.5981 - val_accuracy: 0.1638\n",
            "Epoch 213/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.6659 - accuracy: 0.8174 - val_loss: 21.6587 - val_accuracy: 0.1638\n",
            "Epoch 214/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.6545 - accuracy: 0.8213 - val_loss: 21.6496 - val_accuracy: 0.1632\n",
            "Epoch 215/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.6488 - accuracy: 0.8221 - val_loss: 21.6706 - val_accuracy: 0.1621\n",
            "Epoch 216/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.6426 - accuracy: 0.8240 - val_loss: 21.8553 - val_accuracy: 0.1632\n",
            "Epoch 217/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.6356 - accuracy: 0.8271 - val_loss: 21.8471 - val_accuracy: 0.1626\n",
            "Epoch 218/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.6257 - accuracy: 0.8286 - val_loss: 22.0034 - val_accuracy: 0.1645\n",
            "Epoch 219/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.6252 - accuracy: 0.8281 - val_loss: 22.0694 - val_accuracy: 0.1643\n",
            "Epoch 220/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.6199 - accuracy: 0.8295 - val_loss: 22.0717 - val_accuracy: 0.1624\n",
            "Epoch 221/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.6109 - accuracy: 0.8318 - val_loss: 22.2571 - val_accuracy: 0.1633\n",
            "Epoch 222/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.5999 - accuracy: 0.8342 - val_loss: 22.2218 - val_accuracy: 0.1640\n",
            "Epoch 223/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.6049 - accuracy: 0.8332 - val_loss: 22.3047 - val_accuracy: 0.1636\n",
            "Epoch 224/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.5914 - accuracy: 0.8362 - val_loss: 22.3960 - val_accuracy: 0.1635\n",
            "Epoch 225/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.5872 - accuracy: 0.8382 - val_loss: 22.3962 - val_accuracy: 0.1635\n",
            "Epoch 226/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 0.5820 - accuracy: 0.8393 - val_loss: 22.5189 - val_accuracy: 0.1643\n",
            "Epoch 227/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.5748 - accuracy: 0.8412 - val_loss: 22.6355 - val_accuracy: 0.1647\n",
            "Epoch 228/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 0.5725 - accuracy: 0.8415 - val_loss: 22.7090 - val_accuracy: 0.1641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3feb880630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoP-_Ot7AO4Z",
        "colab_type": "text"
      },
      "source": [
        "## V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK14bGSjeOwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZeVf9SAnCmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a441bf2f-2fc8-46f7-c3cd-ee9f9f496658"
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-3.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "49/49 [==============================] - 6s 126ms/step - loss: 7.8511 - accuracy: 0.0482 - val_loss: 7.2717 - val_accuracy: 0.0643\n",
            "Epoch 2/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 7.1611 - accuracy: 0.0653 - val_loss: 7.2390 - val_accuracy: 0.0643\n",
            "Epoch 3/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 7.0862 - accuracy: 0.0653 - val_loss: 7.1782 - val_accuracy: 0.0643\n",
            "Epoch 4/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 7.0221 - accuracy: 0.0653 - val_loss: 7.1771 - val_accuracy: 0.0643\n",
            "Epoch 5/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 6.9842 - accuracy: 0.0653 - val_loss: 7.1751 - val_accuracy: 0.0643\n",
            "Epoch 6/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 6.9300 - accuracy: 0.0673 - val_loss: 7.1258 - val_accuracy: 0.0729\n",
            "Epoch 7/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 6.8353 - accuracy: 0.0724 - val_loss: 7.0439 - val_accuracy: 0.0773\n",
            "Epoch 8/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.7140 - accuracy: 0.0789 - val_loss: 6.9856 - val_accuracy: 0.0833\n",
            "Epoch 9/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 6.6062 - accuracy: 0.0851 - val_loss: 6.9560 - val_accuracy: 0.0851\n",
            "Epoch 10/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.5149 - accuracy: 0.0892 - val_loss: 6.9468 - val_accuracy: 0.0870\n",
            "Epoch 11/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.4370 - accuracy: 0.0933 - val_loss: 6.9373 - val_accuracy: 0.0897\n",
            "Epoch 12/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 6.3588 - accuracy: 0.0971 - val_loss: 6.9302 - val_accuracy: 0.0923\n",
            "Epoch 13/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.2783 - accuracy: 0.1028 - val_loss: 6.9158 - val_accuracy: 0.0962\n",
            "Epoch 14/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.1933 - accuracy: 0.1072 - val_loss: 6.9049 - val_accuracy: 0.0995\n",
            "Epoch 15/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.1111 - accuracy: 0.1110 - val_loss: 6.8953 - val_accuracy: 0.1022\n",
            "Epoch 16/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 6.0318 - accuracy: 0.1150 - val_loss: 6.9013 - val_accuracy: 0.1039\n",
            "Epoch 17/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.9581 - accuracy: 0.1176 - val_loss: 6.9097 - val_accuracy: 0.1055\n",
            "Epoch 18/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.8890 - accuracy: 0.1211 - val_loss: 6.9235 - val_accuracy: 0.1097\n",
            "Epoch 19/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.8224 - accuracy: 0.1238 - val_loss: 6.9428 - val_accuracy: 0.1117\n",
            "Epoch 20/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.7580 - accuracy: 0.1272 - val_loss: 6.9433 - val_accuracy: 0.1136\n",
            "Epoch 21/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.6933 - accuracy: 0.1291 - val_loss: 6.9676 - val_accuracy: 0.1157\n",
            "Epoch 22/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.6337 - accuracy: 0.1328 - val_loss: 6.9900 - val_accuracy: 0.1175\n",
            "Epoch 23/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.5736 - accuracy: 0.1351 - val_loss: 7.0100 - val_accuracy: 0.1182\n",
            "Epoch 24/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.5132 - accuracy: 0.1379 - val_loss: 7.0415 - val_accuracy: 0.1203\n",
            "Epoch 25/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.4546 - accuracy: 0.1409 - val_loss: 7.0750 - val_accuracy: 0.1217\n",
            "Epoch 26/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.4008 - accuracy: 0.1439 - val_loss: 7.0960 - val_accuracy: 0.1253\n",
            "Epoch 27/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.3451 - accuracy: 0.1459 - val_loss: 7.1198 - val_accuracy: 0.1257\n",
            "Epoch 28/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.2908 - accuracy: 0.1494 - val_loss: 7.1575 - val_accuracy: 0.1273\n",
            "Epoch 29/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.2389 - accuracy: 0.1518 - val_loss: 7.1785 - val_accuracy: 0.1282\n",
            "Epoch 30/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.1841 - accuracy: 0.1538 - val_loss: 7.2275 - val_accuracy: 0.1293\n",
            "Epoch 31/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 5.1324 - accuracy: 0.1576 - val_loss: 7.2500 - val_accuracy: 0.1292\n",
            "Epoch 32/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 5.0840 - accuracy: 0.1589 - val_loss: 7.2979 - val_accuracy: 0.1322\n",
            "Epoch 33/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.0340 - accuracy: 0.1612 - val_loss: 7.3385 - val_accuracy: 0.1349\n",
            "Epoch 34/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.9872 - accuracy: 0.1636 - val_loss: 7.3515 - val_accuracy: 0.1343\n",
            "Epoch 35/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 4.9378 - accuracy: 0.1662 - val_loss: 7.4067 - val_accuracy: 0.1355\n",
            "Epoch 36/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 4.8879 - accuracy: 0.1685 - val_loss: 7.4657 - val_accuracy: 0.1354\n",
            "Epoch 37/500\n",
            "49/49 [==============================] - 6s 130ms/step - loss: 4.8382 - accuracy: 0.1722 - val_loss: 7.4855 - val_accuracy: 0.1367\n",
            "Epoch 38/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 4.7907 - accuracy: 0.1746 - val_loss: 7.5264 - val_accuracy: 0.1387\n",
            "Epoch 39/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 4.7431 - accuracy: 0.1778 - val_loss: 7.5627 - val_accuracy: 0.1416\n",
            "Epoch 40/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 4.6949 - accuracy: 0.1800 - val_loss: 7.6152 - val_accuracy: 0.1411\n",
            "Epoch 41/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.6487 - accuracy: 0.1833 - val_loss: 7.6521 - val_accuracy: 0.1415\n",
            "Epoch 42/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.6018 - accuracy: 0.1862 - val_loss: 7.6882 - val_accuracy: 0.1415\n",
            "Epoch 43/500\n",
            "49/49 [==============================] - 7s 133ms/step - loss: 4.5567 - accuracy: 0.1899 - val_loss: 7.7436 - val_accuracy: 0.1440\n",
            "Epoch 44/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.5094 - accuracy: 0.1943 - val_loss: 7.8111 - val_accuracy: 0.1443\n",
            "Epoch 45/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.4647 - accuracy: 0.1965 - val_loss: 7.8273 - val_accuracy: 0.1447\n",
            "Epoch 46/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.4238 - accuracy: 0.1984 - val_loss: 7.8806 - val_accuracy: 0.1455\n",
            "Epoch 47/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.3801 - accuracy: 0.2029 - val_loss: 7.9261 - val_accuracy: 0.1464\n",
            "Epoch 48/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 4.3394 - accuracy: 0.2061 - val_loss: 7.9509 - val_accuracy: 0.1467\n",
            "Epoch 49/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.2977 - accuracy: 0.2102 - val_loss: 8.0202 - val_accuracy: 0.1480\n",
            "Epoch 50/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.2547 - accuracy: 0.2146 - val_loss: 8.0689 - val_accuracy: 0.1475\n",
            "Epoch 51/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.2173 - accuracy: 0.2170 - val_loss: 8.1083 - val_accuracy: 0.1463\n",
            "Epoch 52/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 4.1754 - accuracy: 0.2217 - val_loss: 8.1688 - val_accuracy: 0.1490\n",
            "Epoch 53/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 4.1426 - accuracy: 0.2245 - val_loss: 8.1930 - val_accuracy: 0.1458\n",
            "Epoch 54/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 4.1028 - accuracy: 0.2276 - val_loss: 8.2571 - val_accuracy: 0.1471\n",
            "Epoch 55/500\n",
            "49/49 [==============================] - 6s 130ms/step - loss: 4.0658 - accuracy: 0.2323 - val_loss: 8.2854 - val_accuracy: 0.1507\n",
            "Epoch 56/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 4.0311 - accuracy: 0.2347 - val_loss: 8.3609 - val_accuracy: 0.1502\n",
            "Epoch 57/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.0005 - accuracy: 0.2383 - val_loss: 8.4175 - val_accuracy: 0.1489\n",
            "Epoch 58/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.9618 - accuracy: 0.2419 - val_loss: 8.4418 - val_accuracy: 0.1491\n",
            "Epoch 59/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.9290 - accuracy: 0.2449 - val_loss: 8.5110 - val_accuracy: 0.1489\n",
            "Epoch 60/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.9030 - accuracy: 0.2482 - val_loss: 8.5190 - val_accuracy: 0.1493\n",
            "Epoch 61/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 3.8660 - accuracy: 0.2515 - val_loss: 8.5913 - val_accuracy: 0.1509\n",
            "Epoch 62/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.8364 - accuracy: 0.2542 - val_loss: 8.6288 - val_accuracy: 0.1471\n",
            "Epoch 63/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.8095 - accuracy: 0.2577 - val_loss: 8.7083 - val_accuracy: 0.1507\n",
            "Epoch 64/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.7766 - accuracy: 0.2601 - val_loss: 8.7507 - val_accuracy: 0.1493\n",
            "Epoch 65/500\n",
            "49/49 [==============================] - 6s 131ms/step - loss: 3.7497 - accuracy: 0.2638 - val_loss: 8.8040 - val_accuracy: 0.1511\n",
            "Epoch 66/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.7216 - accuracy: 0.2652 - val_loss: 8.8302 - val_accuracy: 0.1488\n",
            "Epoch 67/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.6935 - accuracy: 0.2693 - val_loss: 8.8959 - val_accuracy: 0.1490\n",
            "Epoch 68/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.6708 - accuracy: 0.2696 - val_loss: 8.9119 - val_accuracy: 0.1489\n",
            "Epoch 69/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.6429 - accuracy: 0.2740 - val_loss: 8.9631 - val_accuracy: 0.1493\n",
            "Epoch 70/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.6199 - accuracy: 0.2780 - val_loss: 8.9925 - val_accuracy: 0.1493\n",
            "Epoch 71/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.5855 - accuracy: 0.2815 - val_loss: 9.0471 - val_accuracy: 0.1483\n",
            "Epoch 72/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.5624 - accuracy: 0.2848 - val_loss: 9.1020 - val_accuracy: 0.1511\n",
            "Epoch 73/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.5379 - accuracy: 0.2855 - val_loss: 9.1270 - val_accuracy: 0.1484\n",
            "Epoch 74/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.5100 - accuracy: 0.2896 - val_loss: 9.2085 - val_accuracy: 0.1497\n",
            "Epoch 75/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.4859 - accuracy: 0.2914 - val_loss: 9.2706 - val_accuracy: 0.1510\n",
            "Epoch 76/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.4654 - accuracy: 0.2946 - val_loss: 9.2910 - val_accuracy: 0.1508\n",
            "Epoch 77/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.4416 - accuracy: 0.2977 - val_loss: 9.3892 - val_accuracy: 0.1506\n",
            "Epoch 78/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.4219 - accuracy: 0.2995 - val_loss: 9.4155 - val_accuracy: 0.1502\n",
            "Epoch 79/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 3.3992 - accuracy: 0.3025 - val_loss: 9.4585 - val_accuracy: 0.1518\n",
            "Epoch 80/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.3699 - accuracy: 0.3065 - val_loss: 9.4911 - val_accuracy: 0.1510\n",
            "Epoch 81/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.3488 - accuracy: 0.3098 - val_loss: 9.5238 - val_accuracy: 0.1512\n",
            "Epoch 82/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.3309 - accuracy: 0.3110 - val_loss: 9.5873 - val_accuracy: 0.1497\n",
            "Epoch 83/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.3098 - accuracy: 0.3138 - val_loss: 9.6586 - val_accuracy: 0.1516\n",
            "Epoch 84/500\n",
            "49/49 [==============================] - 6s 130ms/step - loss: 3.2882 - accuracy: 0.3169 - val_loss: 9.6822 - val_accuracy: 0.1527\n",
            "Epoch 85/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 3.2664 - accuracy: 0.3186 - val_loss: 9.7109 - val_accuracy: 0.1508\n",
            "Epoch 86/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.2483 - accuracy: 0.3225 - val_loss: 9.7712 - val_accuracy: 0.1501\n",
            "Epoch 87/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.2283 - accuracy: 0.3238 - val_loss: 9.8121 - val_accuracy: 0.1509\n",
            "Epoch 88/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.2057 - accuracy: 0.3273 - val_loss: 9.8656 - val_accuracy: 0.1507\n",
            "Epoch 89/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.1827 - accuracy: 0.3294 - val_loss: 9.8903 - val_accuracy: 0.1512\n",
            "Epoch 90/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.1655 - accuracy: 0.3320 - val_loss: 9.9739 - val_accuracy: 0.1516\n",
            "Epoch 91/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.1422 - accuracy: 0.3358 - val_loss: 10.0214 - val_accuracy: 0.1512\n",
            "Epoch 92/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.1262 - accuracy: 0.3375 - val_loss: 10.0333 - val_accuracy: 0.1502\n",
            "Epoch 93/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 3.1070 - accuracy: 0.3405 - val_loss: 10.0894 - val_accuracy: 0.1523\n",
            "Epoch 94/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.0912 - accuracy: 0.3428 - val_loss: 10.1493 - val_accuracy: 0.1520\n",
            "Epoch 95/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 3.0685 - accuracy: 0.3454 - val_loss: 10.1875 - val_accuracy: 0.1499\n",
            "Epoch 96/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.0520 - accuracy: 0.3463 - val_loss: 10.1856 - val_accuracy: 0.1499\n",
            "Epoch 97/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.0310 - accuracy: 0.3501 - val_loss: 10.2835 - val_accuracy: 0.1523\n",
            "Epoch 98/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.0154 - accuracy: 0.3537 - val_loss: 10.3102 - val_accuracy: 0.1497\n",
            "Epoch 99/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.9942 - accuracy: 0.3552 - val_loss: 10.3516 - val_accuracy: 0.1520\n",
            "Epoch 100/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.9731 - accuracy: 0.3575 - val_loss: 10.3936 - val_accuracy: 0.1511\n",
            "Epoch 101/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.9620 - accuracy: 0.3609 - val_loss: 10.4488 - val_accuracy: 0.1526\n",
            "Epoch 102/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.9422 - accuracy: 0.3618 - val_loss: 10.4824 - val_accuracy: 0.1517\n",
            "Epoch 103/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.9260 - accuracy: 0.3650 - val_loss: 10.5721 - val_accuracy: 0.1521\n",
            "Epoch 104/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.9089 - accuracy: 0.3677 - val_loss: 10.5647 - val_accuracy: 0.1506\n",
            "Epoch 105/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 2.8874 - accuracy: 0.3691 - val_loss: 10.6379 - val_accuracy: 0.1506\n",
            "Epoch 106/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 2.8722 - accuracy: 0.3709 - val_loss: 10.7011 - val_accuracy: 0.1512\n",
            "Epoch 107/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.8548 - accuracy: 0.3756 - val_loss: 10.7605 - val_accuracy: 0.1523\n",
            "Epoch 108/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.8360 - accuracy: 0.3777 - val_loss: 10.7778 - val_accuracy: 0.1521\n",
            "Epoch 109/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 2.8193 - accuracy: 0.3805 - val_loss: 10.8184 - val_accuracy: 0.1502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f40179770f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_8icpaa_4IP",
        "colab_type": "text"
      },
      "source": [
        "## V4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDo87F7AnUdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5c143a1a-05d5-4f4d-d9b8-1fa9eff074be"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(64))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaybHfIeq1ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-4.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y35orGI_7Mh",
        "colab_type": "text"
      },
      "source": [
        "## V5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNmL-cAkspbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cad1ffb7-f4b5-471e-dd04-2fb805cc62aa"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7xulino73AY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dde60ada-e8f9-4883-9d06-0d804d176563"
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-5.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "49/49 [==============================] - 9s 179ms/step - loss: 7.7985 - accuracy: 0.0581 - val_loss: 7.2744 - val_accuracy: 0.0643\n",
            "Epoch 2/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 7.1743 - accuracy: 0.0653 - val_loss: 7.2723 - val_accuracy: 0.0643\n",
            "Epoch 3/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 7.1487 - accuracy: 0.0653 - val_loss: 7.2874 - val_accuracy: 0.0643\n",
            "Epoch 4/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 7.1412 - accuracy: 0.0653 - val_loss: 7.3000 - val_accuracy: 0.0643\n",
            "Epoch 5/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 7.1355 - accuracy: 0.0653 - val_loss: 7.3265 - val_accuracy: 0.0643\n",
            "Epoch 6/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 7.1272 - accuracy: 0.0653 - val_loss: 7.3156 - val_accuracy: 0.0643\n",
            "Epoch 7/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 7.0507 - accuracy: 0.0653 - val_loss: 7.1872 - val_accuracy: 0.0643\n",
            "Epoch 8/500\n",
            "49/49 [==============================] - 8s 172ms/step - loss: 6.9704 - accuracy: 0.0660 - val_loss: 7.1451 - val_accuracy: 0.0699\n",
            "Epoch 9/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 6.8695 - accuracy: 0.0712 - val_loss: 7.0955 - val_accuracy: 0.0741\n",
            "Epoch 10/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 6.7722 - accuracy: 0.0760 - val_loss: 7.0572 - val_accuracy: 0.0784\n",
            "Epoch 11/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 6.6854 - accuracy: 0.0790 - val_loss: 7.0139 - val_accuracy: 0.0791\n",
            "Epoch 12/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 6.6035 - accuracy: 0.0816 - val_loss: 6.9884 - val_accuracy: 0.0829\n",
            "Epoch 13/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 6.5337 - accuracy: 0.0852 - val_loss: 6.9852 - val_accuracy: 0.0831\n",
            "Epoch 14/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 6.4757 - accuracy: 0.0880 - val_loss: 6.9899 - val_accuracy: 0.0861\n",
            "Epoch 15/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 6.4118 - accuracy: 0.0917 - val_loss: 6.9918 - val_accuracy: 0.0885\n",
            "Epoch 16/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 6.3512 - accuracy: 0.0947 - val_loss: 6.9951 - val_accuracy: 0.0899\n",
            "Epoch 17/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 6.2842 - accuracy: 0.0980 - val_loss: 6.9883 - val_accuracy: 0.0928\n",
            "Epoch 18/500\n",
            "49/49 [==============================] - 8s 172ms/step - loss: 6.2173 - accuracy: 0.1008 - val_loss: 6.9932 - val_accuracy: 0.0941\n",
            "Epoch 19/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 6.1516 - accuracy: 0.1036 - val_loss: 6.9893 - val_accuracy: 0.0954\n",
            "Epoch 20/500\n",
            "49/49 [==============================] - 8s 173ms/step - loss: 6.0840 - accuracy: 0.1061 - val_loss: 7.0033 - val_accuracy: 0.0972\n",
            "Epoch 21/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 6.0172 - accuracy: 0.1092 - val_loss: 6.9914 - val_accuracy: 0.0987\n",
            "Epoch 22/500\n",
            "49/49 [==============================] - 8s 172ms/step - loss: 5.9549 - accuracy: 0.1122 - val_loss: 6.9945 - val_accuracy: 0.1000\n",
            "Epoch 23/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 5.8937 - accuracy: 0.1146 - val_loss: 6.9991 - val_accuracy: 0.1014\n",
            "Epoch 24/500\n",
            "49/49 [==============================] - 9s 174ms/step - loss: 5.8368 - accuracy: 0.1151 - val_loss: 7.0173 - val_accuracy: 0.1033\n",
            "Epoch 25/500\n",
            "49/49 [==============================] - 8s 168ms/step - loss: 5.7796 - accuracy: 0.1180 - val_loss: 7.0407 - val_accuracy: 0.1063\n",
            "Epoch 26/500\n",
            "49/49 [==============================] - 8s 173ms/step - loss: 5.7308 - accuracy: 0.1203 - val_loss: 7.0524 - val_accuracy: 0.1073\n",
            "Epoch 27/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 5.6845 - accuracy: 0.1225 - val_loss: 7.0796 - val_accuracy: 0.1080\n",
            "Epoch 28/500\n",
            "49/49 [==============================] - 8s 173ms/step - loss: 5.6351 - accuracy: 0.1233 - val_loss: 7.1069 - val_accuracy: 0.1081\n",
            "Epoch 29/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 5.5898 - accuracy: 0.1243 - val_loss: 7.1241 - val_accuracy: 0.1098\n",
            "Epoch 30/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 5.5489 - accuracy: 0.1274 - val_loss: 7.1394 - val_accuracy: 0.1087\n",
            "Epoch 31/500\n",
            "49/49 [==============================] - 8s 173ms/step - loss: 5.5069 - accuracy: 0.1283 - val_loss: 7.1802 - val_accuracy: 0.1108\n",
            "Epoch 32/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 5.4659 - accuracy: 0.1302 - val_loss: 7.2033 - val_accuracy: 0.1112\n",
            "Epoch 33/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 5.4248 - accuracy: 0.1303 - val_loss: 7.2135 - val_accuracy: 0.1109\n",
            "Epoch 34/500\n",
            "49/49 [==============================] - 9s 174ms/step - loss: 5.3846 - accuracy: 0.1341 - val_loss: 7.2527 - val_accuracy: 0.1134\n",
            "Epoch 35/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 5.3485 - accuracy: 0.1351 - val_loss: 7.2804 - val_accuracy: 0.1139\n",
            "Epoch 36/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 5.3077 - accuracy: 0.1361 - val_loss: 7.3139 - val_accuracy: 0.1126\n",
            "Epoch 37/500\n",
            "49/49 [==============================] - 8s 173ms/step - loss: 5.2707 - accuracy: 0.1375 - val_loss: 7.3195 - val_accuracy: 0.1152\n",
            "Epoch 38/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 5.2301 - accuracy: 0.1393 - val_loss: 7.3431 - val_accuracy: 0.1159\n",
            "Epoch 39/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 5.1922 - accuracy: 0.1393 - val_loss: 7.3812 - val_accuracy: 0.1177\n",
            "Epoch 40/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 5.1575 - accuracy: 0.1425 - val_loss: 7.4029 - val_accuracy: 0.1175\n",
            "Epoch 41/500\n",
            "49/49 [==============================] - 9s 173ms/step - loss: 5.1190 - accuracy: 0.1444 - val_loss: 7.4238 - val_accuracy: 0.1181\n",
            "Epoch 42/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 5.0848 - accuracy: 0.1441 - val_loss: 7.4541 - val_accuracy: 0.1193\n",
            "Epoch 43/500\n",
            "49/49 [==============================] - 8s 173ms/step - loss: 5.0471 - accuracy: 0.1467 - val_loss: 7.4749 - val_accuracy: 0.1194\n",
            "Epoch 44/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 5.0103 - accuracy: 0.1487 - val_loss: 7.5158 - val_accuracy: 0.1203\n",
            "Epoch 45/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 4.9799 - accuracy: 0.1495 - val_loss: 7.5548 - val_accuracy: 0.1203\n",
            "Epoch 46/500\n",
            "49/49 [==============================] - 8s 173ms/step - loss: 4.9399 - accuracy: 0.1511 - val_loss: 7.5819 - val_accuracy: 0.1207\n",
            "Epoch 47/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 4.9108 - accuracy: 0.1520 - val_loss: 7.6019 - val_accuracy: 0.1223\n",
            "Epoch 48/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 4.8733 - accuracy: 0.1550 - val_loss: 7.6270 - val_accuracy: 0.1228\n",
            "Epoch 49/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 4.8437 - accuracy: 0.1554 - val_loss: 7.6620 - val_accuracy: 0.1249\n",
            "Epoch 50/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 4.8100 - accuracy: 0.1565 - val_loss: 7.6892 - val_accuracy: 0.1235\n",
            "Epoch 51/500\n",
            "49/49 [==============================] - 8s 173ms/step - loss: 4.7770 - accuracy: 0.1582 - val_loss: 7.7152 - val_accuracy: 0.1256\n",
            "Epoch 52/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 4.7468 - accuracy: 0.1597 - val_loss: 7.7295 - val_accuracy: 0.1252\n",
            "Epoch 53/500\n",
            "49/49 [==============================] - 9s 176ms/step - loss: 4.7101 - accuracy: 0.1618 - val_loss: 7.7832 - val_accuracy: 0.1259\n",
            "Epoch 54/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 4.6803 - accuracy: 0.1635 - val_loss: 7.8152 - val_accuracy: 0.1275\n",
            "Epoch 55/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 4.6565 - accuracy: 0.1628 - val_loss: 7.8415 - val_accuracy: 0.1271\n",
            "Epoch 56/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 4.6244 - accuracy: 0.1666 - val_loss: 7.8644 - val_accuracy: 0.1267\n",
            "Epoch 57/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 4.5992 - accuracy: 0.1692 - val_loss: 7.8950 - val_accuracy: 0.1267\n",
            "Epoch 58/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 4.5714 - accuracy: 0.1697 - val_loss: 7.9255 - val_accuracy: 0.1274\n",
            "Epoch 59/500\n",
            "49/49 [==============================] - 9s 176ms/step - loss: 4.5420 - accuracy: 0.1714 - val_loss: 7.9307 - val_accuracy: 0.1291\n",
            "Epoch 60/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 4.5123 - accuracy: 0.1738 - val_loss: 7.9825 - val_accuracy: 0.1273\n",
            "Epoch 61/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 4.4891 - accuracy: 0.1762 - val_loss: 8.0204 - val_accuracy: 0.1287\n",
            "Epoch 62/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 4.4662 - accuracy: 0.1757 - val_loss: 8.0301 - val_accuracy: 0.1287\n",
            "Epoch 63/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 4.4386 - accuracy: 0.1787 - val_loss: 8.0715 - val_accuracy: 0.1282\n",
            "Epoch 64/500\n",
            "49/49 [==============================] - 9s 176ms/step - loss: 4.4154 - accuracy: 0.1788 - val_loss: 8.0945 - val_accuracy: 0.1298\n",
            "Epoch 65/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 4.3921 - accuracy: 0.1805 - val_loss: 8.1101 - val_accuracy: 0.1300\n",
            "Epoch 66/500\n",
            "49/49 [==============================] - 9s 176ms/step - loss: 4.3630 - accuracy: 0.1845 - val_loss: 8.1642 - val_accuracy: 0.1313\n",
            "Epoch 67/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 4.3407 - accuracy: 0.1848 - val_loss: 8.2085 - val_accuracy: 0.1311\n",
            "Epoch 68/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 4.3199 - accuracy: 0.1859 - val_loss: 8.2139 - val_accuracy: 0.1313\n",
            "Epoch 69/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 4.2927 - accuracy: 0.1889 - val_loss: 8.2440 - val_accuracy: 0.1299\n",
            "Epoch 70/500\n",
            "49/49 [==============================] - 9s 174ms/step - loss: 4.2726 - accuracy: 0.1897 - val_loss: 8.2823 - val_accuracy: 0.1324\n",
            "Epoch 71/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 4.2487 - accuracy: 0.1914 - val_loss: 8.3079 - val_accuracy: 0.1333\n",
            "Epoch 72/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 4.2255 - accuracy: 0.1937 - val_loss: 8.3488 - val_accuracy: 0.1320\n",
            "Epoch 73/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 4.2033 - accuracy: 0.1961 - val_loss: 8.3688 - val_accuracy: 0.1329\n",
            "Epoch 74/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 4.1834 - accuracy: 0.1970 - val_loss: 8.4186 - val_accuracy: 0.1325\n",
            "Epoch 75/500\n",
            "49/49 [==============================] - 9s 177ms/step - loss: 4.1681 - accuracy: 0.1988 - val_loss: 8.4204 - val_accuracy: 0.1340\n",
            "Epoch 76/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 4.1446 - accuracy: 0.1993 - val_loss: 8.4668 - val_accuracy: 0.1323\n",
            "Epoch 77/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 4.1261 - accuracy: 0.2008 - val_loss: 8.4751 - val_accuracy: 0.1317\n",
            "Epoch 78/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 4.1030 - accuracy: 0.2052 - val_loss: 8.5344 - val_accuracy: 0.1332\n",
            "Epoch 79/500\n",
            "49/49 [==============================] - 9s 177ms/step - loss: 4.0881 - accuracy: 0.2053 - val_loss: 8.5546 - val_accuracy: 0.1343\n",
            "Epoch 80/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 4.0825 - accuracy: 0.2061 - val_loss: 8.5648 - val_accuracy: 0.1343\n",
            "Epoch 81/500\n",
            "49/49 [==============================] - 9s 174ms/step - loss: 4.0486 - accuracy: 0.2092 - val_loss: 8.6315 - val_accuracy: 0.1344\n",
            "Epoch 82/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 4.0275 - accuracy: 0.2100 - val_loss: 8.6394 - val_accuracy: 0.1358\n",
            "Epoch 83/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 4.0129 - accuracy: 0.2117 - val_loss: 8.6847 - val_accuracy: 0.1331\n",
            "Epoch 84/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 3.9955 - accuracy: 0.2131 - val_loss: 8.7112 - val_accuracy: 0.1351\n",
            "Epoch 85/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 3.9685 - accuracy: 0.2162 - val_loss: 8.7217 - val_accuracy: 0.1359\n",
            "Epoch 86/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 3.9577 - accuracy: 0.2174 - val_loss: 8.7284 - val_accuracy: 0.1327\n",
            "Epoch 87/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 3.9446 - accuracy: 0.2176 - val_loss: 8.7872 - val_accuracy: 0.1350\n",
            "Epoch 88/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.9404 - accuracy: 0.2205 - val_loss: 8.8067 - val_accuracy: 0.1356\n",
            "Epoch 89/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.9161 - accuracy: 0.2222 - val_loss: 8.8718 - val_accuracy: 0.1355\n",
            "Epoch 90/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 3.8913 - accuracy: 0.2248 - val_loss: 8.8882 - val_accuracy: 0.1352\n",
            "Epoch 91/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 4.2213 - accuracy: 0.2079 - val_loss: 8.6238 - val_accuracy: 0.1241\n",
            "Epoch 92/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 4.2781 - accuracy: 0.2015 - val_loss: 8.9234 - val_accuracy: 0.1297\n",
            "Epoch 93/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 4.0404 - accuracy: 0.2156 - val_loss: 8.9992 - val_accuracy: 0.1323\n",
            "Epoch 94/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 3.9990 - accuracy: 0.2180 - val_loss: 9.0313 - val_accuracy: 0.1337\n",
            "Epoch 95/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.9461 - accuracy: 0.2224 - val_loss: 9.0717 - val_accuracy: 0.1329\n",
            "Epoch 96/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.9366 - accuracy: 0.2221 - val_loss: 9.1040 - val_accuracy: 0.1353\n",
            "Epoch 97/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.9056 - accuracy: 0.2246 - val_loss: 9.1431 - val_accuracy: 0.1347\n",
            "Epoch 98/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 3.8825 - accuracy: 0.2273 - val_loss: 9.2060 - val_accuracy: 0.1355\n",
            "Epoch 99/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.8502 - accuracy: 0.2292 - val_loss: 9.2085 - val_accuracy: 0.1359\n",
            "Epoch 100/500\n",
            "49/49 [==============================] - 9s 178ms/step - loss: 3.8461 - accuracy: 0.2303 - val_loss: 9.2365 - val_accuracy: 0.1360\n",
            "Epoch 101/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.8116 - accuracy: 0.2334 - val_loss: 9.2962 - val_accuracy: 0.1347\n",
            "Epoch 102/500\n",
            "49/49 [==============================] - 9s 176ms/step - loss: 3.7897 - accuracy: 0.2369 - val_loss: 9.3403 - val_accuracy: 0.1362\n",
            "Epoch 103/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.7758 - accuracy: 0.2383 - val_loss: 9.3642 - val_accuracy: 0.1345\n",
            "Epoch 104/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.7602 - accuracy: 0.2380 - val_loss: 9.3433 - val_accuracy: 0.1361\n",
            "Epoch 105/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.7346 - accuracy: 0.2416 - val_loss: 9.4028 - val_accuracy: 0.1350\n",
            "Epoch 106/500\n",
            "49/49 [==============================] - 8s 172ms/step - loss: 3.7440 - accuracy: 0.2406 - val_loss: 9.4013 - val_accuracy: 0.1369\n",
            "Epoch 107/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.7212 - accuracy: 0.2437 - val_loss: 9.4384 - val_accuracy: 0.1348\n",
            "Epoch 108/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.7011 - accuracy: 0.2460 - val_loss: 9.4640 - val_accuracy: 0.1355\n",
            "Epoch 109/500\n",
            "49/49 [==============================] - 9s 178ms/step - loss: 3.6951 - accuracy: 0.2465 - val_loss: 9.4955 - val_accuracy: 0.1383\n",
            "Epoch 110/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.6820 - accuracy: 0.2478 - val_loss: 9.5205 - val_accuracy: 0.1352\n",
            "Epoch 111/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.6613 - accuracy: 0.2501 - val_loss: 9.5531 - val_accuracy: 0.1352\n",
            "Epoch 112/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.6481 - accuracy: 0.2504 - val_loss: 9.5634 - val_accuracy: 0.1373\n",
            "Epoch 113/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.6389 - accuracy: 0.2529 - val_loss: 9.5799 - val_accuracy: 0.1350\n",
            "Epoch 114/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.6645 - accuracy: 0.2504 - val_loss: 9.5014 - val_accuracy: 0.1363\n",
            "Epoch 115/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.6427 - accuracy: 0.2528 - val_loss: 9.6464 - val_accuracy: 0.1368\n",
            "Epoch 116/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.6124 - accuracy: 0.2559 - val_loss: 9.6823 - val_accuracy: 0.1369\n",
            "Epoch 117/500\n",
            "49/49 [==============================] - 8s 159ms/step - loss: 3.5925 - accuracy: 0.2574 - val_loss: 9.6830 - val_accuracy: 0.1365\n",
            "Epoch 118/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 3.5933 - accuracy: 0.2584 - val_loss: 9.6863 - val_accuracy: 0.1361\n",
            "Epoch 119/500\n",
            "49/49 [==============================] - 8s 159ms/step - loss: 3.5609 - accuracy: 0.2604 - val_loss: 9.7521 - val_accuracy: 0.1367\n",
            "Epoch 120/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.5514 - accuracy: 0.2626 - val_loss: 9.7709 - val_accuracy: 0.1367\n",
            "Epoch 121/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.5372 - accuracy: 0.2627 - val_loss: 9.7915 - val_accuracy: 0.1363\n",
            "Epoch 122/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.5284 - accuracy: 0.2656 - val_loss: 9.8059 - val_accuracy: 0.1368\n",
            "Epoch 123/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.5137 - accuracy: 0.2665 - val_loss: 9.8182 - val_accuracy: 0.1371\n",
            "Epoch 124/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.5036 - accuracy: 0.2683 - val_loss: 9.8564 - val_accuracy: 0.1357\n",
            "Epoch 125/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.4872 - accuracy: 0.2702 - val_loss: 9.8882 - val_accuracy: 0.1374\n",
            "Epoch 126/500\n",
            "49/49 [==============================] - 9s 188ms/step - loss: 3.4809 - accuracy: 0.2705 - val_loss: 9.8911 - val_accuracy: 0.1385\n",
            "Epoch 127/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.4686 - accuracy: 0.2730 - val_loss: 9.9173 - val_accuracy: 0.1374\n",
            "Epoch 128/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 3.4692 - accuracy: 0.2711 - val_loss: 9.9036 - val_accuracy: 0.1378\n",
            "Epoch 129/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.4724 - accuracy: 0.2737 - val_loss: 9.9230 - val_accuracy: 0.1377\n",
            "Epoch 130/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.4457 - accuracy: 0.2751 - val_loss: 9.9513 - val_accuracy: 0.1373\n",
            "Epoch 131/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.4385 - accuracy: 0.2763 - val_loss: 9.9967 - val_accuracy: 0.1372\n",
            "Epoch 132/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.4291 - accuracy: 0.2777 - val_loss: 10.0040 - val_accuracy: 0.1365\n",
            "Epoch 133/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.4471 - accuracy: 0.2756 - val_loss: 10.0268 - val_accuracy: 0.1362\n",
            "Epoch 134/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.4275 - accuracy: 0.2768 - val_loss: 10.0377 - val_accuracy: 0.1373\n",
            "Epoch 135/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 3.4113 - accuracy: 0.2803 - val_loss: 10.0525 - val_accuracy: 0.1379\n",
            "Epoch 136/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.4034 - accuracy: 0.2827 - val_loss: 10.0672 - val_accuracy: 0.1382\n",
            "Epoch 137/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.5211 - accuracy: 0.2691 - val_loss: 9.8516 - val_accuracy: 0.1367\n",
            "Epoch 138/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.5124 - accuracy: 0.2701 - val_loss: 9.9267 - val_accuracy: 0.1349\n",
            "Epoch 139/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.4471 - accuracy: 0.2747 - val_loss: 10.1341 - val_accuracy: 0.1380\n",
            "Epoch 140/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 3.3839 - accuracy: 0.2842 - val_loss: 10.1364 - val_accuracy: 0.1390\n",
            "Epoch 141/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.3655 - accuracy: 0.2852 - val_loss: 10.1975 - val_accuracy: 0.1380\n",
            "Epoch 142/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.3494 - accuracy: 0.2883 - val_loss: 10.2130 - val_accuracy: 0.1375\n",
            "Epoch 143/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 3.3281 - accuracy: 0.2917 - val_loss: 10.2429 - val_accuracy: 0.1397\n",
            "Epoch 144/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.3194 - accuracy: 0.2900 - val_loss: 10.3142 - val_accuracy: 0.1385\n",
            "Epoch 145/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 3.3103 - accuracy: 0.2937 - val_loss: 10.3189 - val_accuracy: 0.1393\n",
            "Epoch 146/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 3.3063 - accuracy: 0.2930 - val_loss: 10.3519 - val_accuracy: 0.1391\n",
            "Epoch 147/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.2972 - accuracy: 0.2944 - val_loss: 10.3471 - val_accuracy: 0.1389\n",
            "Epoch 148/500\n",
            "49/49 [==============================] - 9s 176ms/step - loss: 3.2910 - accuracy: 0.2948 - val_loss: 10.3681 - val_accuracy: 0.1397\n",
            "Epoch 149/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.2736 - accuracy: 0.2977 - val_loss: 10.3891 - val_accuracy: 0.1397\n",
            "Epoch 150/500\n",
            "49/49 [==============================] - 9s 176ms/step - loss: 3.2653 - accuracy: 0.2990 - val_loss: 10.4197 - val_accuracy: 0.1403\n",
            "Epoch 151/500\n",
            "49/49 [==============================] - 8s 168ms/step - loss: 3.2528 - accuracy: 0.3021 - val_loss: 10.3952 - val_accuracy: 0.1411\n",
            "Epoch 152/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 3.2472 - accuracy: 0.3016 - val_loss: 10.4282 - val_accuracy: 0.1403\n",
            "Epoch 153/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.2378 - accuracy: 0.3026 - val_loss: 10.4751 - val_accuracy: 0.1384\n",
            "Epoch 154/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.2290 - accuracy: 0.3036 - val_loss: 10.4772 - val_accuracy: 0.1401\n",
            "Epoch 155/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.2321 - accuracy: 0.3034 - val_loss: 10.4807 - val_accuracy: 0.1409\n",
            "Epoch 156/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.2087 - accuracy: 0.3063 - val_loss: 10.5066 - val_accuracy: 0.1393\n",
            "Epoch 157/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 3.2014 - accuracy: 0.3079 - val_loss: 10.5462 - val_accuracy: 0.1424\n",
            "Epoch 158/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.2055 - accuracy: 0.3062 - val_loss: 10.5597 - val_accuracy: 0.1398\n",
            "Epoch 159/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.1962 - accuracy: 0.3097 - val_loss: 10.6026 - val_accuracy: 0.1413\n",
            "Epoch 160/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 3.1874 - accuracy: 0.3082 - val_loss: 10.6419 - val_accuracy: 0.1418\n",
            "Epoch 161/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.1843 - accuracy: 0.3099 - val_loss: 10.6243 - val_accuracy: 0.1419\n",
            "Epoch 162/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.1720 - accuracy: 0.3110 - val_loss: 10.6345 - val_accuracy: 0.1413\n",
            "Epoch 163/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 3.1647 - accuracy: 0.3133 - val_loss: 10.6728 - val_accuracy: 0.1411\n",
            "Epoch 164/500\n",
            "49/49 [==============================] - 9s 187ms/step - loss: 3.1570 - accuracy: 0.3137 - val_loss: 10.6763 - val_accuracy: 0.1426\n",
            "Epoch 165/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 3.1498 - accuracy: 0.3138 - val_loss: 10.7336 - val_accuracy: 0.1437\n",
            "Epoch 166/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.1396 - accuracy: 0.3171 - val_loss: 10.7532 - val_accuracy: 0.1399\n",
            "Epoch 167/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 3.1292 - accuracy: 0.3174 - val_loss: 10.7740 - val_accuracy: 0.1401\n",
            "Epoch 168/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.1255 - accuracy: 0.3176 - val_loss: 10.7988 - val_accuracy: 0.1414\n",
            "Epoch 169/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.1164 - accuracy: 0.3204 - val_loss: 10.7961 - val_accuracy: 0.1418\n",
            "Epoch 170/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.1125 - accuracy: 0.3207 - val_loss: 10.7776 - val_accuracy: 0.1412\n",
            "Epoch 171/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 3.1130 - accuracy: 0.3203 - val_loss: 10.8055 - val_accuracy: 0.1416\n",
            "Epoch 172/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.0950 - accuracy: 0.3227 - val_loss: 10.8744 - val_accuracy: 0.1414\n",
            "Epoch 173/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.0902 - accuracy: 0.3246 - val_loss: 10.9070 - val_accuracy: 0.1406\n",
            "Epoch 174/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 3.0804 - accuracy: 0.3272 - val_loss: 10.9075 - val_accuracy: 0.1411\n",
            "Epoch 175/500\n",
            "49/49 [==============================] - 9s 177ms/step - loss: 3.0695 - accuracy: 0.3265 - val_loss: 10.9334 - val_accuracy: 0.1438\n",
            "Epoch 176/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.0661 - accuracy: 0.3264 - val_loss: 10.9212 - val_accuracy: 0.1422\n",
            "Epoch 177/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 3.0561 - accuracy: 0.3284 - val_loss: 10.9487 - val_accuracy: 0.1424\n",
            "Epoch 178/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.0534 - accuracy: 0.3280 - val_loss: 10.9648 - val_accuracy: 0.1419\n",
            "Epoch 179/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.0415 - accuracy: 0.3299 - val_loss: 11.0106 - val_accuracy: 0.1434\n",
            "Epoch 180/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.0469 - accuracy: 0.3301 - val_loss: 11.0421 - val_accuracy: 0.1432\n",
            "Epoch 181/500\n",
            "49/49 [==============================] - 9s 178ms/step - loss: 3.0341 - accuracy: 0.3310 - val_loss: 10.9956 - val_accuracy: 0.1450\n",
            "Epoch 182/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.0185 - accuracy: 0.3336 - val_loss: 11.0694 - val_accuracy: 0.1434\n",
            "Epoch 183/500\n",
            "49/49 [==============================] - 8s 168ms/step - loss: 3.0171 - accuracy: 0.3345 - val_loss: 11.0703 - val_accuracy: 0.1433\n",
            "Epoch 184/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 3.0165 - accuracy: 0.3331 - val_loss: 11.0969 - val_accuracy: 0.1430\n",
            "Epoch 185/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 3.0069 - accuracy: 0.3367 - val_loss: 11.1267 - val_accuracy: 0.1424\n",
            "Epoch 186/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 3.0008 - accuracy: 0.3363 - val_loss: 11.0757 - val_accuracy: 0.1437\n",
            "Epoch 187/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.9907 - accuracy: 0.3371 - val_loss: 11.1273 - val_accuracy: 0.1427\n",
            "Epoch 188/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.9802 - accuracy: 0.3373 - val_loss: 11.1457 - val_accuracy: 0.1434\n",
            "Epoch 189/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.9789 - accuracy: 0.3396 - val_loss: 11.1457 - val_accuracy: 0.1440\n",
            "Epoch 190/500\n",
            "49/49 [==============================] - 9s 181ms/step - loss: 2.9726 - accuracy: 0.3407 - val_loss: 11.1531 - val_accuracy: 0.1455\n",
            "Epoch 191/500\n",
            "49/49 [==============================] - 8s 168ms/step - loss: 2.9703 - accuracy: 0.3418 - val_loss: 11.2423 - val_accuracy: 0.1418\n",
            "Epoch 192/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 2.9590 - accuracy: 0.3427 - val_loss: 11.2049 - val_accuracy: 0.1453\n",
            "Epoch 193/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.9490 - accuracy: 0.3429 - val_loss: 11.2679 - val_accuracy: 0.1450\n",
            "Epoch 194/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.9447 - accuracy: 0.3436 - val_loss: 11.2999 - val_accuracy: 0.1440\n",
            "Epoch 195/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 2.9395 - accuracy: 0.3456 - val_loss: 11.2439 - val_accuracy: 0.1451\n",
            "Epoch 196/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.9316 - accuracy: 0.3473 - val_loss: 11.3109 - val_accuracy: 0.1419\n",
            "Epoch 197/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 2.9301 - accuracy: 0.3486 - val_loss: 11.3445 - val_accuracy: 0.1451\n",
            "Epoch 198/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 2.9261 - accuracy: 0.3481 - val_loss: 11.3467 - val_accuracy: 0.1446\n",
            "Epoch 199/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.9239 - accuracy: 0.3467 - val_loss: 11.3476 - val_accuracy: 0.1455\n",
            "Epoch 200/500\n",
            "49/49 [==============================] - 8s 161ms/step - loss: 2.9295 - accuracy: 0.3469 - val_loss: 11.3269 - val_accuracy: 0.1438\n",
            "Epoch 201/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.9947 - accuracy: 0.3404 - val_loss: 11.2908 - val_accuracy: 0.1434\n",
            "Epoch 202/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.9459 - accuracy: 0.3440 - val_loss: 11.3220 - val_accuracy: 0.1439\n",
            "Epoch 203/500\n",
            "49/49 [==============================] - 9s 179ms/step - loss: 2.9446 - accuracy: 0.3449 - val_loss: 11.3043 - val_accuracy: 0.1459\n",
            "Epoch 204/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.9187 - accuracy: 0.3481 - val_loss: 11.3502 - val_accuracy: 0.1459\n",
            "Epoch 205/500\n",
            "49/49 [==============================] - 8s 169ms/step - loss: 2.9052 - accuracy: 0.3506 - val_loss: 11.3616 - val_accuracy: 0.1436\n",
            "Epoch 206/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 2.9045 - accuracy: 0.3514 - val_loss: 11.4128 - val_accuracy: 0.1455\n",
            "Epoch 207/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 2.8892 - accuracy: 0.3537 - val_loss: 11.4530 - val_accuracy: 0.1457\n",
            "Epoch 208/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 2.8932 - accuracy: 0.3529 - val_loss: 11.4118 - val_accuracy: 0.1456\n",
            "Epoch 209/500\n",
            "49/49 [==============================] - 8s 160ms/step - loss: 2.9000 - accuracy: 0.3543 - val_loss: 11.3811 - val_accuracy: 0.1452\n",
            "Epoch 210/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 3.0059 - accuracy: 0.3413 - val_loss: 11.2714 - val_accuracy: 0.1439\n",
            "Epoch 211/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 3.0008 - accuracy: 0.3407 - val_loss: 11.2834 - val_accuracy: 0.1442\n",
            "Epoch 212/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.9413 - accuracy: 0.3481 - val_loss: 11.4305 - val_accuracy: 0.1442\n",
            "Epoch 213/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.8786 - accuracy: 0.3552 - val_loss: 11.4573 - val_accuracy: 0.1454\n",
            "Epoch 214/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.8612 - accuracy: 0.3584 - val_loss: 11.5345 - val_accuracy: 0.1450\n",
            "Epoch 215/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 2.8565 - accuracy: 0.3570 - val_loss: 11.4902 - val_accuracy: 0.1443\n",
            "Epoch 216/500\n",
            "49/49 [==============================] - 9s 175ms/step - loss: 2.8530 - accuracy: 0.3567 - val_loss: 11.5722 - val_accuracy: 0.1468\n",
            "Epoch 217/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 2.8356 - accuracy: 0.3609 - val_loss: 11.5586 - val_accuracy: 0.1464\n",
            "Epoch 218/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 2.8363 - accuracy: 0.3621 - val_loss: 11.6084 - val_accuracy: 0.1462\n",
            "Epoch 219/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.8248 - accuracy: 0.3625 - val_loss: 11.5662 - val_accuracy: 0.1461\n",
            "Epoch 220/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 2.8222 - accuracy: 0.3637 - val_loss: 11.6432 - val_accuracy: 0.1443\n",
            "Epoch 221/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.8110 - accuracy: 0.3648 - val_loss: 11.7015 - val_accuracy: 0.1464\n",
            "Epoch 222/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.8070 - accuracy: 0.3664 - val_loss: 11.7095 - val_accuracy: 0.1459\n",
            "Epoch 223/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.8069 - accuracy: 0.3655 - val_loss: 11.6705 - val_accuracy: 0.1451\n",
            "Epoch 224/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.7933 - accuracy: 0.3667 - val_loss: 11.7898 - val_accuracy: 0.1464\n",
            "Epoch 225/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 2.7940 - accuracy: 0.3685 - val_loss: 11.7518 - val_accuracy: 0.1462\n",
            "Epoch 226/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.7876 - accuracy: 0.3685 - val_loss: 11.7500 - val_accuracy: 0.1460\n",
            "Epoch 227/500\n",
            "49/49 [==============================] - 9s 182ms/step - loss: 2.7847 - accuracy: 0.3683 - val_loss: 11.7706 - val_accuracy: 0.1471\n",
            "Epoch 228/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.7836 - accuracy: 0.3684 - val_loss: 11.8100 - val_accuracy: 0.1457\n",
            "Epoch 229/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 2.7774 - accuracy: 0.3700 - val_loss: 11.7642 - val_accuracy: 0.1467\n",
            "Epoch 230/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 2.7715 - accuracy: 0.3709 - val_loss: 11.8057 - val_accuracy: 0.1447\n",
            "Epoch 231/500\n",
            "49/49 [==============================] - 9s 179ms/step - loss: 2.7630 - accuracy: 0.3713 - val_loss: 11.8727 - val_accuracy: 0.1479\n",
            "Epoch 232/500\n",
            "49/49 [==============================] - 8s 170ms/step - loss: 2.7619 - accuracy: 0.3731 - val_loss: 11.8513 - val_accuracy: 0.1492\n",
            "Epoch 233/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 2.7562 - accuracy: 0.3735 - val_loss: 11.8457 - val_accuracy: 0.1458\n",
            "Epoch 234/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.7410 - accuracy: 0.3767 - val_loss: 11.8917 - val_accuracy: 0.1471\n",
            "Epoch 235/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.7386 - accuracy: 0.3785 - val_loss: 11.9282 - val_accuracy: 0.1470\n",
            "Epoch 236/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.7387 - accuracy: 0.3755 - val_loss: 11.9171 - val_accuracy: 0.1472\n",
            "Epoch 237/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.7327 - accuracy: 0.3778 - val_loss: 11.8816 - val_accuracy: 0.1465\n",
            "Epoch 238/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.7279 - accuracy: 0.3787 - val_loss: 11.9595 - val_accuracy: 0.1485\n",
            "Epoch 239/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.7242 - accuracy: 0.3799 - val_loss: 11.9205 - val_accuracy: 0.1477\n",
            "Epoch 240/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 2.7148 - accuracy: 0.3801 - val_loss: 11.9817 - val_accuracy: 0.1459\n",
            "Epoch 241/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.7163 - accuracy: 0.3791 - val_loss: 11.9885 - val_accuracy: 0.1470\n",
            "Epoch 242/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.7051 - accuracy: 0.3830 - val_loss: 12.0237 - val_accuracy: 0.1474\n",
            "Epoch 243/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.7108 - accuracy: 0.3811 - val_loss: 12.0530 - val_accuracy: 0.1467\n",
            "Epoch 244/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.7052 - accuracy: 0.3809 - val_loss: 12.0690 - val_accuracy: 0.1470\n",
            "Epoch 245/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.6972 - accuracy: 0.3818 - val_loss: 12.0896 - val_accuracy: 0.1467\n",
            "Epoch 246/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.6913 - accuracy: 0.3851 - val_loss: 12.0300 - val_accuracy: 0.1484\n",
            "Epoch 247/500\n",
            "49/49 [==============================] - 9s 178ms/step - loss: 2.6887 - accuracy: 0.3857 - val_loss: 12.0997 - val_accuracy: 0.1508\n",
            "Epoch 248/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.6822 - accuracy: 0.3867 - val_loss: 12.0422 - val_accuracy: 0.1493\n",
            "Epoch 249/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.6777 - accuracy: 0.3856 - val_loss: 12.1283 - val_accuracy: 0.1475\n",
            "Epoch 250/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 2.6726 - accuracy: 0.3889 - val_loss: 12.1350 - val_accuracy: 0.1494\n",
            "Epoch 251/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 2.6680 - accuracy: 0.3877 - val_loss: 12.1428 - val_accuracy: 0.1502\n",
            "Epoch 252/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.6647 - accuracy: 0.3889 - val_loss: 12.1008 - val_accuracy: 0.1488\n",
            "Epoch 253/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.6588 - accuracy: 0.3885 - val_loss: 12.1549 - val_accuracy: 0.1470\n",
            "Epoch 254/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.6565 - accuracy: 0.3912 - val_loss: 12.1757 - val_accuracy: 0.1488\n",
            "Epoch 255/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.6532 - accuracy: 0.3908 - val_loss: 12.1752 - val_accuracy: 0.1490\n",
            "Epoch 256/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.6435 - accuracy: 0.3917 - val_loss: 12.2813 - val_accuracy: 0.1505\n",
            "Epoch 257/500\n",
            "49/49 [==============================] - 8s 163ms/step - loss: 2.6471 - accuracy: 0.3921 - val_loss: 12.2426 - val_accuracy: 0.1507\n",
            "Epoch 258/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.6351 - accuracy: 0.3925 - val_loss: 12.2804 - val_accuracy: 0.1505\n",
            "Epoch 259/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.6374 - accuracy: 0.3929 - val_loss: 12.2653 - val_accuracy: 0.1495\n",
            "Epoch 260/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.6384 - accuracy: 0.3940 - val_loss: 12.2692 - val_accuracy: 0.1496\n",
            "Epoch 261/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.6270 - accuracy: 0.3935 - val_loss: 12.2466 - val_accuracy: 0.1482\n",
            "Epoch 262/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.6214 - accuracy: 0.3966 - val_loss: 12.3379 - val_accuracy: 0.1485\n",
            "Epoch 263/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.6189 - accuracy: 0.3970 - val_loss: 12.3272 - val_accuracy: 0.1497\n",
            "Epoch 264/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.6066 - accuracy: 0.3981 - val_loss: 12.3363 - val_accuracy: 0.1493\n",
            "Epoch 265/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.6057 - accuracy: 0.3957 - val_loss: 12.4337 - val_accuracy: 0.1491\n",
            "Epoch 266/500\n",
            "49/49 [==============================] - 8s 167ms/step - loss: 2.5931 - accuracy: 0.3980 - val_loss: 12.3481 - val_accuracy: 0.1493\n",
            "Epoch 267/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.5912 - accuracy: 0.3999 - val_loss: 12.4313 - val_accuracy: 0.1508\n",
            "Epoch 268/500\n",
            "49/49 [==============================] - 8s 162ms/step - loss: 2.5960 - accuracy: 0.3990 - val_loss: 12.3825 - val_accuracy: 0.1494\n",
            "Epoch 269/500\n",
            "49/49 [==============================] - 8s 166ms/step - loss: 2.5880 - accuracy: 0.4018 - val_loss: 12.3863 - val_accuracy: 0.1504\n",
            "Epoch 270/500\n",
            "49/49 [==============================] - 8s 164ms/step - loss: 2.5869 - accuracy: 0.4015 - val_loss: 12.4727 - val_accuracy: 0.1500\n",
            "Epoch 271/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.5788 - accuracy: 0.4035 - val_loss: 12.5059 - val_accuracy: 0.1495\n",
            "Epoch 272/500\n",
            "49/49 [==============================] - 8s 165ms/step - loss: 2.6105 - accuracy: 0.3999 - val_loss: 12.4227 - val_accuracy: 0.1507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3f774f76d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JOIgTU_-O-",
        "colab_type": "text"
      },
      "source": [
        "## V6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf8Xngj_75dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXlm2KmaA17M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-6.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD93EE9F7qMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "4077e2eb-12cb-4c30-9910-e601ccec4b56"
      },
      "source": [
        "train_utils.plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"dc2c2089-688c-4027-84aa-30c456c68a31\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"dc2c2089-688c-4027-84aa-30c456c68a31\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'dc2c2089-688c-4027-84aa-30c456c68a31',\n",
              "                        [{\"line\": {\"color\": \"royalblue\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"train_loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [7.458602428436279, 7.021402359008789, 7.009311676025391, 6.955985069274902, 6.8343939781188965, 6.712810039520264, 6.608242511749268, 6.514761447906494, 6.441170692443848, 6.371826171875, 6.300393581390381, 6.232043743133545, 6.170290470123291, 6.113407135009766, 6.059351444244385, 6.004700183868408, 5.949937343597412, 5.893865585327148, 5.840144634246826, 5.785127639770508, 5.73313570022583, 5.686136245727539, 5.643167018890381, 5.59910249710083, 5.556097507476807, 5.515835762023926, 5.475383281707764, 5.43580961227417, 5.395748615264893, 5.358993053436279, 5.318986415863037, 5.285747528076172, 5.251445293426514, 5.212973117828369, 5.178022384643555, 5.146390438079834, 5.111725807189941, 5.081413269042969, 5.049098491668701, 5.016397953033447, 4.985947608947754, 4.956323623657227, 4.93022346496582, 4.901480197906494, 4.873708724975586, 4.841719627380371, 4.813971996307373, 4.785449981689453, 4.76080846786499, 4.736237525939941, 4.708621501922607, 4.686616897583008, 4.657742977142334, 4.632593631744385, 4.607674598693848, 4.585359573364258, 4.560201168060303, 4.540639877319336, 4.51220703125, 4.492542266845703, 4.4723005294799805, 4.4511494636535645, 4.42878532409668, 4.413455963134766, 4.391733646392822, 4.369042873382568, 4.3505353927612305, 4.326739311218262, 4.311563968658447, 4.292220115661621, 4.269426345825195, 4.256741523742676, 4.238198757171631, 4.219151973724365, 4.20266056060791, 4.18494176864624, 4.169332027435303, 4.149455547332764, 4.137803554534912, 4.116767406463623, 4.100502967834473, 4.083449840545654, 4.0702009201049805, 4.059648036956787, 4.039373874664307, 4.027568340301514, 4.011825084686279, 3.9942076206207275, 3.9786579608917236, 3.9630703926086426, 3.953157663345337, 3.94500732421875, 3.930150032043457, 3.9062442779541016, 3.8975884914398193, 3.8874988555908203, 3.8704326152801514, 3.8597898483276367, 3.844069242477417, 3.8322761058807373, 3.821240186691284, 3.8114266395568848, 3.7916510105133057, 3.784296989440918, 3.768200397491455, 3.7584316730499268, 3.750084161758423, 3.733917236328125, 3.724353075027466, 3.716841220855713, 3.7006072998046875, 3.686579465866089, 3.67802095413208, 3.669113874435425, 3.6539947986602783, 3.643953323364258, 3.639620542526245, 3.631702423095703, 3.616957664489746, 3.609309196472168, 3.5965981483459473, 3.5904407501220703, 3.5754475593566895, 3.564770221710205, 3.5559475421905518, 3.5410876274108887, 3.54119610786438, 3.526963472366333, 3.5164248943328857, 3.504305601119995, 3.5012247562408447, 3.4923555850982666, 3.47900652885437, 3.474431037902832, 3.457871198654175, 3.4513890743255615, 3.4426355361938477, 3.437422752380371, 3.4223594665527344, 3.420081615447998, 3.404766321182251, 3.398977518081665, 3.389678716659546, 3.3810465335845947, 3.3723390102386475, 3.3669545650482178, 3.3583598136901855, 3.3477187156677246, 3.338785409927368, 3.33193302154541, 3.3255183696746826, 3.3179781436920166, 3.3103854656219482, 3.30106520652771, 3.2952380180358887, 3.2856810092926025, 3.2768495082855225, 3.268112897872925, 3.2588319778442383, 3.2528700828552246, 3.250032663345337, 3.2422804832458496, 3.231519937515259, 3.2216625213623047, 3.219912528991699, 3.2107439041137695, 3.1988728046417236, 3.204078197479248, 3.1879794597625732, 3.179272413253784, 3.1763107776641846, 3.1669631004333496, 3.1663520336151123, 3.156172275543213, 3.1521947383880615, 3.1415719985961914, 3.1325693130493164, 3.128201484680176, 3.1239748001098633, 3.1095027923583984, 3.107022762298584, 3.109335422515869, 3.0960733890533447, 3.0828840732574463, 3.0855002403259277, 3.0722806453704834, 3.0718817710876465, 3.060788631439209, 3.0544981956481934, 3.0475826263427734, 3.041982889175415, 3.0337958335876465, 3.0354974269866943, 3.026292562484741, 3.0212693214416504, 3.012688398361206, 3.0056071281433105, 2.999680995941162, 2.9940104484558105, 2.9915013313293457, 2.9830503463745117, 2.9763174057006836, 2.9720370769500732, 2.9659273624420166, 2.957529067993164, 2.9552230834960938, 2.952772617340088, 2.943516492843628, 2.943812370300293, 2.927532434463501, 2.9208385944366455, 2.9204330444335938, 2.920297622680664, 2.9069883823394775, 2.9014029502868652, 2.899669885635376, 2.887300729751587, 2.887101411819458, 2.878067970275879, 2.8732335567474365, 2.872183322906494, 2.869094133377075, 2.8626792430877686, 2.8570947647094727, 2.8543055057525635, 2.8468871116638184, 2.835191249847412, 2.8365352153778076, 2.8314614295959473, 2.8236448764801025, 2.818066120147705, 2.8167624473571777, 2.8123903274536133, 2.8090639114379883, 2.8043978214263916, 2.798945665359497, 2.796839714050293, 2.7929646968841553, 2.7829573154449463, 2.778717517852783, 2.7744243144989014, 2.7651422023773193, 2.7624804973602295, 2.75708270072937, 2.7479960918426514, 2.7508907318115234, 2.742372751235962, 2.739006280899048, 2.7318272590637207, 2.73349928855896, 2.7258694171905518, 2.720759868621826, 2.7181174755096436, 2.71685528755188, 2.7080912590026855, 2.6990468502044678, 2.6948909759521484, 2.6980807781219482, 2.6961491107940674, 2.685594081878662, 2.6833837032318115, 2.67678165435791, 2.669595956802368, 2.6717209815979004, 2.666579008102417, 2.6554994583129883, 2.6524298191070557, 2.6517951488494873, 2.6420364379882812, 2.6392295360565186, 2.6405091285705566, 2.6317214965820312, 2.6298704147338867, 2.6316287517547607, 2.6189308166503906, 2.6266093254089355, 2.6110169887542725, 2.613149881362915, 2.6061320304870605, 2.6067609786987305, 2.5941669940948486, 2.596419095993042, 2.593614339828491, 2.584463596343994, 2.580822229385376, 2.580596446990967, 2.5706098079681396, 2.5698089599609375, 2.5620675086975098, 2.563743829727173, 2.5594749450683594, 2.553781270980835, 2.5495808124542236, 2.5503041744232178, 2.5479135513305664, 2.541914701461792, 2.540370225906372, 2.5333597660064697, 2.5280609130859375, 2.527148723602295, 2.5240111351013184, 2.5221266746520996, 2.517972469329834, 2.510897636413574, 2.5106399059295654, 2.4961678981781006, 2.50069260597229, 2.4992992877960205, 2.4951581954956055, 2.4896724224090576, 2.4896066188812256, 2.484933614730835, 2.4841721057891846, 2.4836010932922363, 2.480484962463379, 2.4681806564331055, 2.461735725402832, 2.4697868824005127, 2.4554059505462646, 2.4540064334869385, 2.447173595428467, 2.451136350631714, 2.4495129585266113, 2.447692632675171, 2.440633535385132, 2.43415904045105, 2.432251214981079, 2.4319276809692383, 2.4283690452575684, 2.428084135055542, 2.423860788345337, 2.4220213890075684, 2.4219110012054443, 2.41408634185791, 2.407965660095215, 2.404655694961548, 2.408109188079834, 2.4001846313476562, 2.4002151489257812, 2.394381523132324, 2.3932816982269287, 2.3838207721710205, 2.3845086097717285, 2.381629228591919, 2.3772876262664795, 2.380197286605835, 2.3700618743896484, 2.361698865890503, 2.3691210746765137, 2.354217290878296, 2.3582186698913574, 2.3483285903930664, 2.34909725189209, 2.3508801460266113, 2.352296829223633, 2.3456146717071533, 2.343899726867676, 2.3429369926452637, 2.3420512676239014, 2.334465503692627, 2.3293683528900146, 2.332944631576538, 2.3179967403411865, 2.322411298751831, 2.3163552284240723, 2.31400465965271, 2.31575345993042, 2.308544397354126, 2.3045167922973633, 2.304391860961914, 2.3031654357910156, 2.2948250770568848, 2.3007946014404297, 2.2943413257598877, 2.299194097518921, 2.2900660037994385, 2.2869091033935547, 2.280423879623413, 2.2785377502441406, 2.276789665222168, 2.279853582382202, 2.275507688522339, 2.2724356651306152, 2.269101142883301, 2.2687745094299316, 2.2627599239349365, 2.2564404010772705, 2.2533669471740723, 2.256253242492676, 2.2519936561584473, 2.252096652984619, 2.25284481048584, 2.2436461448669434, 2.2367539405822754, 2.241771936416626, 2.2350378036499023, 2.2347187995910645, 2.2325217723846436, 2.234553813934326, 2.228029251098633, 2.220820188522339, 2.2201356887817383, 2.2250750064849854, 2.2170023918151855, 2.214937210083008, 2.2161030769348145, 2.2099266052246094, 2.2059361934661865, 2.2041614055633545, 2.197805643081665, 2.1955502033233643, 2.201632022857666, 2.2009034156799316, 2.1934211254119873, 2.1948812007904053, 2.184741497039795, 2.1895227432250977, 2.1845178604125977, 2.1759610176086426, 2.176597833633423, 2.1796576976776123, 2.1687614917755127, 2.1743369102478027, 2.1618545055389404, 2.1677160263061523, 2.1629109382629395, 2.157944679260254, 2.1609809398651123, 2.1588261127471924, 2.1591691970825195, 2.1612415313720703, 2.148361921310425, 2.1509604454040527, 2.1545166969299316, 2.143122673034668, 2.1410958766937256, 2.141697883605957, 2.1300461292266846, 2.1318600177764893, 2.128767251968384, 2.1309731006622314, 2.1296863555908203, 2.127523422241211, 2.1259565353393555, 2.1223254203796387, 2.115030527114868, 2.119023561477661, 2.116718292236328, 2.1130118370056152, 2.114192247390747, 2.108396530151367, 2.10859751701355, 2.106961965560913, 2.0974416732788086, 2.1060073375701904, 2.0964243412017822, 2.0923948287963867, 2.0999016761779785, 2.093358278274536, 2.0858376026153564, 2.087390422821045, 2.084538698196411, 2.0864334106445312, 2.079694986343384, 2.081512928009033, 2.0752129554748535, 2.08119797706604, 2.0702297687530518, 2.0719404220581055, 2.0688765048980713, 2.0731663703918457, 2.0700695514678955, 2.0657315254211426, 2.058476686477661, 2.0574235916137695, 2.0610194206237793, 2.0544745922088623, 2.052564859390259, 2.058309555053711, 2.048804759979248, 2.053652286529541, 2.041518449783325, 2.0463902950286865, 2.039344310760498, 2.0366008281707764, 2.0353167057037354, 2.038710117340088, 2.0290632247924805, 2.034236431121826, 2.033118486404419, 2.0306506156921387, 2.027523994445801, 2.029937982559204, 2.0225539207458496, 2.024111270904541, 2.0212676525115967, 2.018556594848633, 2.0204620361328125, 2.017644166946411, 2.0152645111083984]}, {\"line\": {\"color\": \"firebrick\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"train_accuracy\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"xaxis\": \"x\", \"y\": [0.0541599839925766, 0.05779228359460831, 0.05779852718114853, 0.0578172504901886, 0.0645139142870903, 0.07083611935377121, 0.07786355912685394, 0.08414207398891449, 0.08962173014879227, 0.09450848400592804, 0.09875865280628204, 0.10367037355899811, 0.10773954540491104, 0.11044817417860031, 0.1136186346411705, 0.11635845899581909, 0.11837432533502579, 0.12124522030353546, 0.12284917384386063, 0.12506474554538727, 0.1273365020751953, 0.12904655933380127, 0.13121220469474792, 0.1330907642841339, 0.13485074043273926, 0.13576817512512207, 0.13764050602912903, 0.13923820853233337, 0.1411791890859604, 0.14272072911262512, 0.14338228106498718, 0.14515474438667297, 0.14648409187793732, 0.1483376920223236, 0.14953598380088806, 0.15018504858016968, 0.15170162916183472, 0.15447266399860382, 0.15454131364822388, 0.155702143907547, 0.15698781609535217, 0.1587165892124176, 0.15924707055091858, 0.16103202104568481, 0.16219286620616913, 0.1639029085636139, 0.16659905016422272, 0.16744160652160645, 0.16827790439128876, 0.16945746541023254, 0.1713859587907791, 0.17171673476696014, 0.17364521324634552, 0.1752803772687912, 0.1763538420200348, 0.1779453158378601, 0.17909367382526398, 0.18139038980007172, 0.18241392076015472, 0.18423007428646088, 0.18559686839580536, 0.18609614670276642, 0.18806833028793335, 0.1887548416852951, 0.19060219824314117, 0.1924121081829071, 0.19377265870571136, 0.19562625885009766, 0.1970929056406021, 0.19883416593074799, 0.19952069222927094, 0.20134307444095612, 0.20247894525527954, 0.20446985960006714, 0.20503778755664825, 0.20734699070453644, 0.20800229907035828, 0.20974980294704437, 0.21161587536334991, 0.21383145451545715, 0.21460534632205963, 0.216696098446846, 0.21679596602916718, 0.21816898882389069, 0.22108358144760132, 0.22125832736492157, 0.2232055366039276, 0.22429148852825165, 0.22633855044841766, 0.22724974155426025, 0.22884121537208557, 0.22905965149402618, 0.23065736889839172, 0.23375917971134186, 0.23399010300636292, 0.2360808551311493, 0.2375599890947342, 0.2381778508424759, 0.24095512926578522, 0.2411423623561859, 0.24287737905979156, 0.2432643324136734, 0.24540501832962036, 0.24626627564430237, 0.2474333643913269, 0.24871277809143066, 0.2504790127277374, 0.2525697648525238, 0.25258225202560425, 0.25417372584342957, 0.25564661622047424, 0.2575688660144806, 0.25863608717918396, 0.260520875453949, 0.2619188725948334, 0.26328566670417786, 0.26449644565582275, 0.2626241147518158, 0.26522040367126465, 0.26653727889060974, 0.2687341272830963, 0.2685968279838562, 0.27151140570640564, 0.27336499094963074, 0.27242258191108704, 0.2752123475074768, 0.27509376406669617, 0.2755680978298187, 0.27858254313468933, 0.27923783659935, 0.2786199748516083, 0.28127244114875793, 0.2823334038257599, 0.2830074429512024, 0.28568485379219055, 0.28665223717689514, 0.28694555163383484, 0.2884184420108795, 0.29057785868644714, 0.29015347361564636, 0.29294323921203613, 0.29165756702423096, 0.29429754614830017, 0.295545756816864, 0.2984790503978729, 0.29732444882392883, 0.29741182923316956, 0.29980841279029846, 0.30133122205734253, 0.30106285214424133, 0.3028790056705475, 0.3036404252052307, 0.30362793803215027, 0.3058185577392578, 0.3073538541793823, 0.3087456226348877, 0.30862703919410706, 0.30980658531188965, 0.3106616139411926, 0.31209081411361694, 0.3127586245536804, 0.3134576082229614, 0.3164907693862915, 0.3174331784248352, 0.3164595663547516, 0.31737077236175537, 0.32045385241508484, 0.31871259212493896, 0.3201417922973633, 0.3216833472251892, 0.3225383758544922, 0.32280048727989197, 0.3240424692630768, 0.32656386494636536, 0.32553407549858093, 0.3271068334579468, 0.3286171555519104, 0.32924750447273254, 0.3300651013851166, 0.33253031969070435, 0.3334040641784668, 0.33323556184768677, 0.33427780866622925, 0.3356570899486542, 0.33639976382255554, 0.33715495467185974, 0.3374670147895813, 0.3392644226551056, 0.34051263332366943, 0.3404689431190491, 0.34261587262153625, 0.3427157402038574, 0.3416547477245331, 0.34538692235946655, 0.3444632291793823, 0.3474963903427124, 0.34747767448425293, 0.3484575152397156, 0.34944984316825867, 0.3494373559951782, 0.35025495290756226, 0.35143449902534485, 0.353737473487854, 0.3531508147716522, 0.3542117774486542, 0.3554038405418396, 0.3552727699279785, 0.35581573843955994, 0.3579813838005066, 0.3592483401298523, 0.3606463372707367, 0.3612954020500183, 0.36030930280685425, 0.36278077960014343, 0.36332374811172485, 0.3632800579071045, 0.36574527621269226, 0.36542072892189026, 0.3663007318973541, 0.3664068281650543, 0.36787348985671997, 0.3679858148097992, 0.3693276643753052, 0.3680731952190399, 0.3709503412246704, 0.3725043535232544, 0.3720799684524536, 0.3717554211616516, 0.37443283200263977, 0.3751380741596222, 0.3767233192920685, 0.37574347853660583, 0.37730371952056885, 0.37679195404052734, 0.37720388174057007, 0.37975022196769714, 0.3793882429599762, 0.38088610768318176, 0.3824900686740875, 0.3819658160209656, 0.38318905234336853, 0.3838880658149719, 0.3844497501850128, 0.3857167065143585, 0.3868962526321411, 0.38655298948287964, 0.388637512922287, 0.3876951038837433, 0.3900729715824127, 0.3896111249923706, 0.388806015253067, 0.39050358533859253, 0.3926505148410797, 0.3917580544948578, 0.3934306502342224, 0.3935367465019226, 0.3959770202636719, 0.3956587016582489, 0.39658862352371216, 0.3974062204360962, 0.397144079208374, 0.3978118896484375, 0.39940959215164185, 0.39943456649780273, 0.39968419075012207, 0.4017874300479889, 0.4022991955280304, 0.4012756645679474, 0.4031604826450348, 0.40432754158973694, 0.4050390422344208, 0.40538230538368225, 0.40599390864372253, 0.4057754874229431, 0.4077601432800293, 0.4071485102176666, 0.40780383348464966, 0.4092455208301544, 0.409401535987854, 0.4097760021686554, 0.4134270250797272, 0.41277173161506653, 0.4110679030418396, 0.4142758250236511, 0.41382020711898804, 0.4131337106227875, 0.4146253168582916, 0.4154990613460541, 0.41732147336006165, 0.41847604513168335, 0.4173027276992798, 0.4189004600048065, 0.4181889593601227, 0.4202422797679901, 0.4192998707294464, 0.4219648241996765, 0.4206417202949524, 0.4223829507827759, 0.42361870408058167, 0.42216452956199646, 0.42401811480522156, 0.4249979853630066, 0.4239557087421417, 0.4256220757961273, 0.42725101113319397, 0.4295476973056793, 0.42767539620399475, 0.4278251826763153, 0.429628849029541, 0.42915451526641846, 0.43006572127342224, 0.43087083101272583, 0.42972245812416077, 0.4296225905418396, 0.43103307485580444, 0.4329615831375122, 0.43375417590141296, 0.43163222074508667, 0.4365564286708832, 0.4353768527507782, 0.4367998242378235, 0.43699952960014343, 0.4363442361354828, 0.4356764256954193, 0.4375612437725067, 0.43915271759033203, 0.4390091598033905, 0.4396519958972931, 0.4407878816127777, 0.4402698576450348, 0.43995779752731323, 0.4418051540851593, 0.44232940673828125, 0.44324684143066406, 0.4441767632961273, 0.4453750550746918, 0.44461989402770996, 0.4444701075553894, 0.4459554851055145, 0.44568711519241333, 0.4464547634124756, 0.44802752137184143, 0.44768425822257996, 0.4482022523880005, 0.4484955966472626, 0.4489574432373047, 0.45026180148124695, 0.4520280361175537, 0.4498249292373657, 0.4541187882423401, 0.45279568433761597, 0.4532824754714966, 0.4544932544231415, 0.45383793115615845, 0.4532325565814972, 0.4546118378639221, 0.45664018392562866, 0.45631563663482666, 0.4550924003124237, 0.4574265480041504, 0.4574640095233917, 0.45537325739860535, 0.4590242803096771, 0.4579695165157318, 0.4593987464904785, 0.46039730310440063, 0.4611150324344635, 0.4620511829853058, 0.46137091517448425, 0.46206989884376526, 0.46265658736228943, 0.4647223651409149, 0.46357402205467224, 0.4628937244415283, 0.46300607919692993, 0.46443527936935425, 0.46357402205467224, 0.46822986006736755, 0.46717509627342224, 0.466731995344162, 0.46645113825798035, 0.46657595038414, 0.4687977731227875, 0.46779295802116394, 0.46949678659439087, 0.46954047679901123, 0.4705016016960144, 0.46989619731903076, 0.4707637131214142, 0.47185590863227844, 0.46979010105133057, 0.47167491912841797, 0.473022997379303, 0.47487035393714905, 0.47252368927001953, 0.4749452471733093, 0.4740215539932251, 0.47412142157554626, 0.47397786378860474, 0.4761996865272522, 0.4772981107234955, 0.477279394865036, 0.4765491783618927, 0.47761017084121704, 0.4777037799358368, 0.47756025195121765, 0.47973838448524475, 0.4799380898475647, 0.4807431995868683, 0.4823783338069916, 0.48210999369621277, 0.4807494282722473, 0.4811488687992096, 0.48229098320007324, 0.4826467037200928, 0.48360782861709595, 0.483245849609375, 0.48328953981399536, 0.48498088121414185, 0.4841570556163788, 0.483938604593277, 0.48630398511886597, 0.48556753993034363, 0.4873899221420288, 0.4865598678588867, 0.48796409368515015, 0.48780182003974915, 0.4879578649997711, 0.488519549369812, 0.48809516429901123, 0.4881700575351715, 0.4912032186985016, 0.48931217193603516, 0.4886007010936737, 0.49066025018692017, 0.4916151165962219, 0.49162137508392334, 0.49328774213790894, 0.4924951195716858, 0.4955906867980957, 0.4936809241771698, 0.493793249130249, 0.494442343711853, 0.49484801292419434, 0.49582159519195557, 0.49737563729286194, 0.4948916733264923, 0.4962584674358368, 0.4972071349620819, 0.49522244930267334, 0.4976939260959625, 0.49799349904060364, 0.4980309307575226, 0.4987673759460449, 0.49928539991378784, 0.5002776980400085, 0.5001591444015503, 0.4989047050476074, 0.5019253492355347, 0.500933051109314, 0.5020189881324768, 0.5015820860862732, 0.5023372769355774, 0.5038164258003235, 0.5035542845726013, 0.5042282938957214, 0.5025744438171387, 0.5066685676574707, 0.5043219327926636, 0.5047088861465454, 0.5030050873756409, 0.5049210786819458, 0.505233108997345, 0.5060444474220276, 0.5073738098144531, 0.507604718208313, 0.5079230070114136, 0.5082350969314575, 0.5075298547744751, 0.5096330642700195, 0.5079042911529541, 0.5109749436378479, 0.5108688473701477, 0.5107439756393433, 0.5106441378593445, 0.5122168660163879, 0.5119172930717468, 0.5130032896995544, 0.5115865468978882, 0.5120670795440674, 0.5143575668334961, 0.5131655335426331, 0.5122855305671692, 0.5125726461410522, 0.513359010219574, 0.5153873562812805, 0.5146009922027588, 0.5140954256057739, 0.5155371427536011, 0.51540607213974], \"yaxis\": \"y2\"}, {\"line\": {\"color\": \"royalblue\", \"dash\": \"dot\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"validation_loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [7.0811052322387695, 7.101848602294922, 7.090435981750488, 7.001238822937012, 6.886741638183594, 6.814002513885498, 6.741083145141602, 6.700465679168701, 6.6743316650390625, 6.641850471496582, 6.620259761810303, 6.593625068664551, 6.589068412780762, 6.578842639923096, 6.558661937713623, 6.549444675445557, 6.537961959838867, 6.5289506912231445, 6.517361640930176, 6.5008158683776855, 6.494589328765869, 6.5000691413879395, 6.493002414703369, 6.504294395446777, 6.5040154457092285, 6.504066467285156, 6.505837440490723, 6.513614177703857, 6.524117946624756, 6.528754711151123, 6.533158779144287, 6.53879976272583, 6.550942420959473, 6.569360256195068, 6.568551540374756, 6.577001571655273, 6.596731662750244, 6.602609157562256, 6.616481304168701, 6.630683898925781, 6.641595363616943, 6.642998218536377, 6.65492057800293, 6.673959255218506, 6.6845855712890625, 6.693271160125732, 6.723233222961426, 6.723042964935303, 6.735398292541504, 6.756357192993164, 6.756956100463867, 6.7870869636535645, 6.791988372802734, 6.813998699188232, 6.819335460662842, 6.841073989868164, 6.853143215179443, 6.858806133270264, 6.877065658569336, 6.890848159790039, 6.906373977661133, 6.9317545890808105, 6.93049430847168, 6.944567680358887, 6.974958896636963, 6.98774528503418, 7.0010857582092285, 7.020697116851807, 7.033700466156006, 7.0398454666137695, 7.05448579788208, 7.063262939453125, 7.078580379486084, 7.105457305908203, 7.130711078643799, 7.128904342651367, 7.146759986877441, 7.168460369110107, 7.1587371826171875, 7.198734283447266, 7.221553802490234, 7.219066143035889, 7.239317893981934, 7.244234085083008, 7.274664878845215, 7.272515773773193, 7.283438205718994, 7.2960357666015625, 7.329097747802734, 7.3549299240112305, 7.370049953460693, 7.3714599609375, 7.3922014236450195, 7.388037204742432, 7.429225921630859, 7.4100661277771, 7.434469223022461, 7.465261936187744, 7.485400199890137, 7.483192443847656, 7.49373722076416, 7.506464004516602, 7.536664962768555, 7.545938014984131, 7.562707424163818, 7.571969032287598, 7.596616268157959, 7.625345706939697, 7.601678371429443, 7.63006591796875, 7.651721477508545, 7.6534743309021, 7.692101001739502, 7.675325393676758, 7.701205730438232, 7.708262920379639, 7.715651988983154, 7.722330570220947, 7.752479076385498, 7.760960578918457, 7.776294708251953, 7.795327186584473, 7.798788547515869, 7.835936546325684, 7.829690933227539, 7.852116584777832, 7.875899314880371, 7.889072418212891, 7.887316703796387, 7.883529186248779, 7.9167962074279785, 7.944774627685547, 7.943746566772461, 7.945460319519043, 7.948983192443848, 7.991881370544434, 7.969258785247803, 8.028912544250488, 8.011883735656738, 8.017423629760742, 8.041860580444336, 8.071242332458496, 8.081646919250488, 8.058266639709473, 8.115095138549805, 8.113784790039062, 8.12731647491455, 8.14806842803955, 8.153298377990723, 8.143675804138184, 8.154038429260254, 8.157585144042969, 8.223134994506836, 8.203485488891602, 8.198715209960938, 8.22368049621582, 8.26600456237793, 8.288798332214355, 8.26154899597168, 8.259553909301758, 8.280402183532715, 8.290225982666016, 8.315468788146973, 8.329297065734863, 8.334005355834961, 8.346994400024414, 8.327754974365234, 8.3567533493042, 8.384794235229492, 8.396097183227539, 8.423657417297363, 8.40749454498291, 8.436782836914062, 8.436640739440918, 8.455733299255371, 8.460367202758789, 8.4896879196167, 8.473045349121094, 8.5108642578125, 8.54369831085205, 8.488730430603027, 8.53065299987793, 8.525118827819824, 8.557458877563477, 8.58045482635498, 8.57884693145752, 8.579798698425293, 8.604087829589844, 8.601811408996582, 8.62378978729248, 8.622810363769531, 8.641777038574219, 8.64533805847168, 8.65832233428955, 8.683515548706055, 8.691615104675293, 8.754294395446777, 8.668593406677246, 8.712483406066895, 8.71349811553955, 8.722867965698242, 8.726900100708008, 8.746829986572266, 8.767716407775879, 8.787223815917969, 8.769930839538574, 8.798505783081055, 8.807829856872559, 8.799479484558105, 8.810611724853516, 8.834456443786621, 8.814477920532227, 8.830406188964844, 8.826130867004395, 8.867487907409668, 8.923562049865723, 8.8707914352417, 8.890682220458984, 8.918107032775879, 8.932426452636719, 8.932161331176758, 8.956990242004395, 8.923675537109375, 8.926551818847656, 8.928370475769043, 8.962406158447266, 9.01309871673584, 8.975807189941406, 9.013347625732422, 9.0341157913208, 8.972091674804688, 9.007434844970703, 9.026371955871582, 9.042062759399414, 9.048633575439453, 9.028782844543457, 9.02031421661377, 9.03646469116211, 9.08127498626709, 9.08030891418457, 9.09367561340332, 9.134988784790039, 9.103424072265625, 9.138724327087402, 9.099093437194824, 9.148544311523438, 9.121636390686035, 9.139052391052246, 9.174615859985352, 9.154459953308105, 9.182940483093262, 9.19161319732666, 9.176226615905762, 9.191561698913574, 9.189057350158691, 9.243419647216797, 9.255827903747559, 9.227087020874023, 9.200124740600586, 9.278923034667969, 9.262886047363281, 9.245074272155762, 9.294384956359863, 9.316097259521484, 9.320127487182617, 9.271126747131348, 9.268879890441895, 9.312228202819824, 9.32699966430664, 9.30872631072998, 9.277027130126953, 9.315185546875, 9.353362083435059, 9.345064163208008, 9.292256355285645, 9.399405479431152, 9.392840385437012, 9.387420654296875, 9.384929656982422, 9.436405181884766, 9.38337230682373, 9.37002182006836, 9.40088939666748, 9.454127311706543, 9.420952796936035, 9.450101852416992, 9.475909233093262, 9.432161331176758, 9.438759803771973, 9.441463470458984, 9.47204303741455, 9.403733253479004, 9.456913948059082, 9.489250183105469, 9.472453117370605, 9.467790603637695, 9.447869300842285, 9.50535774230957, 9.495697975158691, 9.550504684448242, 9.494595527648926, 9.526615142822266, 9.571975708007812, 9.5604248046875, 9.520313262939453, 9.625659942626953, 9.599746704101562, 9.614375114440918, 9.619553565979004, 9.54300594329834, 9.58000659942627, 9.659143447875977, 9.65058422088623, 9.589821815490723, 9.686781883239746, 9.678272247314453, 9.641669273376465, 9.632264137268066, 9.678268432617188, 9.668156623840332, 9.700486183166504, 9.670654296875, 9.736562728881836, 9.714948654174805, 9.747123718261719, 9.710671424865723, 9.726539611816406, 9.767770767211914, 9.706463813781738, 9.687151908874512, 9.706415176391602, 9.693958282470703, 9.683018684387207, 9.801871299743652, 9.735254287719727, 9.724715232849121, 9.698816299438477, 9.753783226013184, 9.760106086730957, 9.839076042175293, 9.845850944519043, 9.855257987976074, 9.79184341430664, 9.765809059143066, 9.850480079650879, 9.797479629516602, 9.804856300354004, 9.857549667358398, 9.81493091583252, 9.823090553283691, 9.786590576171875, 9.7866849899292, 9.8911714553833, 9.788925170898438, 9.90186595916748, 9.879353523254395, 9.860163688659668, 9.89746379852295, 9.779236793518066, 9.85682201385498, 9.872174263000488, 9.904027938842773, 9.97659969329834, 9.994986534118652, 9.93973159790039, 9.892644882202148, 9.907636642456055, 9.947927474975586, 9.991455078125, 9.984703063964844, 9.985127449035645, 9.975567817687988, 9.97638988494873, 9.952582359313965, 9.976996421813965, 9.938711166381836, 10.002985000610352, 10.014431953430176, 10.014984130859375, 10.060877799987793, 10.088844299316406, 10.02536392211914, 9.989370346069336, 10.029144287109375, 9.94666576385498, 10.048295974731445, 10.043638229370117, 10.055829048156738, 10.002419471740723, 10.03606128692627, 9.985846519470215, 10.056924819946289, 10.046748161315918, 10.043429374694824, 10.057329177856445, 10.089051246643066, 10.093863487243652, 10.153517723083496, 10.104842185974121, 10.064675331115723, 10.149765968322754, 10.174967765808105, 10.1535005569458, 10.148859977722168, 10.098540306091309, 10.18799114227295, 10.151997566223145, 10.110013008117676, 10.152444839477539, 10.204248428344727, 10.182126998901367, 10.177501678466797, 10.136330604553223, 10.192605018615723, 10.156577110290527, 10.183003425598145, 10.133132934570312, 10.110381126403809, 10.160709381103516, 10.199335098266602, 10.191929817199707, 10.230559349060059, 10.198878288269043, 10.20158863067627, 10.221324920654297, 10.235447883605957, 10.189408302307129, 10.219768524169922, 10.25086498260498, 10.29871654510498, 10.25256061553955, 10.304192543029785, 10.194900512695312, 10.247417449951172, 10.246251106262207, 10.29559326171875, 10.261749267578125, 10.271471977233887, 10.33713436126709, 10.273933410644531, 10.226823806762695, 10.266448974609375, 10.23887825012207, 10.33549976348877, 10.311206817626953, 10.280656814575195, 10.298513412475586, 10.356525421142578, 10.356785774230957, 10.358695983886719, 10.318659782409668, 10.338214874267578, 10.333951950073242, 10.359245300292969, 10.304974555969238, 10.414581298828125, 10.360187530517578, 10.422396659851074, 10.383631706237793, 10.391356468200684, 10.337711334228516, 10.399136543273926, 10.380802154541016, 10.424531936645508, 10.395012855529785, 10.465195655822754, 10.338091850280762, 10.4320068359375, 10.43752384185791, 10.413241386413574, 10.444828987121582, 10.409032821655273, 10.471036911010742, 10.342068672180176, 10.471902847290039, 10.459872245788574, 10.41877269744873, 10.499322891235352, 10.444331169128418, 10.44308853149414, 10.461743354797363, 10.463170051574707, 10.444615364074707, 10.432865142822266, 10.487117767333984, 10.523528099060059, 10.545230865478516, 10.504417419433594, 10.468966484069824, 10.44719123840332, 10.427950859069824, 10.499178886413574, 10.495201110839844, 10.548486709594727, 10.526596069335938, 10.525449752807617, 10.437758445739746, 10.4793119430542, 10.520360946655273, 10.438840866088867]}, {\"line\": {\"color\": \"firebrick\", \"dash\": \"dot\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"validation_accuracy\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"xaxis\": \"x\", \"y\": [0.05711717903614044, 0.05711717903614044, 0.05711717903614044, 0.05711717903614044, 0.06867542117834091, 0.0748165175318718, 0.08243047446012497, 0.08530130982398987, 0.09149233251810074, 0.0948624461889267, 0.0980827808380127, 0.10175246000289917, 0.10320036113262177, 0.1059214174747467, 0.10819312185049057, 0.10959109663963318, 0.11243696510791779, 0.11338558793067932, 0.11515802145004272, 0.11842828243970871, 0.11912726610898972, 0.11875280737876892, 0.12055020034313202, 0.12147386372089386, 0.1216985359787941, 0.12402017414569855, 0.12509360909461975, 0.12649159133434296, 0.12626691162586212, 0.1270158290863037, 0.12721553444862366, 0.1295870989561081, 0.1320335566997528, 0.13081032037734985, 0.13368116319179535, 0.13223326206207275, 0.13413050770759583, 0.13512906432151794, 0.13575315475463867, 0.1353038102388382, 0.1381496787071228, 0.13877378404140472, 0.13944779336452484, 0.13972240686416626, 0.1399720460176468, 0.1403215378522873, 0.14094562828540802, 0.14189425110816956, 0.14184433221817017, 0.143466979265213, 0.14449049532413483, 0.14518947899341583, 0.1459883153438568, 0.14611314237117767, 0.14696191251277924, 0.14796045422554016, 0.14791053533554077, 0.1489090770483017, 0.15008237957954407, 0.14908382296562195, 0.15083129703998566, 0.1503569781780243, 0.1499325931072235, 0.15220430493354797, 0.14950820803642273, 0.1517299860715866, 0.15220430493354797, 0.152104452252388, 0.15267860889434814, 0.15312796831130981, 0.1549253612756729, 0.15417644381523132, 0.1553747057914734, 0.1549253612756729, 0.1560736894607544, 0.1570972055196762, 0.1564231812953949, 0.1574966311454773, 0.1588696390390396, 0.15879474580287933, 0.15772131085395813, 0.158220574259758, 0.15809576213359833, 0.16054221987724304, 0.15944381058216095, 0.16059213876724243, 0.16169054806232452, 0.16061709821224213, 0.16064207255840302, 0.16079185903072357, 0.16418692469596863, 0.1635378748178482, 0.16411203145980835, 0.16383743286132812, 0.16483598947525024, 0.1661091446876526, 0.16481103003025055, 0.16590942442417145, 0.16735732555389404, 0.1653851866722107, 0.16673323512077332, 0.1692545861005783, 0.16673323512077332, 0.1682560294866562, 0.1682060956954956, 0.1678066849708557, 0.16942933201789856, 0.1703280210494995, 0.1689799726009369, 0.17087723314762115, 0.17025312781333923, 0.1728493720293045, 0.17349842190742493, 0.1710519790649414, 0.17257477343082428, 0.17377303540706635, 0.17384791374206543, 0.17449697852134705, 0.17459683120250702, 0.17552049458026886, 0.17621947824954987, 0.17479655146598816, 0.1764940768480301, 0.17761746048927307, 0.1761196255683899, 0.17584502696990967, 0.1761445850133896, 0.1775175929069519, 0.17844125628471375, 0.17869089543819427, 0.17661890387535095, 0.1778670996427536, 0.1785660833120346, 0.18066303431987762, 0.1807878613471985, 0.18023864924907684, 0.1821608692407608, 0.18131209909915924, 0.18305955827236176, 0.18410804867744446, 0.18445754051208496, 0.18498177826404572, 0.18378351628780365, 0.18600529432296753, 0.18488192558288574, 0.18498177826404572, 0.1843077540397644, 0.185755655169487, 0.18568076193332672, 0.18627989292144775, 0.1868790239095688, 0.18655449151992798, 0.18750311434268951, 0.18979978561401367, 0.18872635066509247, 0.1882520318031311, 0.18977482616901398, 0.1886514574289322, 0.188975989818573, 0.19027410447597504, 0.1904488503932953, 0.19172200560569763, 0.18977482616901398, 0.18984971940517426, 0.19029906392097473, 0.19349443912506104, 0.1917968988418579, 0.1925957351922989, 0.1936691850423813, 0.19334465265274048, 0.19481751322746277, 0.19496729969978333, 0.19461780786514282, 0.19314493238925934, 0.19411852955818176, 0.1957411766052246, 0.19496729969978333, 0.19671475887298584, 0.19678965210914612, 0.19621548056602478, 0.19693943858146667, 0.19676469266414642, 0.1968895047903061, 0.19691447913646698, 0.1975885033607483, 0.19918617606163025, 0.19873683154582977, 0.19803784787654877, 0.1996854543685913, 0.19943581521511078, 0.199710413813591, 0.19996005296707153, 0.20060911774635315, 0.2007589042186737, 0.2004842907190323, 0.20065905153751373, 0.20303060114383698, 0.20195716619491577, 0.20280593633651733, 0.20375455915927887, 0.202930748462677, 0.2036047726869583, 0.20323032140731812, 0.2043287307024002, 0.2043786495923996, 0.20457835495471954, 0.20642568171024323, 0.20712466537952423, 0.2054770588874817, 0.20592640340328217, 0.20487792789936066, 0.2065255343914032, 0.20709970593452454, 0.20729941129684448, 0.20879724621772766, 0.20802336931228638, 0.20804832875728607, 0.20907184481620789, 0.20912177860736847, 0.20949622988700867, 0.2090219110250473, 0.2097209095954895, 0.21051974594593048, 0.2115432620048523, 0.2108442783355713, 0.210769385099411, 0.21174296736717224, 0.21209245920181274, 0.2111688107252121, 0.21246692538261414, 0.2125917375087738, 0.2147136628627777, 0.21391482651233673, 0.2129662036895752, 0.21458885073661804, 0.2147386223077774, 0.21319086849689484, 0.21411453187465668, 0.21271656453609467, 0.2158370316028595, 0.21641120314598083, 0.21683558821678162, 0.21641120314598083, 0.21701033413410187, 0.2165110558271408, 0.2187328338623047, 0.21698537468910217, 0.21743471920490265, 0.21628639101982117, 0.21960656344890594, 0.21890757977962494, 0.21883268654346466, 0.21985620260238647, 0.21980628371238708, 0.21930700540542603, 0.22053022682666779, 0.22055520117282867, 0.2208547592163086, 0.22205302119255066, 0.22397524118423462, 0.2222527265548706, 0.22160367667675018, 0.2233511358499527, 0.22275200486183167, 0.22360077500343323, 0.22355085611343384, 0.22177842259407043, 0.2241000533103943, 0.22340106964111328, 0.2237256020307541, 0.22392530739307404, 0.2233012169599533, 0.22492386400699615, 0.22599729895591736, 0.2262219786643982, 0.22529831528663635, 0.22697089612483978, 0.22524839639663696, 0.2265964299440384, 0.2265215367078781, 0.22642168402671814, 0.22709570825099945, 0.22604723274707794, 0.22664636373519897, 0.22776973247528076, 0.22724549472332, 0.2290928214788437, 0.22891807556152344, 0.22854360938072205, 0.22931748628616333, 0.22999151051044464, 0.23006640374660492, 0.22829397022724152, 0.23061560094356537, 0.23006640374660492, 0.23034100234508514, 0.23143941164016724, 0.23099006712436676, 0.2322881817817688, 0.23039093613624573, 0.23223824799060822, 0.2316141575574875, 0.23178890347480774, 0.23103998601436615, 0.23293724656105042, 0.23391082882881165, 0.23293724656105042, 0.23308701813220978, 0.23276250064373016, 0.23321184515953064, 0.2337111234664917, 0.2326127141714096, 0.23308701813220978, 0.23426032066345215, 0.23416046798229218, 0.234385147690773, 0.23493434488773346, 0.23530879616737366, 0.23383593559265137, 0.23448500037193298, 0.23388586938381195, 0.23533377051353455, 0.23323680460453033, 0.23595786094665527, 0.23685656487941742, 0.23650705814361572, 0.2354835420846939, 0.23783014714717865, 0.23638224601745605, 0.23758050799369812, 0.23788008093833923, 0.23608267307281494, 0.23720605671405792, 0.2376554012298584, 0.23827949166297913, 0.23795495927333832, 0.23920315504074097, 0.23815467953681946, 0.2390533685684204, 0.2409256547689438, 0.23758050799369812, 0.24005192518234253, 0.24080084264278412, 0.24032652378082275, 0.2401517778635025, 0.23817963898181915, 0.24112537503242493, 0.23967747390270233, 0.239802286028862, 0.24070098996162415, 0.24115033447742462, 0.24212391674518585, 0.24152478575706482, 0.23985221982002258, 0.2416246384382248, 0.24284787476062775, 0.24137499928474426, 0.24257327616214752, 0.24359677731990814, 0.2426231950521469, 0.2441210299730301, 0.24147486686706543, 0.24169953167438507, 0.243022620677948, 0.24437066912651062, 0.24312247335910797, 0.24357181787490845, 0.2433970719575882, 0.24422088265419006, 0.24344700574874878, 0.24462029337882996, 0.24579359591007233, 0.24486993253231049, 0.2448200136423111, 0.24634280800819397, 0.24529431760311127, 0.24651755392551422, 0.2462679147720337, 0.24828997254371643, 0.24631783366203308, 0.2465674728155136, 0.2455189973115921, 0.24779070913791656, 0.24566878378391266, 0.24853961169719696, 0.24794048070907593, 0.246941938996315, 0.24789056181907654, 0.24809026718139648, 0.24736632406711578, 0.24601827561855316, 0.2483898401260376, 0.24958810210227966, 0.24973787367343903, 0.24918867647647858, 0.24796545505523682, 0.2498627007007599, 0.24901393055915833, 0.2509361505508423, 0.25076138973236084, 0.2501123249530792, 0.2512606680393219, 0.24828997254371643, 0.25138548016548157, 0.25036197900772095, 0.25243398547172546, 0.25255879759788513, 0.25111088156700134, 0.25041189789772034, 0.2519596517086029, 0.25255879759788513, 0.254181444644928, 0.25315791368484497, 0.2512107491493225, 0.2512856423854828, 0.254181444644928, 0.2534574866294861, 0.2533825933933258, 0.25223425030708313, 0.25353237986564636, 0.25508013367652893, 0.2544809877872467, 0.2548554539680481, 0.2540316581726074, 0.2552049458026886, 0.25353237986564636, 0.2541564702987671, 0.2541564702987671, 0.25530481338500977, 0.2556293308734894, 0.25605371594429016, 0.25537970662117004, 0.2547306418418884, 0.2549053728580475, 0.25602877140045166, 0.25652801990509033, 0.2566278874874115, 0.25607869029045105, 0.2566778063774109, 0.2573268711566925, 0.2577013373374939, 0.2573518455028534, 0.2563033699989319, 0.25650307536125183, 0.25787606835365295, 0.25782614946365356, 0.25707724690437317, 0.2583504021167755, 0.25790104269981384, 0.2572270333766937, 0.25902441143989563, 0.2580508291721344, 0.2602476477622986, 0.25967347621917725, 0.25862500071525574, 0.2597982883453369, 0.2594238221645355, 0.2590493857860565, 0.25972339510917664, 0.258000910282135, 0.26007288694381714, 0.26137101650238037, 0.260097861289978, 0.26034748554229736, 0.2612212300300598, 0.26004794239997864, 0.261321097612381, 0.2611713111400604, 0.262020081281662, 0.259773313999176, 0.26137101650238037, 0.260996550321579, 0.2620450258255005, 0.2627440094947815, 0.2612462043762207, 0.26291877031326294, 0.2625942528247833, 0.263093501329422, 0.26319336891174316, 0.26366767287254333, 0.2624194920063019, 0.2626441717147827, 0.2641420066356659, 0.26329323649406433, 0.26286885142326355, 0.26394227147102356, 0.26359277963638306, 0.2640671133995056, 0.2644166052341461, 0.26549002528190613, 0.26651355624198914, 0.26501572132110596, 0.2655898928642273, 0.26466622948646545, 0.26509061455726624, 0.2665884494781494, 0.265839546918869, 0.2656398117542267, 0.2661890387535095, 0.2666383683681488], \"yaxis\": \"y2\"}],\n",
              "                        {\"legend\": {\"orientation\": \"h\"}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Training Metrics\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.94]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"tickfont\": {\"color\": \"royalblue\"}, \"title\": {\"font\": {\"color\": \"royalblue\"}, \"text\": \"Loss\"}}, \"yaxis2\": {\"anchor\": \"x\", \"overlaying\": \"y\", \"side\": \"right\", \"tickfont\": {\"color\": \"firebrick\"}, \"title\": {\"font\": {\"color\": \"firebrick\"}, \"text\": \"Accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dc2c2089-688c-4027-84aa-30c456c68a31');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMTPlfM8I_Wd",
        "colab_type": "text"
      },
      "source": [
        "# Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W8epfdOM9vm",
        "colab_type": "text"
      },
      "source": [
        "### 100 Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn7qrsCu-LI9",
        "colab_type": "text"
      },
      "source": [
        "<tr>\n",
        "    <th>LSTM Layers</th>\n",
        "    <th>LSTM Cells per Layer</th>\n",
        "    <th>Dropout %</th>\n",
        "    <th>Validation Loss</th>\n",
        "    <th>Validation Accuracy</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2</td>\n",
        "    <td>8.9918</td>\n",
        "    <td>0.2271</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2</td>\n",
        "    <td>10.7950</td>\n",
        "    <td>0.2354</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5</td>\n",
        "    <td>8.9682</td>\n",
        "    <td>0.2153</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>6.9549</td>\n",
        "    <td>0.1490</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>7.4581</td>\n",
        "    <td>0.1683</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5, 0.5</td>\n",
        "    <td>7.3286</td>\n",
        "    <td>0.1650</td>\n",
        "</tr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXjdqe2PNKFv",
        "colab_type": "text"
      },
      "source": [
        "### 500 Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnZhzWmNGKs",
        "colab_type": "text"
      },
      "source": [
        "<tr>\n",
        "    <th>LSTM Layers</th>\n",
        "    <th>LSTM Cells per Layer</th>\n",
        "    <th>Dropout %</th>\n",
        "    <th>Validation Loss</th>\n",
        "    <th>Validation Accuracy</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2</td>\n",
        "    <td>16.4351</td>\n",
        "    <td>0.3229</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2</td>\n",
        "    <td>22.5922</td>\n",
        "    <td>0.3373</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5</td>\n",
        "    <td>18.8511</td>\n",
        "    <td>0.3329</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>8.8127</td>\n",
        "    <td>0.2230</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>10.8240</td>\n",
        "    <td>0.2854</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5, 0.5</td>\n",
        "    <td>10.4388</td>\n",
        "    <td>0.2666</td>\n",
        "</tr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TPnMMNvDAXC",
        "colab_type": "text"
      },
      "source": [
        "# Generate Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rmhEm6UESld",
        "colab_type": "text"
      },
      "source": [
        "### Load Objects To Infer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbsosSoSDBVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-2.h5')\n",
        "model = load_model(model_filepath)\n",
        "TRAINING_LENGTH = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j56FokGJQUKc",
        "colab_type": "text"
      },
      "source": [
        "## Existing Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68jLzLENLLOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "59accda9-8d84-4a8c-c88e-e60bb5e5d9c3"
      },
      "source": [
        "original_sequence, gen_list, a = predict_utils.generate_output(\n",
        "    model,\n",
        "    sequences,\n",
        "    idx_word,\n",
        "    seed_length=TRAINING_LENGTH,\n",
        "    new_words=20,\n",
        "    diversity=1,\n",
        "    n_gen=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py:42: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFkDMZOGLRKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4337d3e-40c8-4980-8b39-ccce6bbab6c2"
      },
      "source": [
        "' '.join(word for word in original_sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'speeds the standard speed is 512kbps though faster connections are'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x2HCA1iLRMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "64380e3d-b2d9-4711-8d43-b43ece4c8580"
      },
      "source": [
        "' '.join(word for word in gen_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'< --- > available this year an analyst at a firm based broadcaster studio telecommunications company in response with 6 to 9 january'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmsySNZULlku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bc5e5aaa-5702-4897-e175-1ae548172774"
      },
      "source": [
        "' '.join(word for word in a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'< --- > available this breakthrough led to a dramatic increase in orders as we were suddenly able to satisfy the pent up'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6sCL1XKLlo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxudAXauQRiT",
        "colab_type": "text"
      },
      "source": [
        "## Custom Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvCuoZqgLls7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "875e9092-97c5-4101-d8ee-46ddf5c6a9e0"
      },
      "source": [
        "sentence = 'Stocks of major large technology firms are becoming even more fragile even though'\n",
        "predict_utils.generate_custom_sentence(sentence, word_idx, idx_word, model, new_words=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 3, 546, 490, 45, 126, 13, 518, 150, 24, 9544, 150, 456]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c04f970c9511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Stocks of major large technology firms are becoming even more fragile even though'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_custom_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py\u001b[0m in \u001b[0;36mgenerate_custom_sentence\u001b[0;34m(sentence, word_idx, idx_word, model, new_words, diversity)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Make a prediction from the seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \t\tpreds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n\u001b[0m\u001b[1;32m     93\u001b[0m \t\t\tnp.float64)\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8_8bcGDQaG7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "74b90cde-dcae-4035-ffc8-a9cb39a67f38"
      },
      "source": [
        "sentence = 'However, there have been many instances of'\n",
        "predict_utils.generate_custom_sentence(sentence, word_idx, idx_word, model, new_words=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 56, 18, 46, 67, 7424, 3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-833bdbd219b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'However, there have been many instances of'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_custom_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py\u001b[0m in \u001b[0;36mgenerate_custom_sentence\u001b[0;34m(sentence, word_idx, idx_word, model, new_words, diversity)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Make a prediction from the seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \t\tpreds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n\u001b[0m\u001b[1;32m     93\u001b[0m \t\t\tnp.float64)\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "240jLlPSQvDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}