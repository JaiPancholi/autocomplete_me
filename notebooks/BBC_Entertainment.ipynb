{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "BBC Entertainment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRwiWNu6INq5",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5LjiPvdBUHV",
        "colab_type": "text"
      },
      "source": [
        "## Google Only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjSCr2_-BUHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e993bc09-db33-436f-cdbb-2e1bdc0d745e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvs4RSdij9st",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAJzt-moBUHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
        "sys.path.append('/content/drive/My Drive/Code/autocomplete_me')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3cVnuFRvRK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7617e338-93d0-4ba0-c745-8558d7d77c98"
      },
      "source": [
        "!ls -l '/content/drive/My Drive/Code/autocomplete_me/src'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 20\n",
            "-rw------- 1 root root 2516 Jul 12 14:10 predict_utils.py\n",
            "drwx------ 2 root root 4096 Jul 11 13:20 __pycache__\n",
            "-rw------- 1 root root 3266 Jul 12 16:08 reader.py\n",
            "-rw------- 1 root root 9341 Jul 12 14:24 utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q94gYQGGBUHe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46719b1c-919c-4683-cf92-a3892c28b854"
      },
      "source": [
        "from src import utils, reader, predict_utils"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK5LK2ohBUHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b2c28cd-4917-4030-fb20-f0a8f8c49038"
      },
      "source": [
        "from importlib import reload\n",
        "reload(utils)\n",
        "reload(reader)\n",
        "reload(predict_utils)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'src.predict_utils' from '/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2i_aQqeF444",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6ACaCZLF9wP",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGdkdzSeBUHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = reader.read_bbc('entertainment')\n",
        "content_type = 'BBC-Entertainment'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFB5H3BdLHAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "c0267bb1-72b8-483f-fcab-e9f802bcbc82"
      },
      "source": [
        "text[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Blair buys copies of new Band Aid\\n\\nPrime Minister Tony Blair purchased two copies of the charity single Band Aid 20 in Edinburgh on Friday.\\n\\nStaff were surprised when the Prime Minister walked into HMV at 0900 GMT, accompanied by aides and local police. \"When Mr Blair came in unannounced, we were all pretty gobsmacked,\" said HMV manager Clive Smith. \"Our customer helper approached him... it was only then we realised he wanted to buy copies of the Band Aid single, rather than the latest Eminem album.\" Predicted chart-topper Do They Know it\\'s Christmas? is expected to sell at least 300,000 copies by the time the new chart is announced on Sunday. However, the new version of the 1984 single is not going to be released in the US, despite being sold in many countries around the world. US record shops are stocking an import version of Do They Know It\\'s Christmas, which is said to be selling very well in Los Angeles and New York. The original track was released in the US, and reached number 13 in the singles chart. British stars who appear on the current recording, such as Dido and Coldplay\\'s Chris Martin, are well-known to music fans across the Atlantic, along with U2 frontman Bono.\\n\\nRecord company Universal is responsible for the global distribution of the single, which will be available across Europe, Asia, South America and Canada. But music fans in the US are still able to access the song and download it on Band Aid 20\\'s official website. In 1985, a group of high-profile American stars known as USA For Africa came together to record their own fund-raising single, We Are The World. The song was written by Lionel Richie and Michael Jackson, with Quincy Jones as producer. It topped the US charts for three weeks and went on win Grammy awards for best record and song.\\n\\nDionne Warwick, Diana Ross and Tina Turner were among the line-up of performers. It is predicted that the Band Aid 20 song will sell 300,000 copies in the UK by the time the new chart is announced on Sunday. The record is also tipped to become this year\\'s Christmas number one, as the original version did in 1984. Proceeds from the sales are going towards relief for the Darfur region of Sudan and to combat HIV and Aids across Africa.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ptgRgRReTyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jTtRx-nX8Ol",
        "colab_type": "text"
      },
      "source": [
        "### Short Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvyKn6MjXaXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8185d3e6-9494-452d-9108-d7957d5bdf5b"
      },
      "source": [
        "training_dict, word_idx, idx_word, sequences, num_words = utils.get_data(text, training_len=20, short=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 15195 unique words.\n",
            "There are 131676 sequences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4XMvOm0Zd6K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f174b14e-8fe0-42c6-b195-3af680ee1151"
      },
      "source": [
        "len(training_dict)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp7d0n2qXkuL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "4a8ef785-5172-439c-a5cc-81efd24d74c6"
      },
      "source": [
        "# embedding_matrix = utils.create_embedding_matrix(word_idx, num_words, '/Users/jaipancholi/data/glove.6B.100d.txt')\n",
        "embedding_matrix = utils.create_embedding_matrix(word_idx, num_words, '/content/drive/My Drive/Code/autocomplete_me/data/glove.6B.100d.txt')\n",
        "embedding_matrix"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glove Vectors loading with dimension 100\n",
            "There were 7098 words without pre-trained embeddings.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Code/autocomplete_me/src/utils.py:180: RuntimeWarning: invalid value encountered in true_divide\n",
            "  embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.00656124, -0.04206555,  0.12508174, ..., -0.02506376,\n",
              "         0.14220549,  0.04648907],\n",
              "       [-0.06223089,  0.03835242,  0.08488411, ..., -0.04284497,\n",
              "         0.08662399, -0.00527513],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.05417296, -0.04813922,  0.13245429, ..., -0.20398643,\n",
              "         0.01218687,  0.03430193]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO9SeBF0vGRb",
        "colab_type": "text"
      },
      "source": [
        "#### Unidirectional, 1 layer, Trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFcGJJduuqw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = utils.make_word_level_model(\n",
        "    num_words,\n",
        "    embedding_matrix,\n",
        "    lstm_cells=64,\n",
        "    trainable=True,\n",
        "    lstm_layers=1,\n",
        "    bi_direc=False\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EycVkVjAutzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43f176a0-ce19-459a-bef2-d8419f64dc99"
      },
      "source": [
        "history = utils.train_model(\n",
        "    training_dict,\n",
        "    f'{content_type}_uni-1_layer-trainable-20_seq',\n",
        "    # model=model,\n",
        "    use_pretrained_model=True,\n",
        "    epochs=250\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 92173 samples, validate on 39503 samples\n",
            "Epoch 1/250\n",
            "92173/92173 [==============================] - 13s 146us/step - loss: 8.3708 - accuracy: 0.0279 - val_loss: 7.2150 - val_accuracy: 0.0464\n",
            "Epoch 2/250\n",
            "92173/92173 [==============================] - 11s 123us/step - loss: 7.1469 - accuracy: 0.0460 - val_loss: 7.1921 - val_accuracy: 0.0464\n",
            "Epoch 3/250\n",
            "92173/92173 [==============================] - 12s 125us/step - loss: 7.0660 - accuracy: 0.0455 - val_loss: 7.1606 - val_accuracy: 0.0464\n",
            "Epoch 4/250\n",
            "92173/92173 [==============================] - 11s 124us/step - loss: 6.9808 - accuracy: 0.0444 - val_loss: 7.1348 - val_accuracy: 0.0460\n",
            "Epoch 5/250\n",
            "92173/92173 [==============================] - 11s 124us/step - loss: 6.9245 - accuracy: 0.0458 - val_loss: 7.1498 - val_accuracy: 0.0464\n",
            "Epoch 6/250\n",
            "92173/92173 [==============================] - 11s 122us/step - loss: 6.8928 - accuracy: 0.0483 - val_loss: 7.1710 - val_accuracy: 0.0417\n",
            "Epoch 7/250\n",
            "92173/92173 [==============================] - 11s 125us/step - loss: 6.8664 - accuracy: 0.0497 - val_loss: 7.1934 - val_accuracy: 0.0470\n",
            "Epoch 8/250\n",
            "92173/92173 [==============================] - 11s 123us/step - loss: 6.8435 - accuracy: 0.0571 - val_loss: 7.1980 - val_accuracy: 0.0708\n",
            "Epoch 9/250\n",
            "92173/92173 [==============================] - 11s 124us/step - loss: 6.8186 - accuracy: 0.0649 - val_loss: 7.1899 - val_accuracy: 0.0716\n",
            "Epoch 10/250\n",
            "92173/92173 [==============================] - 11s 125us/step - loss: 6.7802 - accuracy: 0.0703 - val_loss: 7.1660 - val_accuracy: 0.0735\n",
            "Epoch 11/250\n",
            "92173/92173 [==============================] - 11s 124us/step - loss: 6.7374 - accuracy: 0.0748 - val_loss: 7.1470 - val_accuracy: 0.0764\n",
            "Epoch 12/250\n",
            "92173/92173 [==============================] - 11s 125us/step - loss: 6.6934 - accuracy: 0.0771 - val_loss: 7.1316 - val_accuracy: 0.0776\n",
            "Epoch 13/250\n",
            "92173/92173 [==============================] - 11s 124us/step - loss: 6.6552 - accuracy: 0.0798 - val_loss: 7.1277 - val_accuracy: 0.0788\n",
            "Epoch 14/250\n",
            "92173/92173 [==============================] - 11s 124us/step - loss: 6.6128 - accuracy: 0.0822 - val_loss: 7.1297 - val_accuracy: 0.0780\n",
            "Epoch 15/250\n",
            "92173/92173 [==============================] - 11s 123us/step - loss: 6.5782 - accuracy: 0.0843 - val_loss: 7.1401 - val_accuracy: 0.0803\n",
            "Epoch 16/250\n",
            "92173/92173 [==============================] - 11s 122us/step - loss: 6.5437 - accuracy: 0.0863 - val_loss: 7.1404 - val_accuracy: 0.0804\n",
            "Epoch 17/250\n",
            "92173/92173 [==============================] - 11s 122us/step - loss: 6.5088 - accuracy: 0.0865 - val_loss: 7.1491 - val_accuracy: 0.0811\n",
            "Epoch 18/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 6.4787 - accuracy: 0.0888 - val_loss: 7.1717 - val_accuracy: 0.0820\n",
            "Epoch 19/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 6.4438 - accuracy: 0.0925 - val_loss: 7.1641 - val_accuracy: 0.0832\n",
            "Epoch 20/250\n",
            "92173/92173 [==============================] - 11s 123us/step - loss: 6.4117 - accuracy: 0.0949 - val_loss: 7.1802 - val_accuracy: 0.0918\n",
            "Epoch 21/250\n",
            "92173/92173 [==============================] - 11s 123us/step - loss: 6.3776 - accuracy: 0.0996 - val_loss: 7.1869 - val_accuracy: 0.0927\n",
            "Epoch 22/250\n",
            "92173/92173 [==============================] - 11s 122us/step - loss: 6.3415 - accuracy: 0.1006 - val_loss: 7.1607 - val_accuracy: 0.0909\n",
            "Epoch 23/250\n",
            "92173/92173 [==============================] - 11s 122us/step - loss: 6.3022 - accuracy: 0.1028 - val_loss: 7.1793 - val_accuracy: 0.0928\n",
            "Epoch 24/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 6.2660 - accuracy: 0.1052 - val_loss: 7.1771 - val_accuracy: 0.0927\n",
            "Epoch 25/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 6.2284 - accuracy: 0.1074 - val_loss: 7.1564 - val_accuracy: 0.0918\n",
            "Epoch 26/250\n",
            "92173/92173 [==============================] - 11s 123us/step - loss: 6.1889 - accuracy: 0.1100 - val_loss: 7.1768 - val_accuracy: 0.0941\n",
            "Epoch 27/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 6.1482 - accuracy: 0.1124 - val_loss: 7.1654 - val_accuracy: 0.0955\n",
            "Epoch 28/250\n",
            "92173/92173 [==============================] - 11s 122us/step - loss: 6.1068 - accuracy: 0.1125 - val_loss: 7.1789 - val_accuracy: 0.0969\n",
            "Epoch 29/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 6.0607 - accuracy: 0.1146 - val_loss: 7.1860 - val_accuracy: 0.0987\n",
            "Epoch 30/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 6.0155 - accuracy: 0.1142 - val_loss: 7.1780 - val_accuracy: 0.0999\n",
            "Epoch 31/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 5.9714 - accuracy: 0.1169 - val_loss: 7.1593 - val_accuracy: 0.0994\n",
            "Epoch 32/250\n",
            "92173/92173 [==============================] - 11s 122us/step - loss: 5.9276 - accuracy: 0.1180 - val_loss: 7.1753 - val_accuracy: 0.1022\n",
            "Epoch 33/250\n",
            "92173/92173 [==============================] - 11s 124us/step - loss: 5.8852 - accuracy: 0.1211 - val_loss: 7.2108 - val_accuracy: 0.1058\n",
            "Epoch 34/250\n",
            "92173/92173 [==============================] - 11s 122us/step - loss: 5.8403 - accuracy: 0.1225 - val_loss: 7.1813 - val_accuracy: 0.1054\n",
            "Epoch 35/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 5.7949 - accuracy: 0.1251 - val_loss: 7.2014 - val_accuracy: 0.1072\n",
            "Epoch 36/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 5.7570 - accuracy: 0.1265 - val_loss: 7.1794 - val_accuracy: 0.1090\n",
            "Epoch 37/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 5.7108 - accuracy: 0.1306 - val_loss: 7.2204 - val_accuracy: 0.1098\n",
            "Epoch 38/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 5.6762 - accuracy: 0.1308 - val_loss: 7.2589 - val_accuracy: 0.1119\n",
            "Epoch 39/250\n",
            "92173/92173 [==============================] - 11s 124us/step - loss: 5.6419 - accuracy: 0.1338 - val_loss: 7.2495 - val_accuracy: 0.1154\n",
            "Epoch 40/250\n",
            "92173/92173 [==============================] - 11s 122us/step - loss: 5.5980 - accuracy: 0.1364 - val_loss: 7.2569 - val_accuracy: 0.1159\n",
            "Epoch 41/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 5.5611 - accuracy: 0.1383 - val_loss: 7.2324 - val_accuracy: 0.1154\n",
            "Epoch 42/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 5.5246 - accuracy: 0.1397 - val_loss: 7.2803 - val_accuracy: 0.1162\n",
            "Epoch 43/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 5.4895 - accuracy: 0.1400 - val_loss: 7.2929 - val_accuracy: 0.1191\n",
            "Epoch 44/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 5.4515 - accuracy: 0.1416 - val_loss: 7.3320 - val_accuracy: 0.1185\n",
            "Epoch 45/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 5.4175 - accuracy: 0.1451 - val_loss: 7.3138 - val_accuracy: 0.1194\n",
            "Epoch 46/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 5.3721 - accuracy: 0.1490 - val_loss: 7.3535 - val_accuracy: 0.1216\n",
            "Epoch 47/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 5.3350 - accuracy: 0.1499 - val_loss: 7.3178 - val_accuracy: 0.1204\n",
            "Epoch 48/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 5.3039 - accuracy: 0.1500 - val_loss: 7.3628 - val_accuracy: 0.1207\n",
            "Epoch 49/250\n",
            "92173/92173 [==============================] - 11s 121us/step - loss: 5.2631 - accuracy: 0.1509 - val_loss: 7.4169 - val_accuracy: 0.1233\n",
            "Epoch 50/250\n",
            "92173/92173 [==============================] - 11s 118us/step - loss: 5.2201 - accuracy: 0.1551 - val_loss: 7.3998 - val_accuracy: 0.1240\n",
            "Epoch 51/250\n",
            "92173/92173 [==============================] - 11s 119us/step - loss: 5.1850 - accuracy: 0.1569 - val_loss: 7.4487 - val_accuracy: 0.1260\n",
            "Epoch 52/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 5.1486 - accuracy: 0.1580 - val_loss: 7.4307 - val_accuracy: 0.1266\n",
            "Epoch 53/250\n",
            "92173/92173 [==============================] - 11s 120us/step - loss: 5.1095 - accuracy: 0.1599 - val_loss: 7.4502 - val_accuracy: 0.1274\n",
            "Epoch 54/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 5.0733 - accuracy: 0.1622 - val_loss: 7.4634 - val_accuracy: 0.1281\n",
            "Epoch 55/250\n",
            "92173/92173 [==============================] - 11s 118us/step - loss: 5.0427 - accuracy: 0.1634 - val_loss: 7.5066 - val_accuracy: 0.1281\n",
            "Epoch 56/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 5.0053 - accuracy: 0.1660 - val_loss: 7.5345 - val_accuracy: 0.1298\n",
            "Epoch 57/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.9686 - accuracy: 0.1677 - val_loss: 7.5377 - val_accuracy: 0.1309\n",
            "Epoch 58/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.9349 - accuracy: 0.1700 - val_loss: 7.5784 - val_accuracy: 0.1323\n",
            "Epoch 59/250\n",
            "92173/92173 [==============================] - 11s 118us/step - loss: 4.8980 - accuracy: 0.1711 - val_loss: 7.5905 - val_accuracy: 0.1334\n",
            "Epoch 60/250\n",
            "92173/92173 [==============================] - 11s 119us/step - loss: 4.8698 - accuracy: 0.1752 - val_loss: 7.6540 - val_accuracy: 0.1333\n",
            "Epoch 61/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.8271 - accuracy: 0.1760 - val_loss: 7.6591 - val_accuracy: 0.1338\n",
            "Epoch 62/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 4.7992 - accuracy: 0.1781 - val_loss: 7.6653 - val_accuracy: 0.1332\n",
            "Epoch 63/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.7661 - accuracy: 0.1799 - val_loss: 7.7094 - val_accuracy: 0.1348\n",
            "Epoch 64/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.7325 - accuracy: 0.1813 - val_loss: 7.7324 - val_accuracy: 0.1340\n",
            "Epoch 65/250\n",
            "92173/92173 [==============================] - 11s 119us/step - loss: 4.7009 - accuracy: 0.1826 - val_loss: 7.7937 - val_accuracy: 0.1367\n",
            "Epoch 66/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 4.6715 - accuracy: 0.1853 - val_loss: 7.8152 - val_accuracy: 0.1362\n",
            "Epoch 67/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.6382 - accuracy: 0.1855 - val_loss: 7.8445 - val_accuracy: 0.1377\n",
            "Epoch 68/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 4.6109 - accuracy: 0.1885 - val_loss: 7.8292 - val_accuracy: 0.1372\n",
            "Epoch 69/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.5797 - accuracy: 0.1913 - val_loss: 7.8753 - val_accuracy: 0.1374\n",
            "Epoch 70/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.5505 - accuracy: 0.1925 - val_loss: 7.9086 - val_accuracy: 0.1390\n",
            "Epoch 71/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 4.5245 - accuracy: 0.1958 - val_loss: 7.9546 - val_accuracy: 0.1394\n",
            "Epoch 72/250\n",
            "92173/92173 [==============================] - 11s 118us/step - loss: 4.4974 - accuracy: 0.1950 - val_loss: 7.9194 - val_accuracy: 0.1384\n",
            "Epoch 73/250\n",
            "92173/92173 [==============================] - 11s 118us/step - loss: 4.4715 - accuracy: 0.1979 - val_loss: 8.0308 - val_accuracy: 0.1419\n",
            "Epoch 74/250\n",
            "92173/92173 [==============================] - 11s 118us/step - loss: 4.4377 - accuracy: 0.2006 - val_loss: 8.0398 - val_accuracy: 0.1399\n",
            "Epoch 75/250\n",
            "92173/92173 [==============================] - 11s 118us/step - loss: 4.4098 - accuracy: 0.2024 - val_loss: 8.0407 - val_accuracy: 0.1403\n",
            "Epoch 76/250\n",
            "92173/92173 [==============================] - 11s 118us/step - loss: 4.3895 - accuracy: 0.2043 - val_loss: 8.0805 - val_accuracy: 0.1412\n",
            "Epoch 77/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.3618 - accuracy: 0.2060 - val_loss: 8.1543 - val_accuracy: 0.1422\n",
            "Epoch 78/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 4.3351 - accuracy: 0.2078 - val_loss: 8.1394 - val_accuracy: 0.1416\n",
            "Epoch 79/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 4.3180 - accuracy: 0.2107 - val_loss: 8.2165 - val_accuracy: 0.1413\n",
            "Epoch 80/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 4.2856 - accuracy: 0.2120 - val_loss: 8.2183 - val_accuracy: 0.1437\n",
            "Epoch 81/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.2656 - accuracy: 0.2137 - val_loss: 8.2661 - val_accuracy: 0.1436\n",
            "Epoch 82/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.2400 - accuracy: 0.2151 - val_loss: 8.2707 - val_accuracy: 0.1447\n",
            "Epoch 83/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 4.2178 - accuracy: 0.2159 - val_loss: 8.3102 - val_accuracy: 0.1444\n",
            "Epoch 84/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 4.1908 - accuracy: 0.2197 - val_loss: 8.3909 - val_accuracy: 0.1446\n",
            "Epoch 85/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 4.1684 - accuracy: 0.2199 - val_loss: 8.4271 - val_accuracy: 0.1434\n",
            "Epoch 86/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 4.1505 - accuracy: 0.2205 - val_loss: 8.4479 - val_accuracy: 0.1465\n",
            "Epoch 87/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 4.1257 - accuracy: 0.2252 - val_loss: 8.4732 - val_accuracy: 0.1444\n",
            "Epoch 88/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 4.1129 - accuracy: 0.2252 - val_loss: 8.5183 - val_accuracy: 0.1468\n",
            "Epoch 89/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 4.0881 - accuracy: 0.2267 - val_loss: 8.5418 - val_accuracy: 0.1462\n",
            "Epoch 90/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 4.0623 - accuracy: 0.2307 - val_loss: 8.5673 - val_accuracy: 0.1459\n",
            "Epoch 91/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 4.0471 - accuracy: 0.2345 - val_loss: 8.6406 - val_accuracy: 0.1476\n",
            "Epoch 92/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 4.0248 - accuracy: 0.2321 - val_loss: 8.7246 - val_accuracy: 0.1480\n",
            "Epoch 93/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 4.0060 - accuracy: 0.2355 - val_loss: 8.7054 - val_accuracy: 0.1469\n",
            "Epoch 94/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.9895 - accuracy: 0.2366 - val_loss: 8.7171 - val_accuracy: 0.1480\n",
            "Epoch 95/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.9621 - accuracy: 0.2391 - val_loss: 8.7793 - val_accuracy: 0.1490\n",
            "Epoch 96/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.9431 - accuracy: 0.2421 - val_loss: 8.7982 - val_accuracy: 0.1483\n",
            "Epoch 97/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.9278 - accuracy: 0.2434 - val_loss: 8.8537 - val_accuracy: 0.1498\n",
            "Epoch 98/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.9198 - accuracy: 0.2430 - val_loss: 8.8982 - val_accuracy: 0.1491\n",
            "Epoch 99/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.8865 - accuracy: 0.2474 - val_loss: 8.9071 - val_accuracy: 0.1484\n",
            "Epoch 100/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.8736 - accuracy: 0.2494 - val_loss: 8.9156 - val_accuracy: 0.1503\n",
            "Epoch 101/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.8638 - accuracy: 0.2506 - val_loss: 8.9518 - val_accuracy: 0.1502\n",
            "Epoch 102/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.8463 - accuracy: 0.2526 - val_loss: 9.0299 - val_accuracy: 0.1514\n",
            "Epoch 103/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.8290 - accuracy: 0.2548 - val_loss: 9.0814 - val_accuracy: 0.1502\n",
            "Epoch 104/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.8071 - accuracy: 0.2569 - val_loss: 9.0663 - val_accuracy: 0.1500\n",
            "Epoch 105/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.7863 - accuracy: 0.2589 - val_loss: 9.1329 - val_accuracy: 0.1514\n",
            "Epoch 106/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.7742 - accuracy: 0.2613 - val_loss: 9.2012 - val_accuracy: 0.1504\n",
            "Epoch 107/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.7686 - accuracy: 0.2596 - val_loss: 9.1915 - val_accuracy: 0.1527\n",
            "Epoch 108/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.7442 - accuracy: 0.2636 - val_loss: 9.2364 - val_accuracy: 0.1518\n",
            "Epoch 109/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.7309 - accuracy: 0.2648 - val_loss: 9.2353 - val_accuracy: 0.1525\n",
            "Epoch 110/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.7167 - accuracy: 0.2660 - val_loss: 9.3031 - val_accuracy: 0.1516\n",
            "Epoch 111/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.6990 - accuracy: 0.2677 - val_loss: 9.2828 - val_accuracy: 0.1519\n",
            "Epoch 112/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.6953 - accuracy: 0.2679 - val_loss: 9.3738 - val_accuracy: 0.1534\n",
            "Epoch 113/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.6742 - accuracy: 0.2703 - val_loss: 9.3921 - val_accuracy: 0.1525\n",
            "Epoch 114/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.6590 - accuracy: 0.2717 - val_loss: 9.4464 - val_accuracy: 0.1541\n",
            "Epoch 115/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.6448 - accuracy: 0.2728 - val_loss: 9.4584 - val_accuracy: 0.1531\n",
            "Epoch 116/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.6321 - accuracy: 0.2758 - val_loss: 9.5399 - val_accuracy: 0.1540\n",
            "Epoch 117/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.6186 - accuracy: 0.2775 - val_loss: 9.5656 - val_accuracy: 0.1542\n",
            "Epoch 118/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.5904 - accuracy: 0.2774 - val_loss: 9.5969 - val_accuracy: 0.1546\n",
            "Epoch 119/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.5868 - accuracy: 0.2828 - val_loss: 9.5990 - val_accuracy: 0.1541\n",
            "Epoch 120/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.5716 - accuracy: 0.2830 - val_loss: 9.6202 - val_accuracy: 0.1540\n",
            "Epoch 121/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.5641 - accuracy: 0.2849 - val_loss: 9.6652 - val_accuracy: 0.1552\n",
            "Epoch 122/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.5500 - accuracy: 0.2849 - val_loss: 9.6925 - val_accuracy: 0.1544\n",
            "Epoch 123/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.5406 - accuracy: 0.2850 - val_loss: 9.7351 - val_accuracy: 0.1547\n",
            "Epoch 124/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.5224 - accuracy: 0.2893 - val_loss: 9.7626 - val_accuracy: 0.1553\n",
            "Epoch 125/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.5098 - accuracy: 0.2894 - val_loss: 9.8555 - val_accuracy: 0.1564\n",
            "Epoch 126/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.5113 - accuracy: 0.2899 - val_loss: 9.8429 - val_accuracy: 0.1576\n",
            "Epoch 127/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.4840 - accuracy: 0.2944 - val_loss: 9.8912 - val_accuracy: 0.1559\n",
            "Epoch 128/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.4770 - accuracy: 0.2955 - val_loss: 9.9241 - val_accuracy: 0.1570\n",
            "Epoch 129/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.4668 - accuracy: 0.2960 - val_loss: 9.9506 - val_accuracy: 0.1571\n",
            "Epoch 130/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.4545 - accuracy: 0.2957 - val_loss: 9.9482 - val_accuracy: 0.1564\n",
            "Epoch 131/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.4459 - accuracy: 0.2970 - val_loss: 10.0551 - val_accuracy: 0.1564\n",
            "Epoch 132/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 3.4254 - accuracy: 0.2992 - val_loss: 10.0212 - val_accuracy: 0.1574\n",
            "Epoch 133/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.4188 - accuracy: 0.3026 - val_loss: 10.1188 - val_accuracy: 0.1585\n",
            "Epoch 134/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.4047 - accuracy: 0.3011 - val_loss: 10.1012 - val_accuracy: 0.1581\n",
            "Epoch 135/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.3873 - accuracy: 0.3045 - val_loss: 10.1687 - val_accuracy: 0.1580\n",
            "Epoch 136/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.3740 - accuracy: 0.3079 - val_loss: 10.2586 - val_accuracy: 0.1594\n",
            "Epoch 137/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.3709 - accuracy: 0.3076 - val_loss: 10.2600 - val_accuracy: 0.1587\n",
            "Epoch 138/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.3599 - accuracy: 0.3085 - val_loss: 10.2396 - val_accuracy: 0.1579\n",
            "Epoch 139/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 3.3571 - accuracy: 0.3080 - val_loss: 10.3969 - val_accuracy: 0.1583\n",
            "Epoch 140/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.3365 - accuracy: 0.3106 - val_loss: 10.3899 - val_accuracy: 0.1596\n",
            "Epoch 141/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.3338 - accuracy: 0.3125 - val_loss: 10.4323 - val_accuracy: 0.1597\n",
            "Epoch 142/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.3145 - accuracy: 0.3142 - val_loss: 10.4497 - val_accuracy: 0.1586\n",
            "Epoch 143/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.3140 - accuracy: 0.3143 - val_loss: 10.4759 - val_accuracy: 0.1599\n",
            "Epoch 144/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.2983 - accuracy: 0.3151 - val_loss: 10.4744 - val_accuracy: 0.1582\n",
            "Epoch 145/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.2893 - accuracy: 0.3160 - val_loss: 10.5230 - val_accuracy: 0.1594\n",
            "Epoch 146/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.2797 - accuracy: 0.3196 - val_loss: 10.6364 - val_accuracy: 0.1599\n",
            "Epoch 147/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.2646 - accuracy: 0.3227 - val_loss: 10.5594 - val_accuracy: 0.1603\n",
            "Epoch 148/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.2558 - accuracy: 0.3217 - val_loss: 10.6600 - val_accuracy: 0.1598\n",
            "Epoch 149/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.2445 - accuracy: 0.3224 - val_loss: 10.6270 - val_accuracy: 0.1609\n",
            "Epoch 150/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.2462 - accuracy: 0.3223 - val_loss: 10.7174 - val_accuracy: 0.1609\n",
            "Epoch 151/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.2297 - accuracy: 0.3258 - val_loss: 10.7832 - val_accuracy: 0.1614\n",
            "Epoch 152/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.2164 - accuracy: 0.3277 - val_loss: 10.7764 - val_accuracy: 0.1606\n",
            "Epoch 153/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.2098 - accuracy: 0.3284 - val_loss: 10.8083 - val_accuracy: 0.1609\n",
            "Epoch 154/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.1899 - accuracy: 0.3312 - val_loss: 10.8714 - val_accuracy: 0.1608\n",
            "Epoch 155/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.1914 - accuracy: 0.3310 - val_loss: 10.8688 - val_accuracy: 0.1619\n",
            "Epoch 156/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.1850 - accuracy: 0.3307 - val_loss: 10.9264 - val_accuracy: 0.1617\n",
            "Epoch 157/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.1775 - accuracy: 0.3313 - val_loss: 10.9362 - val_accuracy: 0.1613\n",
            "Epoch 158/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.1663 - accuracy: 0.3331 - val_loss: 10.8824 - val_accuracy: 0.1614\n",
            "Epoch 159/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.1661 - accuracy: 0.3340 - val_loss: 10.9584 - val_accuracy: 0.1605\n",
            "Epoch 160/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.1449 - accuracy: 0.3371 - val_loss: 11.0782 - val_accuracy: 0.1615\n",
            "Epoch 161/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.1392 - accuracy: 0.3378 - val_loss: 11.1040 - val_accuracy: 0.1609\n",
            "Epoch 162/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.1209 - accuracy: 0.3389 - val_loss: 11.1425 - val_accuracy: 0.1624\n",
            "Epoch 163/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.1182 - accuracy: 0.3418 - val_loss: 11.1700 - val_accuracy: 0.1616\n",
            "Epoch 164/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.1125 - accuracy: 0.3410 - val_loss: 11.1258 - val_accuracy: 0.1615\n",
            "Epoch 165/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.1120 - accuracy: 0.3405 - val_loss: 11.2369 - val_accuracy: 0.1622\n",
            "Epoch 166/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.0964 - accuracy: 0.3429 - val_loss: 11.2603 - val_accuracy: 0.1614\n",
            "Epoch 167/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.1056 - accuracy: 0.3429 - val_loss: 11.2581 - val_accuracy: 0.1612\n",
            "Epoch 168/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.0915 - accuracy: 0.3455 - val_loss: 11.3142 - val_accuracy: 0.1619\n",
            "Epoch 169/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.0707 - accuracy: 0.3474 - val_loss: 11.2755 - val_accuracy: 0.1618\n",
            "Epoch 170/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.0715 - accuracy: 0.3472 - val_loss: 11.3823 - val_accuracy: 0.1628\n",
            "Epoch 171/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 3.0553 - accuracy: 0.3477 - val_loss: 11.3961 - val_accuracy: 0.1624\n",
            "Epoch 172/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 3.0475 - accuracy: 0.3493 - val_loss: 11.3975 - val_accuracy: 0.1631\n",
            "Epoch 173/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.0514 - accuracy: 0.3501 - val_loss: 11.3739 - val_accuracy: 0.1626\n",
            "Epoch 174/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.0383 - accuracy: 0.3512 - val_loss: 11.4976 - val_accuracy: 0.1636\n",
            "Epoch 175/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 3.0268 - accuracy: 0.3545 - val_loss: 11.5163 - val_accuracy: 0.1626\n",
            "Epoch 176/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.0243 - accuracy: 0.3538 - val_loss: 11.6054 - val_accuracy: 0.1622\n",
            "Epoch 177/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 3.0116 - accuracy: 0.3538 - val_loss: 11.6359 - val_accuracy: 0.1633\n",
            "Epoch 178/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.0084 - accuracy: 0.3549 - val_loss: 11.6951 - val_accuracy: 0.1638\n",
            "Epoch 179/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.9961 - accuracy: 0.3567 - val_loss: 11.5926 - val_accuracy: 0.1636\n",
            "Epoch 180/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 3.0178 - accuracy: 0.3540 - val_loss: 11.6713 - val_accuracy: 0.1637\n",
            "Epoch 181/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 2.9820 - accuracy: 0.3583 - val_loss: 11.7259 - val_accuracy: 0.1631\n",
            "Epoch 182/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 2.9791 - accuracy: 0.3598 - val_loss: 11.7124 - val_accuracy: 0.1632\n",
            "Epoch 183/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 2.9711 - accuracy: 0.3605 - val_loss: 11.8233 - val_accuracy: 0.1634\n",
            "Epoch 184/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.9552 - accuracy: 0.3630 - val_loss: 11.8518 - val_accuracy: 0.1640\n",
            "Epoch 185/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 2.9517 - accuracy: 0.3635 - val_loss: 11.8531 - val_accuracy: 0.1647\n",
            "Epoch 186/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 2.9451 - accuracy: 0.3628 - val_loss: 11.9618 - val_accuracy: 0.1633\n",
            "Epoch 187/250\n",
            "92173/92173 [==============================] - 10s 111us/step - loss: 2.9417 - accuracy: 0.3619 - val_loss: 11.9140 - val_accuracy: 0.1640\n",
            "Epoch 188/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.9323 - accuracy: 0.3650 - val_loss: 12.0569 - val_accuracy: 0.1639\n",
            "Epoch 189/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.9269 - accuracy: 0.3673 - val_loss: 12.1171 - val_accuracy: 0.1641\n",
            "Epoch 190/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.9210 - accuracy: 0.3678 - val_loss: 12.0555 - val_accuracy: 0.1641\n",
            "Epoch 191/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.9080 - accuracy: 0.3709 - val_loss: 12.0117 - val_accuracy: 0.1645\n",
            "Epoch 192/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.8975 - accuracy: 0.3687 - val_loss: 12.1408 - val_accuracy: 0.1650\n",
            "Epoch 193/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.8942 - accuracy: 0.3716 - val_loss: 12.1271 - val_accuracy: 0.1645\n",
            "Epoch 194/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.8868 - accuracy: 0.3714 - val_loss: 12.1372 - val_accuracy: 0.1637\n",
            "Epoch 195/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.8822 - accuracy: 0.3718 - val_loss: 12.2918 - val_accuracy: 0.1642\n",
            "Epoch 196/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 2.8799 - accuracy: 0.3738 - val_loss: 12.2095 - val_accuracy: 0.1639\n",
            "Epoch 197/250\n",
            "92173/92173 [==============================] - 10s 111us/step - loss: 2.8670 - accuracy: 0.3747 - val_loss: 12.3275 - val_accuracy: 0.1626\n",
            "Epoch 198/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.8670 - accuracy: 0.3768 - val_loss: 12.3336 - val_accuracy: 0.1637\n",
            "Epoch 199/250\n",
            "92173/92173 [==============================] - 10s 111us/step - loss: 2.8545 - accuracy: 0.3774 - val_loss: 12.3030 - val_accuracy: 0.1649\n",
            "Epoch 200/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.8534 - accuracy: 0.3779 - val_loss: 12.3680 - val_accuracy: 0.1650\n",
            "Epoch 201/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.8414 - accuracy: 0.3784 - val_loss: 12.3459 - val_accuracy: 0.1647\n",
            "Epoch 202/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.8449 - accuracy: 0.3784 - val_loss: 12.4373 - val_accuracy: 0.1645\n",
            "Epoch 203/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.8378 - accuracy: 0.3794 - val_loss: 12.4450 - val_accuracy: 0.1633\n",
            "Epoch 204/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.8230 - accuracy: 0.3824 - val_loss: 12.4571 - val_accuracy: 0.1633\n",
            "Epoch 205/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.8330 - accuracy: 0.3802 - val_loss: 12.5514 - val_accuracy: 0.1649\n",
            "Epoch 206/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 2.8076 - accuracy: 0.3840 - val_loss: 12.5621 - val_accuracy: 0.1651\n",
            "Epoch 207/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.8069 - accuracy: 0.3818 - val_loss: 12.5673 - val_accuracy: 0.1642\n",
            "Epoch 208/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.8055 - accuracy: 0.3840 - val_loss: 12.6896 - val_accuracy: 0.1650\n",
            "Epoch 209/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.8094 - accuracy: 0.3829 - val_loss: 12.5816 - val_accuracy: 0.1650\n",
            "Epoch 210/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.7938 - accuracy: 0.3865 - val_loss: 12.7076 - val_accuracy: 0.1665\n",
            "Epoch 211/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 2.7821 - accuracy: 0.3861 - val_loss: 12.7299 - val_accuracy: 0.1660\n",
            "Epoch 212/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.7832 - accuracy: 0.3874 - val_loss: 12.7897 - val_accuracy: 0.1657\n",
            "Epoch 213/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.7644 - accuracy: 0.3887 - val_loss: 12.7805 - val_accuracy: 0.1651\n",
            "Epoch 214/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 2.7688 - accuracy: 0.3895 - val_loss: 12.8183 - val_accuracy: 0.1657\n",
            "Epoch 215/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.7558 - accuracy: 0.3909 - val_loss: 12.9212 - val_accuracy: 0.1652\n",
            "Epoch 216/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 2.7570 - accuracy: 0.3887 - val_loss: 12.8975 - val_accuracy: 0.1668\n",
            "Epoch 217/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.7428 - accuracy: 0.3935 - val_loss: 12.7918 - val_accuracy: 0.1660\n",
            "Epoch 218/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 2.7361 - accuracy: 0.3945 - val_loss: 12.9124 - val_accuracy: 0.1648\n",
            "Epoch 219/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.7371 - accuracy: 0.3946 - val_loss: 13.0301 - val_accuracy: 0.1653\n",
            "Epoch 220/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.7230 - accuracy: 0.3965 - val_loss: 12.9011 - val_accuracy: 0.1658\n",
            "Epoch 221/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 2.7282 - accuracy: 0.3955 - val_loss: 12.9948 - val_accuracy: 0.1671\n",
            "Epoch 222/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.7185 - accuracy: 0.3973 - val_loss: 13.1222 - val_accuracy: 0.1672\n",
            "Epoch 223/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.7131 - accuracy: 0.3956 - val_loss: 13.1821 - val_accuracy: 0.1674\n",
            "Epoch 224/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.7098 - accuracy: 0.3986 - val_loss: 13.1425 - val_accuracy: 0.1662\n",
            "Epoch 225/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.6996 - accuracy: 0.3999 - val_loss: 13.1076 - val_accuracy: 0.1660\n",
            "Epoch 226/250\n",
            "92173/92173 [==============================] - 10s 111us/step - loss: 2.6968 - accuracy: 0.4021 - val_loss: 13.3403 - val_accuracy: 0.1663\n",
            "Epoch 227/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.6919 - accuracy: 0.4014 - val_loss: 13.2243 - val_accuracy: 0.1658\n",
            "Epoch 228/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.6910 - accuracy: 0.4007 - val_loss: 13.2831 - val_accuracy: 0.1657\n",
            "Epoch 229/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 2.6866 - accuracy: 0.4020 - val_loss: 13.2941 - val_accuracy: 0.1658\n",
            "Epoch 230/250\n",
            "92173/92173 [==============================] - 11s 116us/step - loss: 2.6745 - accuracy: 0.4046 - val_loss: 13.1897 - val_accuracy: 0.1653\n",
            "Epoch 231/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.6763 - accuracy: 0.4022 - val_loss: 13.2145 - val_accuracy: 0.1649\n",
            "Epoch 232/250\n",
            "92173/92173 [==============================] - 11s 115us/step - loss: 2.6808 - accuracy: 0.4028 - val_loss: 13.3769 - val_accuracy: 0.1663\n",
            "Epoch 233/250\n",
            "92173/92173 [==============================] - 11s 117us/step - loss: 2.6680 - accuracy: 0.4050 - val_loss: 13.3690 - val_accuracy: 0.1653\n",
            "Epoch 234/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 2.6588 - accuracy: 0.4072 - val_loss: 13.4358 - val_accuracy: 0.1658\n",
            "Epoch 235/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.6474 - accuracy: 0.4070 - val_loss: 13.5016 - val_accuracy: 0.1658\n",
            "Epoch 236/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.6392 - accuracy: 0.4089 - val_loss: 13.5047 - val_accuracy: 0.1664\n",
            "Epoch 237/250\n",
            "92173/92173 [==============================] - 10s 111us/step - loss: 2.6347 - accuracy: 0.4093 - val_loss: 13.5256 - val_accuracy: 0.1666\n",
            "Epoch 238/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.6297 - accuracy: 0.4094 - val_loss: 13.5815 - val_accuracy: 0.1674\n",
            "Epoch 239/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.6216 - accuracy: 0.4108 - val_loss: 13.6502 - val_accuracy: 0.1672\n",
            "Epoch 240/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.6146 - accuracy: 0.4121 - val_loss: 13.6113 - val_accuracy: 0.1662\n",
            "Epoch 241/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.6113 - accuracy: 0.4127 - val_loss: 13.5724 - val_accuracy: 0.1652\n",
            "Epoch 242/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 2.6100 - accuracy: 0.4125 - val_loss: 13.7159 - val_accuracy: 0.1667\n",
            "Epoch 243/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 2.6114 - accuracy: 0.4120 - val_loss: 13.6552 - val_accuracy: 0.1666\n",
            "Epoch 244/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 2.5971 - accuracy: 0.4140 - val_loss: 13.7684 - val_accuracy: 0.1662\n",
            "Epoch 245/250\n",
            "92173/92173 [==============================] - 10s 114us/step - loss: 2.5912 - accuracy: 0.4162 - val_loss: 13.8860 - val_accuracy: 0.1672\n",
            "Epoch 246/250\n",
            "92173/92173 [==============================] - 10s 112us/step - loss: 2.5969 - accuracy: 0.4141 - val_loss: 13.7433 - val_accuracy: 0.1665\n",
            "Epoch 247/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.5867 - accuracy: 0.4155 - val_loss: 13.8809 - val_accuracy: 0.1666\n",
            "Epoch 248/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.5869 - accuracy: 0.4162 - val_loss: 13.8280 - val_accuracy: 0.1657\n",
            "Epoch 249/250\n",
            "92173/92173 [==============================] - 10s 113us/step - loss: 2.5882 - accuracy: 0.4176 - val_loss: 13.8759 - val_accuracy: 0.1658\n",
            "Epoch 250/250\n",
            "92173/92173 [==============================] - 11s 114us/step - loss: 2.5733 - accuracy: 0.4186 - val_loss: 13.7993 - val_accuracy: 0.1657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-AP8swgwKuL",
        "colab_type": "text"
      },
      "source": [
        "## Compare Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ5j11YH-Ilt",
        "colab_type": "text"
      },
      "source": [
        "<tr>\n",
        "    <th>Name of Model</th>\n",
        "    <th>LTSM Layers</th>\n",
        "    <th>Bidirectional</th>\n",
        "    <th>Trainable Embeddings</th>\n",
        "    <th>Sequence Length</th>\n",
        "    <th>LSTM Dropout</th>\n",
        "    <th>LSTM Recurrent Dropout</th>\n",
        "    <th>Val Loss</th>\n",
        "    <th>Val Accuracy</th>\n",
        "</tr>\n",
        "<!-- <tr>\n",
        "    <td></td>\n",
        "    <td>1</td>\n",
        "    <td>False</td>\n",
        "    <td>True</td>\n",
        "    <td>50</td>\n",
        "    <td>0.1</td>\n",
        "    <td>0.1</td>\n",
        "    <td>6.5235</td>\n",
        "    <td>0.1715</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td></td>\n",
        "    <td>1</td>\n",
        "    <td>False</td>\n",
        "    <td>True</td>\n",
        "    <td>50</td>\n",
        "    <td>0.2</td>\n",
        "    <td>0.2</td>\n",
        "    <td>6.4229</td>\n",
        "    <td>0.1706</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td></td>\n",
        "    <td>1</td>\n",
        "    <td>False</td>\n",
        "    <td>True</td>\n",
        "    <td>50</td>\n",
        "    <td>0.5</td>\n",
        "    <td>0.5</td>\n",
        "    <td>6.2247</td>\n",
        "    <td>0.1638</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td></td>\n",
        "    <td>1</td>\n",
        "    <td>False</td>\n",
        "    <td>True</td>\n",
        "    <td>20</td>\n",
        "    <td>0.1</td>\n",
        "    <td>0.1</td>\n",
        "    <td>6.4449</td>\n",
        "    <td>0.1725</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td></td>\n",
        "    <td>1</td>\n",
        "    <td>False</td>\n",
        "    <td>True</td>\n",
        "    <td>20</td>\n",
        "    <td>0.2</td>\n",
        "    <td>0.2</td>\n",
        "    <td>6.2899</td>\n",
        "    <td>0.1706</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td></td>\n",
        "    <td>1</td>\n",
        "    <td>False</td>\n",
        "    <td>True</td>\n",
        "    <td>20</td>\n",
        "    <td>0.5</td>\n",
        "    <td>0.5</td>\n",
        "    <td>6.1667</td>\n",
        "    <td>0.1719</td>\n",
        "</tr> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4sO-VEvxo27",
        "colab_type": "text"
      },
      "source": [
        "# Sample Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYttbKDgsWU-",
        "colab_type": "text"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TYDcQIf6-Yz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1e88c347-2e1c-479a-c00e-1f1fc7e26aa2"
      },
      "source": [
        "from keras.models import load_model\n",
        "# Get Model Weights and Architecture\n",
        "\n",
        "# MODELS_DIR = os.path.join(os.path.dirname(os.path.abspath('')), 'models')\n",
        "MODELS_DIR = '/content/drive/My Drive/Code/autocomplete_me/models'\n",
        "model_filepath = os.path.join(MODELS_DIR, f'{content_type}_uni-1_layer-trainable-20_seq.h5')\n",
        "\n",
        "model = load_model(model_filepath)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHGsZlQj6-b6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "24b29ba8-46a3-469c-92fc-aaca9a481591"
      },
      "source": [
        "# Get Text Data\n",
        "TRAINING_LENGTH = 20\n",
        "training_dict, word_idx, idx_word, sequences, num_words = utils.get_data(text, training_len=TRAINING_LENGTH)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 15195 unique words.\n",
            "There are 131676 sequences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaZAatAF6-hZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_sequence, gen_list, a = predict_utils.generate_output(\n",
        "    model,\n",
        "    sequences,\n",
        "    idx_word,\n",
        "    seed_length=TRAINING_LENGTH,\n",
        "    new_words=50,\n",
        "    diversity=1,\n",
        "    n_gen=1\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64w4-h_e7Jr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2bcdfbb8-22e8-4a94-e093-2bed814f5778"
      },
      "source": [
        "' '.join(word for word in original_sequence)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Coronation Street pulled away from its BBC One rival EastEnders in the ratings and dominated other TV awards . Last'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1jQ8g547Ju7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c2a8a382-f53f-447b-9047-226414732685"
      },
      "source": [
        "' '.join(word for word in gen_list[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'< --- > released of books to a respect to Fox by reveal of musical more turning each tells the one . Animation was month . somebody Of show . Christmas and to jail original news ago and him on money of the Shakespeare , crime Zach angry . rapper describing at The'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmdlXoedZMK-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8501217e-c4bf-4c6e-c487-953224f8f869"
      },
      "source": [
        "' '.join(word for word in a)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"< --- > year , the South Bank Award for best TV drama went to Steven Poliakoff's period piece The Lost Prince , while Bloody Sunday , about the 1972 killings in Northern Ireland , won in 2003. In other South Bank categories , Little Britain's second series beat Nighty Night and The\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3UVTdcZ7Jz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2ecf2924-b64e-40ad-ed9b-0d7cf3f6423c"
      },
      "source": [
        "sentence = 'Stocks of major large technology firms are becoming'\n",
        "predict_utils.generate_custom_sentence(sentence, word_idx, idx_word, model, new_words=50)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 5, 498, 1619, 3683, 13749, 33, 1141]\n",
            "which after The Podium to been Christmas 1999 to and (Twentieth as seen a Other which of I but 13 rights refused and attention is date he this sing it's John McEwan the Their Trapp , . half in one Festival , week the artists at best in Mike incur\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG6xgAXVu0Og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "de021890-9b09-475e-aa8d-4df0ca4a2141"
      },
      "source": [
        "sentence = 'However, there have been many instances of'\n",
        "predict_utils.generate_custom_sentence(sentence, word_idx, idx_word, model, new_words=50)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 100, 27, 38, 189, None, 5]\n",
            "Pictures wanting the whose ceremony Caroline broken of loopholes , in second also that 25th Fightstar that - May Taylor into garnered overcome ever deals on stage Live gallery prize it all the more station any any , well a £100,000 grail . Wal-Mart season of composer week and other\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE1ngq-fZBAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}