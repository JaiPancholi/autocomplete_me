{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "autocomplete_me",
      "language": "python",
      "name": "autocomplete_me"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "BBC Business - Trial Own Process.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvWcI8T2eOvw",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNXYErppeOvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "010260ad-d99c-4066-9de3-4235bf99972e"
      },
      "source": [
        "# Google Only\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ROOT_FOLDER = '/content/drive/My Drive/Code/autocomplete_me/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9p9MmnceOv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Set Variables for Local and Cloud File Finding\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(ROOT_FOLDER)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W397RMzgCvy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c006412f-9682-4e11-d81b-fddf4798e7ad"
      },
      "source": [
        "!ls -l '/content/drive/My Drive/Code/autocomplete_me/src'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 27\n",
            "-rw------- 1 root root 2516 Jul 14 22:20 predict_utils.py\n",
            "drwx------ 2 root root 4096 Jul 11 13:20 __pycache__\n",
            "-rw------- 1 root root 3340 Jul 15 12:47 reader.py\n",
            "-rw------- 1 root root 3580 Jul 14 22:20 train_model_baseline.py\n",
            "-rw------- 1 root root 3203 Jul 14 22:20 train_utils.py\n",
            "-rw------- 1 root root 9341 Jul 12 14:24 utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fiv0531eOv7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1de7f23f-3260-4c47-b3f5-a46edc1ebc6f"
      },
      "source": [
        "from src import utils, reader, predict_utils, train_utils\n",
        "from importlib import reload\n",
        "reload(utils)\n",
        "reload(reader)\n",
        "reload(predict_utils)\n",
        "reload(train_utils)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'src.train_utils' from '/content/drive/My Drive/Code/autocomplete_me/src/train_utils.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpn1lT4IeOv_",
        "colab_type": "text"
      },
      "source": [
        "## Load Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhWMNtqceOv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = reader.read_bbc('business')\n",
        "content_type = 'BBC-Business'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YyHF8aQeOwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "e3c433df-5175-4f59-a87d-5fb9aabfb784"
      },
      "source": [
        "text[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'UK economy facing \\'major risks\\'\\n\\nThe UK manufacturing sector will continue to face \"serious challenges\" over the next two years, the British Chamber of Commerce (BCC) has said.\\n\\nThe group\\'s quarterly survey of companies found exports had picked up in the last three months of 2004 to their best levels in eight years. The rise came despite exchange rates being cited as a major concern. However, the BCC found the whole UK economy still faced \"major risks\" and warned that growth is set to slow. It recently forecast economic growth will slow from more than 3% in 2004 to a little below 2.5% in both 2005 and 2006.\\n\\nManufacturers\\' domestic sales growth fell back slightly in the quarter, the survey of 5,196 firms found. Employment in manufacturing also fell and job expectations were at their lowest level for a year.\\n\\n\"Despite some positive news for the export sector, there are worrying signs for manufacturing,\" the BCC said. \"These results reinforce our concern over the sector\\'s persistent inability to sustain recovery.\" The outlook for the service sector was \"uncertain\" despite an increase in exports and orders over the quarter, the BCC noted.\\n\\nThe BCC found confidence increased in the quarter across both the manufacturing and service sectors although overall it failed to reach the levels at the start of 2004. The reduced threat of interest rate increases had contributed to improved confidence, it said. The Bank of England raised interest rates five times between November 2003 and August last year. But rates have been kept on hold since then amid signs of falling consumer confidence and a slowdown in output. \"The pressure on costs and margins, the relentless increase in regulations, and the threat of higher taxes remain serious problems,\" BCC director general David Frost said. \"While consumer spending is set to decelerate significantly over the next 12-18 months, it is unlikely that investment and exports will rise sufficiently strongly to pick up the slack.\"\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8y2Nv_MeOwE",
        "colab_type": "text"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Q8y2ILeOwH",
        "colab_type": "text"
      },
      "source": [
        "## Process Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtdMjwySG_ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences, num_words, word_idx, idx_word = train_utils.preprocess_text(text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESiYIwBkHN7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac897a8d-71c5-4d60-8465-d1f4826aec95"
      },
      "source": [
        "features, labels = train_utils.pass_sliding_window(sequences, sequence_len=10)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 165294 sequences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gny329GmHSMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4fb62cd1-b506-4fdc-96d5-95d0f0e0992c"
      },
      "source": [
        "labels = train_utils.one_hot_labels_and_improve_efficiency(labels)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels matrix shape:  (165294, 12675)\n",
            "Labels matrix shape:  (165294, 12675)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eny8qjVzeOwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Test Train Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20, random_state=42, shuffle=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x5EeUMbkpPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73da202e-5a63-4cfd-c8cd-f87022e235b0"
      },
      "source": [
        "import gc\n",
        "gc.enable()\n",
        "del labels\n",
        "gc.collect()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8N7zK7IeOwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7096a33c-9405-44e9-b7d4-7018fa00cd9d"
      },
      "source": [
        "print('X_train shape: ', X_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape:  (132235, 10)\n",
            "X_test shape:  (33059, 10)\n",
            "y_train shape:  (132235, 12675)\n",
            "y_test shape:  (33059, 12675)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOA6uzXKlu0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f83b92fa-3998-4463-ec9b-2bf136f7c02c"
      },
      "source": [
        "import sys\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "                         key= lambda x: -x[1])[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       y_train:  1.4 GiB\n",
            "                        y_test: 355.5 MiB\n",
            "                      features: 12.6 MiB\n",
            "                       X_train: 10.1 MiB\n",
            "                        X_test:  2.5 MiB\n",
            "                      word_idx: 576.1 KiB\n",
            "                      idx_word: 576.1 KiB\n",
            "                     sequences:  4.5 KiB\n",
            "                          text:  4.2 KiB\n",
            "                            __:  778.0 B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DROHQO32eOwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "b44eced6-5f30-4641-9ee4-8e18b8b859d8"
      },
      "source": [
        "# Embedding Matrix\n",
        "# embedding_matrix = utils.create_embedding_matrix(word_idx, num_words, '/Users/jaipancholi/data/glove.6B.100d.txt')\n",
        "embedding_matrix = utils.create_embedding_matrix(word_idx, num_words, '/content/drive/My Drive/Code/autocomplete_me/data/glove.6B.100d.txt')\n",
        "embedding_matrix"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glove Vectors loading with dimension 100\n",
            "There were 1387 words without pre-trained embeddings.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Code/autocomplete_me/src/utils.py:180: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.00656124, -0.04206555,  0.12508174, ..., -0.02506376,\n",
              "         0.14220549,  0.04648907],\n",
              "       [-0.02940788,  0.00775488,  0.02958461, ..., -0.0617054 ,\n",
              "         0.07386386, -0.02477734],\n",
              "       ...,\n",
              "       [ 0.03117583, -0.01426646, -0.09698666, ..., -0.0022697 ,\n",
              "         0.1283632 ,  0.13427433],\n",
              "       [ 0.09864704, -0.09904199, -0.07051982, ...,  0.01649705,\n",
              "        -0.12355457,  0.12704042],\n",
              "       [ 0.14297517,  0.01937677, -0.11857604, ..., -0.00369095,\n",
              "        -0.25356799, -0.00142062]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDWbIoaAcRbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1284478f-6496-43ff-f640-f0085ba52faf"
      },
      "source": [
        "embedding_matrix.shape\n",
        "# temp_embedding_matrix = embedding_matrix[:-1, :]\n",
        "# temp_embedding_matrix.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12675, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IZp-EmFeOwY",
        "colab_type": "text"
      },
      "source": [
        "# Design Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gstEw4vgGsPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1eSpudQeOwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZOti2z7JJW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=False, epochs=100):\n",
        "  if not model and not use_pretrained_model:\n",
        "    print('Provide one of either model or use_pretrained_model.')\n",
        "  elif model and use_pretrained_model:\n",
        "      print('Provide one of either model or use_pretrained_model.')\n",
        "  elif use_pretrained_model:\n",
        "    model = load_model(model_filepath)\n",
        "  \n",
        "  callbacks = [\n",
        "      EarlyStopping(monitor='val_accuracy', patience=25),\n",
        "      ModelCheckpoint(f'{model_filepath}', save_best_only=True, save_weights_only=False, monitor='val_accuracy')\n",
        "  ]\n",
        "\n",
        "  history = model.fit(\n",
        "      X_train, \n",
        "      y_train, \n",
        "      epochs=epochs, \n",
        "      batch_size=2048, \n",
        "      validation_data=(X_test, y_test), \n",
        "      verbose=1,\n",
        "      callbacks=callbacks\n",
        "  )\n",
        "\n",
        "  return history"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHNsyM7H_qkC",
        "colab_type": "text"
      },
      "source": [
        "##V1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoT44K4meOwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    # weights=[temp_embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(64))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBtAHhA1eOwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "outputId": "5a3e6fa0-012a-460d-d53e-97022c6d759c"
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-1.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-d94c7cce5157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{content_type}-custom-1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pretrained_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-f92ebd24014c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(filepath, X_train, y_train, X_test, y_test, use_pretrained_model, model, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 11272) and (None, 11273) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3pRn4k_2Jx",
        "colab_type": "text"
      },
      "source": [
        "## V2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSI6BxH5ALw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfRMwq-rALzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "910f0ccd-5460-4560-f683-50b49d266e5d"
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-2.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "65/65 [==============================] - 9s 132ms/step - loss: 7.6509 - accuracy: 0.0622 - val_loss: 7.1367 - val_accuracy: 0.0641\n",
            "Epoch 2/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 7.0544 - accuracy: 0.0646 - val_loss: 7.0874 - val_accuracy: 0.0641\n",
            "Epoch 3/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 6.9668 - accuracy: 0.0646 - val_loss: 7.0468 - val_accuracy: 0.0641\n",
            "Epoch 4/500\n",
            "65/65 [==============================] - 8s 127ms/step - loss: 6.8907 - accuracy: 0.0688 - val_loss: 6.9774 - val_accuracy: 0.0737\n",
            "Epoch 5/500\n",
            "65/65 [==============================] - 8s 121ms/step - loss: 6.7650 - accuracy: 0.0744 - val_loss: 6.8847 - val_accuracy: 0.0822\n",
            "Epoch 6/500\n",
            "65/65 [==============================] - 8s 121ms/step - loss: 6.6304 - accuracy: 0.0836 - val_loss: 6.8216 - val_accuracy: 0.0869\n",
            "Epoch 7/500\n",
            "65/65 [==============================] - 8s 120ms/step - loss: 6.5102 - accuracy: 0.0910 - val_loss: 6.7701 - val_accuracy: 0.0920\n",
            "Epoch 8/500\n",
            "65/65 [==============================] - 8s 121ms/step - loss: 6.4070 - accuracy: 0.0975 - val_loss: 6.7375 - val_accuracy: 0.0962\n",
            "Epoch 9/500\n",
            "65/65 [==============================] - 8s 121ms/step - loss: 6.2982 - accuracy: 0.1039 - val_loss: 6.6878 - val_accuracy: 0.1047\n",
            "Epoch 10/500\n",
            "65/65 [==============================] - 8s 123ms/step - loss: 6.1801 - accuracy: 0.1113 - val_loss: 6.6413 - val_accuracy: 0.1091\n",
            "Epoch 11/500\n",
            "65/65 [==============================] - 8s 121ms/step - loss: 6.0627 - accuracy: 0.1185 - val_loss: 6.5993 - val_accuracy: 0.1138\n",
            "Epoch 12/500\n",
            "65/65 [==============================] - 8s 122ms/step - loss: 5.9536 - accuracy: 0.1258 - val_loss: 6.5757 - val_accuracy: 0.1189\n",
            "Epoch 13/500\n",
            "65/65 [==============================] - 8s 127ms/step - loss: 5.8523 - accuracy: 0.1316 - val_loss: 6.5534 - val_accuracy: 0.1226\n",
            "Epoch 14/500\n",
            "65/65 [==============================] - 8s 130ms/step - loss: 5.7556 - accuracy: 0.1369 - val_loss: 6.5292 - val_accuracy: 0.1248\n",
            "Epoch 15/500\n",
            "65/65 [==============================] - 8s 119ms/step - loss: 5.6674 - accuracy: 0.1416 - val_loss: 6.5295 - val_accuracy: 0.1286\n",
            "Epoch 16/500\n",
            "65/65 [==============================] - 8s 119ms/step - loss: 5.5804 - accuracy: 0.1449 - val_loss: 6.5298 - val_accuracy: 0.1296\n",
            "Epoch 17/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 5.5005 - accuracy: 0.1485 - val_loss: 6.5427 - val_accuracy: 0.1325\n",
            "Epoch 18/500\n",
            "65/65 [==============================] - 8s 120ms/step - loss: 5.4188 - accuracy: 0.1516 - val_loss: 6.5554 - val_accuracy: 0.1352\n",
            "Epoch 19/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 5.3420 - accuracy: 0.1555 - val_loss: 6.5813 - val_accuracy: 0.1365\n",
            "Epoch 20/500\n",
            "65/65 [==============================] - 8s 119ms/step - loss: 5.2706 - accuracy: 0.1593 - val_loss: 6.6025 - val_accuracy: 0.1396\n",
            "Epoch 21/500\n",
            "65/65 [==============================] - 7s 115ms/step - loss: 5.2014 - accuracy: 0.1631 - val_loss: 6.6397 - val_accuracy: 0.1393\n",
            "Epoch 22/500\n",
            "65/65 [==============================] - 8s 124ms/step - loss: 5.1339 - accuracy: 0.1657 - val_loss: 6.6793 - val_accuracy: 0.1411\n",
            "Epoch 23/500\n",
            "65/65 [==============================] - 8s 120ms/step - loss: 5.0681 - accuracy: 0.1689 - val_loss: 6.7060 - val_accuracy: 0.1424\n",
            "Epoch 24/500\n",
            "65/65 [==============================] - 8s 119ms/step - loss: 5.0059 - accuracy: 0.1728 - val_loss: 6.7427 - val_accuracy: 0.1439\n",
            "Epoch 25/500\n",
            "65/65 [==============================] - 8s 119ms/step - loss: 4.9422 - accuracy: 0.1765 - val_loss: 6.7888 - val_accuracy: 0.1470\n",
            "Epoch 26/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 4.8815 - accuracy: 0.1797 - val_loss: 6.8212 - val_accuracy: 0.1467\n",
            "Epoch 27/500\n",
            "65/65 [==============================] - 8s 123ms/step - loss: 4.8246 - accuracy: 0.1833 - val_loss: 6.8768 - val_accuracy: 0.1486\n",
            "Epoch 28/500\n",
            "65/65 [==============================] - 8s 119ms/step - loss: 4.7632 - accuracy: 0.1872 - val_loss: 6.9311 - val_accuracy: 0.1488\n",
            "Epoch 29/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 4.7050 - accuracy: 0.1914 - val_loss: 6.9658 - val_accuracy: 0.1499\n",
            "Epoch 30/500\n",
            "65/65 [==============================] - 8s 120ms/step - loss: 4.6461 - accuracy: 0.1953 - val_loss: 7.0197 - val_accuracy: 0.1503\n",
            "Epoch 31/500\n",
            "65/65 [==============================] - 8s 128ms/step - loss: 4.5878 - accuracy: 0.1996 - val_loss: 7.0666 - val_accuracy: 0.1529\n",
            "Epoch 32/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 4.5332 - accuracy: 0.2027 - val_loss: 7.1251 - val_accuracy: 0.1523\n",
            "Epoch 33/500\n",
            "65/65 [==============================] - 8s 125ms/step - loss: 4.4755 - accuracy: 0.2085 - val_loss: 7.1641 - val_accuracy: 0.1531\n",
            "Epoch 34/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 4.4215 - accuracy: 0.2125 - val_loss: 7.2311 - val_accuracy: 0.1523\n",
            "Epoch 35/500\n",
            "65/65 [==============================] - 8s 125ms/step - loss: 4.3673 - accuracy: 0.2168 - val_loss: 7.2856 - val_accuracy: 0.1538\n",
            "Epoch 36/500\n",
            "65/65 [==============================] - 8s 120ms/step - loss: 4.3153 - accuracy: 0.2209 - val_loss: 7.3193 - val_accuracy: 0.1544\n",
            "Epoch 37/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 4.2646 - accuracy: 0.2253 - val_loss: 7.3702 - val_accuracy: 0.1535\n",
            "Epoch 38/500\n",
            "65/65 [==============================] - 8s 125ms/step - loss: 4.2108 - accuracy: 0.2299 - val_loss: 7.4428 - val_accuracy: 0.1551\n",
            "Epoch 39/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 4.1655 - accuracy: 0.2342 - val_loss: 7.5064 - val_accuracy: 0.1539\n",
            "Epoch 40/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 4.1145 - accuracy: 0.2380 - val_loss: 7.5526 - val_accuracy: 0.1544\n",
            "Epoch 41/500\n",
            "65/65 [==============================] - 8s 126ms/step - loss: 4.0656 - accuracy: 0.2415 - val_loss: 7.6133 - val_accuracy: 0.1564\n",
            "Epoch 42/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 4.0187 - accuracy: 0.2468 - val_loss: 7.6710 - val_accuracy: 0.1553\n",
            "Epoch 43/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 3.9703 - accuracy: 0.2504 - val_loss: 7.7433 - val_accuracy: 0.1560\n",
            "Epoch 44/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 3.9249 - accuracy: 0.2554 - val_loss: 7.7852 - val_accuracy: 0.1558\n",
            "Epoch 45/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 3.8821 - accuracy: 0.2591 - val_loss: 7.8629 - val_accuracy: 0.1552\n",
            "Epoch 46/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 3.8383 - accuracy: 0.2627 - val_loss: 7.9275 - val_accuracy: 0.1546\n",
            "Epoch 47/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 3.7945 - accuracy: 0.2676 - val_loss: 7.9789 - val_accuracy: 0.1558\n",
            "Epoch 48/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 3.7549 - accuracy: 0.2724 - val_loss: 8.0474 - val_accuracy: 0.1545\n",
            "Epoch 49/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 3.7139 - accuracy: 0.2756 - val_loss: 8.1088 - val_accuracy: 0.1552\n",
            "Epoch 50/500\n",
            "65/65 [==============================] - 8s 127ms/step - loss: 3.6744 - accuracy: 0.2812 - val_loss: 8.1778 - val_accuracy: 0.1567\n",
            "Epoch 51/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 3.6343 - accuracy: 0.2849 - val_loss: 8.2158 - val_accuracy: 0.1548\n",
            "Epoch 52/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 3.5982 - accuracy: 0.2875 - val_loss: 8.3059 - val_accuracy: 0.1547\n",
            "Epoch 53/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 3.5630 - accuracy: 0.2928 - val_loss: 8.3768 - val_accuracy: 0.1545\n",
            "Epoch 54/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 3.5254 - accuracy: 0.2974 - val_loss: 8.4234 - val_accuracy: 0.1552\n",
            "Epoch 55/500\n",
            "65/65 [==============================] - 8s 122ms/step - loss: 3.4888 - accuracy: 0.3019 - val_loss: 8.5058 - val_accuracy: 0.1552\n",
            "Epoch 56/500\n",
            "65/65 [==============================] - 8s 121ms/step - loss: 3.4548 - accuracy: 0.3047 - val_loss: 8.5469 - val_accuracy: 0.1554\n",
            "Epoch 57/500\n",
            "65/65 [==============================] - 7s 115ms/step - loss: 3.4229 - accuracy: 0.3071 - val_loss: 8.6207 - val_accuracy: 0.1539\n",
            "Epoch 58/500\n",
            "65/65 [==============================] - 7s 115ms/step - loss: 3.3925 - accuracy: 0.3119 - val_loss: 8.6609 - val_accuracy: 0.1530\n",
            "Epoch 59/500\n",
            "65/65 [==============================] - 8s 115ms/step - loss: 3.3549 - accuracy: 0.3163 - val_loss: 8.7545 - val_accuracy: 0.1551\n",
            "Epoch 60/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 3.3206 - accuracy: 0.3207 - val_loss: 8.7843 - val_accuracy: 0.1525\n",
            "Epoch 61/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 3.2890 - accuracy: 0.3240 - val_loss: 8.8732 - val_accuracy: 0.1544\n",
            "Epoch 62/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 3.2579 - accuracy: 0.3279 - val_loss: 8.9408 - val_accuracy: 0.1528\n",
            "Epoch 63/500\n",
            "65/65 [==============================] - 8s 115ms/step - loss: 3.2201 - accuracy: 0.3343 - val_loss: 9.0010 - val_accuracy: 0.1521\n",
            "Epoch 64/500\n",
            "65/65 [==============================] - 7s 115ms/step - loss: 3.1966 - accuracy: 0.3369 - val_loss: 9.0351 - val_accuracy: 0.1533\n",
            "Epoch 65/500\n",
            "65/65 [==============================] - 7s 115ms/step - loss: 3.1629 - accuracy: 0.3400 - val_loss: 9.1405 - val_accuracy: 0.1532\n",
            "Epoch 66/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 3.1346 - accuracy: 0.3463 - val_loss: 9.2144 - val_accuracy: 0.1529\n",
            "Epoch 67/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 3.1032 - accuracy: 0.3501 - val_loss: 9.2829 - val_accuracy: 0.1517\n",
            "Epoch 68/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 3.0760 - accuracy: 0.3524 - val_loss: 9.3215 - val_accuracy: 0.1525\n",
            "Epoch 69/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 3.0497 - accuracy: 0.3557 - val_loss: 9.3986 - val_accuracy: 0.1524\n",
            "Epoch 70/500\n",
            "65/65 [==============================] - 8s 119ms/step - loss: 3.0162 - accuracy: 0.3601 - val_loss: 9.4698 - val_accuracy: 0.1496\n",
            "Epoch 71/500\n",
            "65/65 [==============================] - 8s 118ms/step - loss: 2.9866 - accuracy: 0.3653 - val_loss: 9.5322 - val_accuracy: 0.1491\n",
            "Epoch 72/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 2.9595 - accuracy: 0.3668 - val_loss: 9.6046 - val_accuracy: 0.1531\n",
            "Epoch 73/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 2.9257 - accuracy: 0.3740 - val_loss: 9.6945 - val_accuracy: 0.1532\n",
            "Epoch 74/500\n",
            "65/65 [==============================] - 8s 116ms/step - loss: 2.9036 - accuracy: 0.3773 - val_loss: 9.7307 - val_accuracy: 0.1524\n",
            "Epoch 75/500\n",
            "65/65 [==============================] - 8s 117ms/step - loss: 2.8781 - accuracy: 0.3802 - val_loss: 9.7914 - val_accuracy: 0.1508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8f780579e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoP-_Ot7AO4Z",
        "colab_type": "text"
      },
      "source": [
        "## V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK14bGSjeOwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZeVf9SAnCmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a441bf2f-2fc8-46f7-c3cd-ee9f9f496658"
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-3.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "49/49 [==============================] - 6s 126ms/step - loss: 7.8511 - accuracy: 0.0482 - val_loss: 7.2717 - val_accuracy: 0.0643\n",
            "Epoch 2/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 7.1611 - accuracy: 0.0653 - val_loss: 7.2390 - val_accuracy: 0.0643\n",
            "Epoch 3/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 7.0862 - accuracy: 0.0653 - val_loss: 7.1782 - val_accuracy: 0.0643\n",
            "Epoch 4/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 7.0221 - accuracy: 0.0653 - val_loss: 7.1771 - val_accuracy: 0.0643\n",
            "Epoch 5/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 6.9842 - accuracy: 0.0653 - val_loss: 7.1751 - val_accuracy: 0.0643\n",
            "Epoch 6/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 6.9300 - accuracy: 0.0673 - val_loss: 7.1258 - val_accuracy: 0.0729\n",
            "Epoch 7/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 6.8353 - accuracy: 0.0724 - val_loss: 7.0439 - val_accuracy: 0.0773\n",
            "Epoch 8/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.7140 - accuracy: 0.0789 - val_loss: 6.9856 - val_accuracy: 0.0833\n",
            "Epoch 9/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 6.6062 - accuracy: 0.0851 - val_loss: 6.9560 - val_accuracy: 0.0851\n",
            "Epoch 10/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.5149 - accuracy: 0.0892 - val_loss: 6.9468 - val_accuracy: 0.0870\n",
            "Epoch 11/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.4370 - accuracy: 0.0933 - val_loss: 6.9373 - val_accuracy: 0.0897\n",
            "Epoch 12/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 6.3588 - accuracy: 0.0971 - val_loss: 6.9302 - val_accuracy: 0.0923\n",
            "Epoch 13/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.2783 - accuracy: 0.1028 - val_loss: 6.9158 - val_accuracy: 0.0962\n",
            "Epoch 14/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.1933 - accuracy: 0.1072 - val_loss: 6.9049 - val_accuracy: 0.0995\n",
            "Epoch 15/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 6.1111 - accuracy: 0.1110 - val_loss: 6.8953 - val_accuracy: 0.1022\n",
            "Epoch 16/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 6.0318 - accuracy: 0.1150 - val_loss: 6.9013 - val_accuracy: 0.1039\n",
            "Epoch 17/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.9581 - accuracy: 0.1176 - val_loss: 6.9097 - val_accuracy: 0.1055\n",
            "Epoch 18/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.8890 - accuracy: 0.1211 - val_loss: 6.9235 - val_accuracy: 0.1097\n",
            "Epoch 19/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.8224 - accuracy: 0.1238 - val_loss: 6.9428 - val_accuracy: 0.1117\n",
            "Epoch 20/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.7580 - accuracy: 0.1272 - val_loss: 6.9433 - val_accuracy: 0.1136\n",
            "Epoch 21/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.6933 - accuracy: 0.1291 - val_loss: 6.9676 - val_accuracy: 0.1157\n",
            "Epoch 22/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.6337 - accuracy: 0.1328 - val_loss: 6.9900 - val_accuracy: 0.1175\n",
            "Epoch 23/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.5736 - accuracy: 0.1351 - val_loss: 7.0100 - val_accuracy: 0.1182\n",
            "Epoch 24/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.5132 - accuracy: 0.1379 - val_loss: 7.0415 - val_accuracy: 0.1203\n",
            "Epoch 25/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.4546 - accuracy: 0.1409 - val_loss: 7.0750 - val_accuracy: 0.1217\n",
            "Epoch 26/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.4008 - accuracy: 0.1439 - val_loss: 7.0960 - val_accuracy: 0.1253\n",
            "Epoch 27/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 5.3451 - accuracy: 0.1459 - val_loss: 7.1198 - val_accuracy: 0.1257\n",
            "Epoch 28/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.2908 - accuracy: 0.1494 - val_loss: 7.1575 - val_accuracy: 0.1273\n",
            "Epoch 29/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 5.2389 - accuracy: 0.1518 - val_loss: 7.1785 - val_accuracy: 0.1282\n",
            "Epoch 30/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.1841 - accuracy: 0.1538 - val_loss: 7.2275 - val_accuracy: 0.1293\n",
            "Epoch 31/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 5.1324 - accuracy: 0.1576 - val_loss: 7.2500 - val_accuracy: 0.1292\n",
            "Epoch 32/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 5.0840 - accuracy: 0.1589 - val_loss: 7.2979 - val_accuracy: 0.1322\n",
            "Epoch 33/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 5.0340 - accuracy: 0.1612 - val_loss: 7.3385 - val_accuracy: 0.1349\n",
            "Epoch 34/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.9872 - accuracy: 0.1636 - val_loss: 7.3515 - val_accuracy: 0.1343\n",
            "Epoch 35/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 4.9378 - accuracy: 0.1662 - val_loss: 7.4067 - val_accuracy: 0.1355\n",
            "Epoch 36/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 4.8879 - accuracy: 0.1685 - val_loss: 7.4657 - val_accuracy: 0.1354\n",
            "Epoch 37/500\n",
            "49/49 [==============================] - 6s 130ms/step - loss: 4.8382 - accuracy: 0.1722 - val_loss: 7.4855 - val_accuracy: 0.1367\n",
            "Epoch 38/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 4.7907 - accuracy: 0.1746 - val_loss: 7.5264 - val_accuracy: 0.1387\n",
            "Epoch 39/500\n",
            "49/49 [==============================] - 6s 121ms/step - loss: 4.7431 - accuracy: 0.1778 - val_loss: 7.5627 - val_accuracy: 0.1416\n",
            "Epoch 40/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 4.6949 - accuracy: 0.1800 - val_loss: 7.6152 - val_accuracy: 0.1411\n",
            "Epoch 41/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.6487 - accuracy: 0.1833 - val_loss: 7.6521 - val_accuracy: 0.1415\n",
            "Epoch 42/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.6018 - accuracy: 0.1862 - val_loss: 7.6882 - val_accuracy: 0.1415\n",
            "Epoch 43/500\n",
            "49/49 [==============================] - 7s 133ms/step - loss: 4.5567 - accuracy: 0.1899 - val_loss: 7.7436 - val_accuracy: 0.1440\n",
            "Epoch 44/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.5094 - accuracy: 0.1943 - val_loss: 7.8111 - val_accuracy: 0.1443\n",
            "Epoch 45/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.4647 - accuracy: 0.1965 - val_loss: 7.8273 - val_accuracy: 0.1447\n",
            "Epoch 46/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.4238 - accuracy: 0.1984 - val_loss: 7.8806 - val_accuracy: 0.1455\n",
            "Epoch 47/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.3801 - accuracy: 0.2029 - val_loss: 7.9261 - val_accuracy: 0.1464\n",
            "Epoch 48/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 4.3394 - accuracy: 0.2061 - val_loss: 7.9509 - val_accuracy: 0.1467\n",
            "Epoch 49/500\n",
            "49/49 [==============================] - 6s 120ms/step - loss: 4.2977 - accuracy: 0.2102 - val_loss: 8.0202 - val_accuracy: 0.1480\n",
            "Epoch 50/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.2547 - accuracy: 0.2146 - val_loss: 8.0689 - val_accuracy: 0.1475\n",
            "Epoch 51/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.2173 - accuracy: 0.2170 - val_loss: 8.1083 - val_accuracy: 0.1463\n",
            "Epoch 52/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 4.1754 - accuracy: 0.2217 - val_loss: 8.1688 - val_accuracy: 0.1490\n",
            "Epoch 53/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 4.1426 - accuracy: 0.2245 - val_loss: 8.1930 - val_accuracy: 0.1458\n",
            "Epoch 54/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 4.1028 - accuracy: 0.2276 - val_loss: 8.2571 - val_accuracy: 0.1471\n",
            "Epoch 55/500\n",
            "49/49 [==============================] - 6s 130ms/step - loss: 4.0658 - accuracy: 0.2323 - val_loss: 8.2854 - val_accuracy: 0.1507\n",
            "Epoch 56/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 4.0311 - accuracy: 0.2347 - val_loss: 8.3609 - val_accuracy: 0.1502\n",
            "Epoch 57/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 4.0005 - accuracy: 0.2383 - val_loss: 8.4175 - val_accuracy: 0.1489\n",
            "Epoch 58/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.9618 - accuracy: 0.2419 - val_loss: 8.4418 - val_accuracy: 0.1491\n",
            "Epoch 59/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.9290 - accuracy: 0.2449 - val_loss: 8.5110 - val_accuracy: 0.1489\n",
            "Epoch 60/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.9030 - accuracy: 0.2482 - val_loss: 8.5190 - val_accuracy: 0.1493\n",
            "Epoch 61/500\n",
            "49/49 [==============================] - 6s 128ms/step - loss: 3.8660 - accuracy: 0.2515 - val_loss: 8.5913 - val_accuracy: 0.1509\n",
            "Epoch 62/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.8364 - accuracy: 0.2542 - val_loss: 8.6288 - val_accuracy: 0.1471\n",
            "Epoch 63/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.8095 - accuracy: 0.2577 - val_loss: 8.7083 - val_accuracy: 0.1507\n",
            "Epoch 64/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.7766 - accuracy: 0.2601 - val_loss: 8.7507 - val_accuracy: 0.1493\n",
            "Epoch 65/500\n",
            "49/49 [==============================] - 6s 131ms/step - loss: 3.7497 - accuracy: 0.2638 - val_loss: 8.8040 - val_accuracy: 0.1511\n",
            "Epoch 66/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.7216 - accuracy: 0.2652 - val_loss: 8.8302 - val_accuracy: 0.1488\n",
            "Epoch 67/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.6935 - accuracy: 0.2693 - val_loss: 8.8959 - val_accuracy: 0.1490\n",
            "Epoch 68/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.6708 - accuracy: 0.2696 - val_loss: 8.9119 - val_accuracy: 0.1489\n",
            "Epoch 69/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.6429 - accuracy: 0.2740 - val_loss: 8.9631 - val_accuracy: 0.1493\n",
            "Epoch 70/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.6199 - accuracy: 0.2780 - val_loss: 8.9925 - val_accuracy: 0.1493\n",
            "Epoch 71/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.5855 - accuracy: 0.2815 - val_loss: 9.0471 - val_accuracy: 0.1483\n",
            "Epoch 72/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.5624 - accuracy: 0.2848 - val_loss: 9.1020 - val_accuracy: 0.1511\n",
            "Epoch 73/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.5379 - accuracy: 0.2855 - val_loss: 9.1270 - val_accuracy: 0.1484\n",
            "Epoch 74/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.5100 - accuracy: 0.2896 - val_loss: 9.2085 - val_accuracy: 0.1497\n",
            "Epoch 75/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.4859 - accuracy: 0.2914 - val_loss: 9.2706 - val_accuracy: 0.1510\n",
            "Epoch 76/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.4654 - accuracy: 0.2946 - val_loss: 9.2910 - val_accuracy: 0.1508\n",
            "Epoch 77/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.4416 - accuracy: 0.2977 - val_loss: 9.3892 - val_accuracy: 0.1506\n",
            "Epoch 78/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.4219 - accuracy: 0.2995 - val_loss: 9.4155 - val_accuracy: 0.1502\n",
            "Epoch 79/500\n",
            "49/49 [==============================] - 6s 129ms/step - loss: 3.3992 - accuracy: 0.3025 - val_loss: 9.4585 - val_accuracy: 0.1518\n",
            "Epoch 80/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.3699 - accuracy: 0.3065 - val_loss: 9.4911 - val_accuracy: 0.1510\n",
            "Epoch 81/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.3488 - accuracy: 0.3098 - val_loss: 9.5238 - val_accuracy: 0.1512\n",
            "Epoch 82/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.3309 - accuracy: 0.3110 - val_loss: 9.5873 - val_accuracy: 0.1497\n",
            "Epoch 83/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 3.3098 - accuracy: 0.3138 - val_loss: 9.6586 - val_accuracy: 0.1516\n",
            "Epoch 84/500\n",
            "49/49 [==============================] - 6s 130ms/step - loss: 3.2882 - accuracy: 0.3169 - val_loss: 9.6822 - val_accuracy: 0.1527\n",
            "Epoch 85/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 3.2664 - accuracy: 0.3186 - val_loss: 9.7109 - val_accuracy: 0.1508\n",
            "Epoch 86/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.2483 - accuracy: 0.3225 - val_loss: 9.7712 - val_accuracy: 0.1501\n",
            "Epoch 87/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.2283 - accuracy: 0.3238 - val_loss: 9.8121 - val_accuracy: 0.1509\n",
            "Epoch 88/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.2057 - accuracy: 0.3273 - val_loss: 9.8656 - val_accuracy: 0.1507\n",
            "Epoch 89/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.1827 - accuracy: 0.3294 - val_loss: 9.8903 - val_accuracy: 0.1512\n",
            "Epoch 90/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.1655 - accuracy: 0.3320 - val_loss: 9.9739 - val_accuracy: 0.1516\n",
            "Epoch 91/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.1422 - accuracy: 0.3358 - val_loss: 10.0214 - val_accuracy: 0.1512\n",
            "Epoch 92/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.1262 - accuracy: 0.3375 - val_loss: 10.0333 - val_accuracy: 0.1502\n",
            "Epoch 93/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 3.1070 - accuracy: 0.3405 - val_loss: 10.0894 - val_accuracy: 0.1523\n",
            "Epoch 94/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 3.0912 - accuracy: 0.3428 - val_loss: 10.1493 - val_accuracy: 0.1520\n",
            "Epoch 95/500\n",
            "49/49 [==============================] - 6s 119ms/step - loss: 3.0685 - accuracy: 0.3454 - val_loss: 10.1875 - val_accuracy: 0.1499\n",
            "Epoch 96/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.0520 - accuracy: 0.3463 - val_loss: 10.1856 - val_accuracy: 0.1499\n",
            "Epoch 97/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.0310 - accuracy: 0.3501 - val_loss: 10.2835 - val_accuracy: 0.1523\n",
            "Epoch 98/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 3.0154 - accuracy: 0.3537 - val_loss: 10.3102 - val_accuracy: 0.1497\n",
            "Epoch 99/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.9942 - accuracy: 0.3552 - val_loss: 10.3516 - val_accuracy: 0.1520\n",
            "Epoch 100/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.9731 - accuracy: 0.3575 - val_loss: 10.3936 - val_accuracy: 0.1511\n",
            "Epoch 101/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.9620 - accuracy: 0.3609 - val_loss: 10.4488 - val_accuracy: 0.1526\n",
            "Epoch 102/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.9422 - accuracy: 0.3618 - val_loss: 10.4824 - val_accuracy: 0.1517\n",
            "Epoch 103/500\n",
            "49/49 [==============================] - 6s 116ms/step - loss: 2.9260 - accuracy: 0.3650 - val_loss: 10.5721 - val_accuracy: 0.1521\n",
            "Epoch 104/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.9089 - accuracy: 0.3677 - val_loss: 10.5647 - val_accuracy: 0.1506\n",
            "Epoch 105/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 2.8874 - accuracy: 0.3691 - val_loss: 10.6379 - val_accuracy: 0.1506\n",
            "Epoch 106/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 2.8722 - accuracy: 0.3709 - val_loss: 10.7011 - val_accuracy: 0.1512\n",
            "Epoch 107/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.8548 - accuracy: 0.3756 - val_loss: 10.7605 - val_accuracy: 0.1523\n",
            "Epoch 108/500\n",
            "49/49 [==============================] - 6s 117ms/step - loss: 2.8360 - accuracy: 0.3777 - val_loss: 10.7778 - val_accuracy: 0.1521\n",
            "Epoch 109/500\n",
            "49/49 [==============================] - 6s 118ms/step - loss: 2.8193 - accuracy: 0.3805 - val_loss: 10.8184 - val_accuracy: 0.1502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f40179770f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_8icpaa_4IP",
        "colab_type": "text"
      },
      "source": [
        "## V4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDo87F7AnUdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5c143a1a-05d5-4f4d-d9b8-1fa9eff074be"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(64))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaybHfIeq1ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-4.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y35orGI_7Mh",
        "colab_type": "text"
      },
      "source": [
        "## V5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNmL-cAkspbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8742a989-9716-40c0-9c66-cf78e8b8eb45"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7xulino73AY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e67c459-9d8e-472b-be3a-542a24f761d2"
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-5.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "65/65 [==============================] - 16s 250ms/step - loss: 7.5981 - accuracy: 0.0612 - val_loss: 7.1419 - val_accuracy: 0.0641\n",
            "Epoch 2/500\n",
            "65/65 [==============================] - 15s 234ms/step - loss: 7.0865 - accuracy: 0.0646 - val_loss: 7.1518 - val_accuracy: 0.0641\n",
            "Epoch 3/500\n",
            "65/65 [==============================] - 16s 242ms/step - loss: 7.0693 - accuracy: 0.0646 - val_loss: 7.1983 - val_accuracy: 0.0641\n",
            "Epoch 4/500\n",
            "65/65 [==============================] - 15s 232ms/step - loss: 7.0584 - accuracy: 0.0646 - val_loss: 7.1747 - val_accuracy: 0.0641\n",
            "Epoch 5/500\n",
            "65/65 [==============================] - 15s 231ms/step - loss: 6.9775 - accuracy: 0.0647 - val_loss: 7.0458 - val_accuracy: 0.0641\n",
            "Epoch 6/500\n",
            "65/65 [==============================] - 16s 243ms/step - loss: 6.8599 - accuracy: 0.0705 - val_loss: 6.9444 - val_accuracy: 0.0735\n",
            "Epoch 7/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 6.7380 - accuracy: 0.0758 - val_loss: 6.8965 - val_accuracy: 0.0832\n",
            "Epoch 8/500\n",
            "65/65 [==============================] - 16s 241ms/step - loss: 6.6403 - accuracy: 0.0833 - val_loss: 6.8333 - val_accuracy: 0.0872\n",
            "Epoch 9/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 6.5499 - accuracy: 0.0879 - val_loss: 6.7922 - val_accuracy: 0.0907\n",
            "Epoch 10/500\n",
            "65/65 [==============================] - 16s 249ms/step - loss: 6.4789 - accuracy: 0.0910 - val_loss: 6.7785 - val_accuracy: 0.0916\n",
            "Epoch 11/500\n",
            "65/65 [==============================] - 16s 246ms/step - loss: 6.4193 - accuracy: 0.0939 - val_loss: 6.7645 - val_accuracy: 0.0946\n",
            "Epoch 12/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 6.3552 - accuracy: 0.0981 - val_loss: 6.7403 - val_accuracy: 0.0984\n",
            "Epoch 13/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 6.2877 - accuracy: 0.1027 - val_loss: 6.7187 - val_accuracy: 0.1012\n",
            "Epoch 14/500\n",
            "65/65 [==============================] - 16s 243ms/step - loss: 6.2192 - accuracy: 0.1073 - val_loss: 6.6978 - val_accuracy: 0.1055\n",
            "Epoch 15/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 6.1501 - accuracy: 0.1115 - val_loss: 6.6852 - val_accuracy: 0.1074\n",
            "Epoch 16/500\n",
            "65/65 [==============================] - 16s 243ms/step - loss: 6.0914 - accuracy: 0.1138 - val_loss: 6.6823 - val_accuracy: 0.1102\n",
            "Epoch 17/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 6.0348 - accuracy: 0.1168 - val_loss: 6.6814 - val_accuracy: 0.1113\n",
            "Epoch 18/500\n",
            "65/65 [==============================] - 16s 243ms/step - loss: 5.9836 - accuracy: 0.1185 - val_loss: 6.6734 - val_accuracy: 0.1135\n",
            "Epoch 19/500\n",
            "65/65 [==============================] - 16s 242ms/step - loss: 5.9340 - accuracy: 0.1210 - val_loss: 6.6716 - val_accuracy: 0.1138\n",
            "Epoch 20/500\n",
            "65/65 [==============================] - 16s 246ms/step - loss: 5.8855 - accuracy: 0.1232 - val_loss: 6.6674 - val_accuracy: 0.1149\n",
            "Epoch 21/500\n",
            "65/65 [==============================] - 16s 251ms/step - loss: 5.8367 - accuracy: 0.1252 - val_loss: 6.6745 - val_accuracy: 0.1161\n",
            "Epoch 22/500\n",
            "65/65 [==============================] - 16s 242ms/step - loss: 5.7918 - accuracy: 0.1271 - val_loss: 6.6678 - val_accuracy: 0.1171\n",
            "Epoch 23/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 5.7441 - accuracy: 0.1292 - val_loss: 6.6771 - val_accuracy: 0.1193\n",
            "Epoch 24/500\n",
            "65/65 [==============================] - 16s 245ms/step - loss: 5.6997 - accuracy: 0.1303 - val_loss: 6.6818 - val_accuracy: 0.1203\n",
            "Epoch 25/500\n",
            "65/65 [==============================] - 16s 240ms/step - loss: 5.6548 - accuracy: 0.1323 - val_loss: 6.6873 - val_accuracy: 0.1215\n",
            "Epoch 26/500\n",
            "65/65 [==============================] - 16s 243ms/step - loss: 5.6120 - accuracy: 0.1342 - val_loss: 6.6886 - val_accuracy: 0.1234\n",
            "Epoch 27/500\n",
            "65/65 [==============================] - 16s 243ms/step - loss: 5.5704 - accuracy: 0.1354 - val_loss: 6.6946 - val_accuracy: 0.1238\n",
            "Epoch 28/500\n",
            "65/65 [==============================] - 16s 247ms/step - loss: 5.5239 - accuracy: 0.1373 - val_loss: 6.7038 - val_accuracy: 0.1253\n",
            "Epoch 29/500\n",
            "65/65 [==============================] - 16s 241ms/step - loss: 5.4848 - accuracy: 0.1393 - val_loss: 6.7164 - val_accuracy: 0.1270\n",
            "Epoch 30/500\n",
            "65/65 [==============================] - 16s 243ms/step - loss: 5.4391 - accuracy: 0.1412 - val_loss: 6.7206 - val_accuracy: 0.1281\n",
            "Epoch 31/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 5.3997 - accuracy: 0.1415 - val_loss: 6.7178 - val_accuracy: 0.1293\n",
            "Epoch 32/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 5.3585 - accuracy: 0.1430 - val_loss: 6.7363 - val_accuracy: 0.1288\n",
            "Epoch 33/500\n",
            "65/65 [==============================] - 16s 249ms/step - loss: 5.3192 - accuracy: 0.1455 - val_loss: 6.7428 - val_accuracy: 0.1319\n",
            "Epoch 34/500\n",
            "65/65 [==============================] - 15s 232ms/step - loss: 5.2784 - accuracy: 0.1470 - val_loss: 6.7595 - val_accuracy: 0.1303\n",
            "Epoch 35/500\n",
            "65/65 [==============================] - 16s 245ms/step - loss: 5.2399 - accuracy: 0.1481 - val_loss: 6.7583 - val_accuracy: 0.1323\n",
            "Epoch 36/500\n",
            "65/65 [==============================] - 16s 245ms/step - loss: 5.1992 - accuracy: 0.1505 - val_loss: 6.7842 - val_accuracy: 0.1342\n",
            "Epoch 37/500\n",
            "65/65 [==============================] - 16s 242ms/step - loss: 5.1626 - accuracy: 0.1510 - val_loss: 6.7867 - val_accuracy: 0.1353\n",
            "Epoch 38/500\n",
            "65/65 [==============================] - 15s 236ms/step - loss: 5.1284 - accuracy: 0.1529 - val_loss: 6.8100 - val_accuracy: 0.1347\n",
            "Epoch 39/500\n",
            "65/65 [==============================] - 15s 232ms/step - loss: 5.0902 - accuracy: 0.1534 - val_loss: 6.8230 - val_accuracy: 0.1348\n",
            "Epoch 40/500\n",
            "65/65 [==============================] - 16s 245ms/step - loss: 5.0526 - accuracy: 0.1557 - val_loss: 6.8434 - val_accuracy: 0.1360\n",
            "Epoch 41/500\n",
            "65/65 [==============================] - 16s 253ms/step - loss: 5.0237 - accuracy: 0.1564 - val_loss: 6.8617 - val_accuracy: 0.1362\n",
            "Epoch 42/500\n",
            "65/65 [==============================] - 15s 232ms/step - loss: 4.9909 - accuracy: 0.1580 - val_loss: 6.8932 - val_accuracy: 0.1354\n",
            "Epoch 43/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 4.9566 - accuracy: 0.1587 - val_loss: 6.8953 - val_accuracy: 0.1368\n",
            "Epoch 44/500\n",
            "65/65 [==============================] - 16s 241ms/step - loss: 4.9269 - accuracy: 0.1598 - val_loss: 6.9143 - val_accuracy: 0.1374\n",
            "Epoch 45/500\n",
            "65/65 [==============================] - 16s 240ms/step - loss: 4.8944 - accuracy: 0.1616 - val_loss: 6.9396 - val_accuracy: 0.1390\n",
            "Epoch 46/500\n",
            "65/65 [==============================] - 16s 240ms/step - loss: 4.8614 - accuracy: 0.1623 - val_loss: 6.9573 - val_accuracy: 0.1403\n",
            "Epoch 47/500\n",
            "65/65 [==============================] - 15s 233ms/step - loss: 4.8300 - accuracy: 0.1642 - val_loss: 6.9909 - val_accuracy: 0.1398\n",
            "Epoch 48/500\n",
            "65/65 [==============================] - 15s 231ms/step - loss: 4.8039 - accuracy: 0.1660 - val_loss: 7.0060 - val_accuracy: 0.1388\n",
            "Epoch 49/500\n",
            "65/65 [==============================] - 16s 242ms/step - loss: 4.7754 - accuracy: 0.1665 - val_loss: 7.0340 - val_accuracy: 0.1407\n",
            "Epoch 50/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 4.7457 - accuracy: 0.1680 - val_loss: 7.0474 - val_accuracy: 0.1405\n",
            "Epoch 51/500\n",
            "65/65 [==============================] - 16s 243ms/step - loss: 4.7180 - accuracy: 0.1706 - val_loss: 7.0634 - val_accuracy: 0.1421\n",
            "Epoch 52/500\n",
            "65/65 [==============================] - 15s 234ms/step - loss: 4.6890 - accuracy: 0.1711 - val_loss: 7.0996 - val_accuracy: 0.1418\n",
            "Epoch 53/500\n",
            "65/65 [==============================] - 15s 233ms/step - loss: 4.6617 - accuracy: 0.1735 - val_loss: 7.1054 - val_accuracy: 0.1420\n",
            "Epoch 54/500\n",
            "65/65 [==============================] - 15s 233ms/step - loss: 4.6410 - accuracy: 0.1742 - val_loss: 7.1322 - val_accuracy: 0.1414\n",
            "Epoch 55/500\n",
            "65/65 [==============================] - 16s 249ms/step - loss: 4.6128 - accuracy: 0.1745 - val_loss: 7.1437 - val_accuracy: 0.1424\n",
            "Epoch 56/500\n",
            "65/65 [==============================] - 16s 239ms/step - loss: 4.5890 - accuracy: 0.1768 - val_loss: 7.1728 - val_accuracy: 0.1417\n",
            "Epoch 57/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 4.5639 - accuracy: 0.1778 - val_loss: 7.2087 - val_accuracy: 0.1414\n",
            "Epoch 58/500\n",
            "65/65 [==============================] - 16s 244ms/step - loss: 4.5373 - accuracy: 0.1795 - val_loss: 7.2248 - val_accuracy: 0.1439\n",
            "Epoch 59/500\n",
            "65/65 [==============================] - 15s 234ms/step - loss: 4.5150 - accuracy: 0.1801 - val_loss: 7.2459 - val_accuracy: 0.1432\n",
            "Epoch 60/500\n",
            "65/65 [==============================] - 15s 232ms/step - loss: 4.4915 - accuracy: 0.1826 - val_loss: 7.2678 - val_accuracy: 0.1435\n",
            "Epoch 61/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 4.4667 - accuracy: 0.1836 - val_loss: 7.2881 - val_accuracy: 0.1430\n",
            "Epoch 62/500\n",
            "65/65 [==============================] - 15s 236ms/step - loss: 4.4443 - accuracy: 0.1854 - val_loss: 7.3248 - val_accuracy: 0.1425\n",
            "Epoch 63/500\n",
            "65/65 [==============================] - 16s 242ms/step - loss: 4.4213 - accuracy: 0.1870 - val_loss: 7.3352 - val_accuracy: 0.1441\n",
            "Epoch 64/500\n",
            "65/65 [==============================] - 15s 230ms/step - loss: 4.3977 - accuracy: 0.1887 - val_loss: 7.3510 - val_accuracy: 0.1435\n",
            "Epoch 65/500\n",
            "65/65 [==============================] - 15s 228ms/step - loss: 4.3777 - accuracy: 0.1897 - val_loss: 7.3850 - val_accuracy: 0.1426\n",
            "Epoch 66/500\n",
            "65/65 [==============================] - 16s 241ms/step - loss: 4.3559 - accuracy: 0.1910 - val_loss: 7.4160 - val_accuracy: 0.1444\n",
            "Epoch 67/500\n",
            "65/65 [==============================] - 15s 233ms/step - loss: 4.3374 - accuracy: 0.1927 - val_loss: 7.4296 - val_accuracy: 0.1433\n",
            "Epoch 68/500\n",
            "65/65 [==============================] - 15s 230ms/step - loss: 4.3182 - accuracy: 0.1941 - val_loss: 7.4486 - val_accuracy: 0.1441\n",
            "Epoch 69/500\n",
            "65/65 [==============================] - 15s 234ms/step - loss: 4.2908 - accuracy: 0.1959 - val_loss: 7.4812 - val_accuracy: 0.1433\n",
            "Epoch 70/500\n",
            "65/65 [==============================] - 15s 233ms/step - loss: 4.2752 - accuracy: 0.1971 - val_loss: 7.5070 - val_accuracy: 0.1432\n",
            "Epoch 71/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 4.2512 - accuracy: 0.1991 - val_loss: 7.5179 - val_accuracy: 0.1439\n",
            "Epoch 72/500\n",
            "65/65 [==============================] - 15s 237ms/step - loss: 4.2394 - accuracy: 0.1993 - val_loss: 7.5543 - val_accuracy: 0.1425\n",
            "Epoch 73/500\n",
            "65/65 [==============================] - 16s 248ms/step - loss: 4.2163 - accuracy: 0.2013 - val_loss: 7.5573 - val_accuracy: 0.1446\n",
            "Epoch 74/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 4.1990 - accuracy: 0.2025 - val_loss: 7.5939 - val_accuracy: 0.1445\n",
            "Epoch 75/500\n",
            "65/65 [==============================] - 15s 233ms/step - loss: 4.1803 - accuracy: 0.2047 - val_loss: 7.6186 - val_accuracy: 0.1423\n",
            "Epoch 76/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 4.1601 - accuracy: 0.2049 - val_loss: 7.6405 - val_accuracy: 0.1417\n",
            "Epoch 77/500\n",
            "65/65 [==============================] - 16s 247ms/step - loss: 4.1416 - accuracy: 0.2070 - val_loss: 7.6763 - val_accuracy: 0.1447\n",
            "Epoch 78/500\n",
            "65/65 [==============================] - 15s 231ms/step - loss: 4.1300 - accuracy: 0.2076 - val_loss: 7.6816 - val_accuracy: 0.1436\n",
            "Epoch 79/500\n",
            "65/65 [==============================] - 15s 230ms/step - loss: 4.1109 - accuracy: 0.2104 - val_loss: 7.7190 - val_accuracy: 0.1443\n",
            "Epoch 80/500\n",
            "65/65 [==============================] - 15s 233ms/step - loss: 4.0931 - accuracy: 0.2107 - val_loss: 7.7413 - val_accuracy: 0.1440\n",
            "Epoch 81/500\n",
            "65/65 [==============================] - 15s 227ms/step - loss: 4.0740 - accuracy: 0.2136 - val_loss: 7.7554 - val_accuracy: 0.1440\n",
            "Epoch 82/500\n",
            "65/65 [==============================] - 15s 227ms/step - loss: 4.0640 - accuracy: 0.2142 - val_loss: 7.7783 - val_accuracy: 0.1427\n",
            "Epoch 83/500\n",
            "65/65 [==============================] - 15s 238ms/step - loss: 4.0434 - accuracy: 0.2159 - val_loss: 7.8102 - val_accuracy: 0.1448\n",
            "Epoch 84/500\n",
            "65/65 [==============================] - 16s 239ms/step - loss: 4.0298 - accuracy: 0.2160 - val_loss: 7.8039 - val_accuracy: 0.1453\n",
            "Epoch 85/500\n",
            "65/65 [==============================] - 15s 230ms/step - loss: 4.0086 - accuracy: 0.2193 - val_loss: 7.8531 - val_accuracy: 0.1437\n",
            "Epoch 86/500\n",
            "65/65 [==============================] - 15s 228ms/step - loss: 3.9966 - accuracy: 0.2210 - val_loss: 7.8744 - val_accuracy: 0.1437\n",
            "Epoch 87/500\n",
            "65/65 [==============================] - 15s 231ms/step - loss: 3.9857 - accuracy: 0.2204 - val_loss: 7.8921 - val_accuracy: 0.1453\n",
            "Epoch 88/500\n",
            "65/65 [==============================] - 15s 228ms/step - loss: 3.9609 - accuracy: 0.2226 - val_loss: 7.9025 - val_accuracy: 0.1426\n",
            "Epoch 89/500\n",
            "65/65 [==============================] - 15s 229ms/step - loss: 3.9524 - accuracy: 0.2243 - val_loss: 7.9520 - val_accuracy: 0.1431\n",
            "Epoch 90/500\n",
            "65/65 [==============================] - 15s 229ms/step - loss: 3.9338 - accuracy: 0.2253 - val_loss: 7.9851 - val_accuracy: 0.1413\n",
            "Epoch 91/500\n",
            "65/65 [==============================] - 15s 229ms/step - loss: 3.9211 - accuracy: 0.2270 - val_loss: 7.9962 - val_accuracy: 0.1433\n",
            "Epoch 92/500\n",
            "65/65 [==============================] - 15s 231ms/step - loss: 3.9055 - accuracy: 0.2278 - val_loss: 7.9936 - val_accuracy: 0.1437\n",
            "Epoch 93/500\n",
            "65/65 [==============================] - 15s 231ms/step - loss: 3.8962 - accuracy: 0.2288 - val_loss: 8.0466 - val_accuracy: 0.1431\n",
            "Epoch 94/500\n",
            "65/65 [==============================] - 15s 232ms/step - loss: 3.8773 - accuracy: 0.2301 - val_loss: 8.0779 - val_accuracy: 0.1420\n",
            "Epoch 95/500\n",
            "65/65 [==============================] - 15s 234ms/step - loss: 3.8665 - accuracy: 0.2324 - val_loss: 8.0548 - val_accuracy: 0.1436\n",
            "Epoch 96/500\n",
            "65/65 [==============================] - 15s 236ms/step - loss: 3.8529 - accuracy: 0.2336 - val_loss: 8.0946 - val_accuracy: 0.1424\n",
            "Epoch 97/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 3.8342 - accuracy: 0.2355 - val_loss: 8.1207 - val_accuracy: 0.1408\n",
            "Epoch 98/500\n",
            "65/65 [==============================] - 15s 236ms/step - loss: 3.8264 - accuracy: 0.2363 - val_loss: 8.1502 - val_accuracy: 0.1435\n",
            "Epoch 99/500\n",
            "65/65 [==============================] - 15s 237ms/step - loss: 3.8109 - accuracy: 0.2378 - val_loss: 8.1630 - val_accuracy: 0.1424\n",
            "Epoch 100/500\n",
            "65/65 [==============================] - 16s 240ms/step - loss: 3.7948 - accuracy: 0.2389 - val_loss: 8.2124 - val_accuracy: 0.1414\n",
            "Epoch 101/500\n",
            "65/65 [==============================] - 16s 241ms/step - loss: 3.7833 - accuracy: 0.2409 - val_loss: 8.2232 - val_accuracy: 0.1459\n",
            "Epoch 102/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 3.7709 - accuracy: 0.2409 - val_loss: 8.2409 - val_accuracy: 0.1448\n",
            "Epoch 103/500\n",
            "65/65 [==============================] - 15s 238ms/step - loss: 3.7554 - accuracy: 0.2440 - val_loss: 8.2814 - val_accuracy: 0.1414\n",
            "Epoch 104/500\n",
            "65/65 [==============================] - 15s 234ms/step - loss: 3.7477 - accuracy: 0.2437 - val_loss: 8.2777 - val_accuracy: 0.1440\n",
            "Epoch 105/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 3.7327 - accuracy: 0.2449 - val_loss: 8.3000 - val_accuracy: 0.1420\n",
            "Epoch 106/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 3.7227 - accuracy: 0.2457 - val_loss: 8.3083 - val_accuracy: 0.1412\n",
            "Epoch 107/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 3.7087 - accuracy: 0.2489 - val_loss: 8.3542 - val_accuracy: 0.1422\n",
            "Epoch 108/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 3.6917 - accuracy: 0.2499 - val_loss: 8.3783 - val_accuracy: 0.1424\n",
            "Epoch 109/500\n",
            "65/65 [==============================] - 15s 232ms/step - loss: 3.6815 - accuracy: 0.2521 - val_loss: 8.3969 - val_accuracy: 0.1435\n",
            "Epoch 110/500\n",
            "65/65 [==============================] - 15s 234ms/step - loss: 3.6712 - accuracy: 0.2520 - val_loss: 8.4333 - val_accuracy: 0.1434\n",
            "Epoch 111/500\n",
            "65/65 [==============================] - 16s 239ms/step - loss: 3.6575 - accuracy: 0.2535 - val_loss: 8.4440 - val_accuracy: 0.1435\n",
            "Epoch 112/500\n",
            "65/65 [==============================] - 15s 236ms/step - loss: 3.6495 - accuracy: 0.2553 - val_loss: 8.4692 - val_accuracy: 0.1436\n",
            "Epoch 113/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 3.6337 - accuracy: 0.2561 - val_loss: 8.4923 - val_accuracy: 0.1430\n",
            "Epoch 114/500\n",
            "65/65 [==============================] - 15s 238ms/step - loss: 3.6237 - accuracy: 0.2572 - val_loss: 8.4857 - val_accuracy: 0.1441\n",
            "Epoch 115/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 3.6111 - accuracy: 0.2578 - val_loss: 8.5383 - val_accuracy: 0.1424\n",
            "Epoch 116/500\n",
            "65/65 [==============================] - 15s 235ms/step - loss: 3.6013 - accuracy: 0.2604 - val_loss: 8.5487 - val_accuracy: 0.1414\n",
            "Epoch 117/500\n",
            "65/65 [==============================] - 15s 232ms/step - loss: 3.5912 - accuracy: 0.2601 - val_loss: 8.5876 - val_accuracy: 0.1421\n",
            "Epoch 118/500\n",
            "65/65 [==============================] - 15s 237ms/step - loss: 3.5778 - accuracy: 0.2635 - val_loss: 8.5769 - val_accuracy: 0.1433\n",
            "Epoch 119/500\n",
            "65/65 [==============================] - 15s 229ms/step - loss: 3.5672 - accuracy: 0.2639 - val_loss: 8.5989 - val_accuracy: 0.1428\n",
            "Epoch 120/500\n",
            "65/65 [==============================] - 15s 233ms/step - loss: 3.5531 - accuracy: 0.2657 - val_loss: 8.6417 - val_accuracy: 0.1429\n",
            "Epoch 121/500\n",
            "65/65 [==============================] - 15s 231ms/step - loss: 3.5436 - accuracy: 0.2649 - val_loss: 8.6738 - val_accuracy: 0.1421\n",
            "Epoch 122/500\n",
            "65/65 [==============================] - 15s 232ms/step - loss: 3.5321 - accuracy: 0.2673 - val_loss: 8.6739 - val_accuracy: 0.1424\n",
            "Epoch 123/500\n",
            "65/65 [==============================] - 15s 236ms/step - loss: 3.5156 - accuracy: 0.2708 - val_loss: 8.7016 - val_accuracy: 0.1424\n",
            "Epoch 124/500\n",
            "65/65 [==============================] - 15s 238ms/step - loss: 3.5060 - accuracy: 0.2713 - val_loss: 8.7229 - val_accuracy: 0.1419\n",
            "Epoch 125/500\n",
            "65/65 [==============================] - 15s 233ms/step - loss: 3.4965 - accuracy: 0.2721 - val_loss: 8.7547 - val_accuracy: 0.1425\n",
            "Epoch 126/500\n",
            "65/65 [==============================] - 15s 234ms/step - loss: 3.4855 - accuracy: 0.2717 - val_loss: 8.7431 - val_accuracy: 0.1440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8f207c59e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JOIgTU_-O-",
        "colab_type": "text"
      },
      "source": [
        "## V6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf8Xngj_75dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(    \n",
        "    Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=embedding_matrix.shape[1],\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True)\n",
        ")\n",
        "\n",
        "model.add(LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXlm2KmaA17M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-6.h5')\n",
        "train_model(model_filepath, X_train, y_train, X_test, y_test, use_pretrained_model=False, model=model, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD93EE9F7qMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "4077e2eb-12cb-4c30-9910-e601ccec4b56"
      },
      "source": [
        "train_utils.plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"dc2c2089-688c-4027-84aa-30c456c68a31\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"dc2c2089-688c-4027-84aa-30c456c68a31\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'dc2c2089-688c-4027-84aa-30c456c68a31',\n",
              "                        [{\"line\": {\"color\": \"royalblue\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"train_loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [7.458602428436279, 7.021402359008789, 7.009311676025391, 6.955985069274902, 6.8343939781188965, 6.712810039520264, 6.608242511749268, 6.514761447906494, 6.441170692443848, 6.371826171875, 6.300393581390381, 6.232043743133545, 6.170290470123291, 6.113407135009766, 6.059351444244385, 6.004700183868408, 5.949937343597412, 5.893865585327148, 5.840144634246826, 5.785127639770508, 5.73313570022583, 5.686136245727539, 5.643167018890381, 5.59910249710083, 5.556097507476807, 5.515835762023926, 5.475383281707764, 5.43580961227417, 5.395748615264893, 5.358993053436279, 5.318986415863037, 5.285747528076172, 5.251445293426514, 5.212973117828369, 5.178022384643555, 5.146390438079834, 5.111725807189941, 5.081413269042969, 5.049098491668701, 5.016397953033447, 4.985947608947754, 4.956323623657227, 4.93022346496582, 4.901480197906494, 4.873708724975586, 4.841719627380371, 4.813971996307373, 4.785449981689453, 4.76080846786499, 4.736237525939941, 4.708621501922607, 4.686616897583008, 4.657742977142334, 4.632593631744385, 4.607674598693848, 4.585359573364258, 4.560201168060303, 4.540639877319336, 4.51220703125, 4.492542266845703, 4.4723005294799805, 4.4511494636535645, 4.42878532409668, 4.413455963134766, 4.391733646392822, 4.369042873382568, 4.3505353927612305, 4.326739311218262, 4.311563968658447, 4.292220115661621, 4.269426345825195, 4.256741523742676, 4.238198757171631, 4.219151973724365, 4.20266056060791, 4.18494176864624, 4.169332027435303, 4.149455547332764, 4.137803554534912, 4.116767406463623, 4.100502967834473, 4.083449840545654, 4.0702009201049805, 4.059648036956787, 4.039373874664307, 4.027568340301514, 4.011825084686279, 3.9942076206207275, 3.9786579608917236, 3.9630703926086426, 3.953157663345337, 3.94500732421875, 3.930150032043457, 3.9062442779541016, 3.8975884914398193, 3.8874988555908203, 3.8704326152801514, 3.8597898483276367, 3.844069242477417, 3.8322761058807373, 3.821240186691284, 3.8114266395568848, 3.7916510105133057, 3.784296989440918, 3.768200397491455, 3.7584316730499268, 3.750084161758423, 3.733917236328125, 3.724353075027466, 3.716841220855713, 3.7006072998046875, 3.686579465866089, 3.67802095413208, 3.669113874435425, 3.6539947986602783, 3.643953323364258, 3.639620542526245, 3.631702423095703, 3.616957664489746, 3.609309196472168, 3.5965981483459473, 3.5904407501220703, 3.5754475593566895, 3.564770221710205, 3.5559475421905518, 3.5410876274108887, 3.54119610786438, 3.526963472366333, 3.5164248943328857, 3.504305601119995, 3.5012247562408447, 3.4923555850982666, 3.47900652885437, 3.474431037902832, 3.457871198654175, 3.4513890743255615, 3.4426355361938477, 3.437422752380371, 3.4223594665527344, 3.420081615447998, 3.404766321182251, 3.398977518081665, 3.389678716659546, 3.3810465335845947, 3.3723390102386475, 3.3669545650482178, 3.3583598136901855, 3.3477187156677246, 3.338785409927368, 3.33193302154541, 3.3255183696746826, 3.3179781436920166, 3.3103854656219482, 3.30106520652771, 3.2952380180358887, 3.2856810092926025, 3.2768495082855225, 3.268112897872925, 3.2588319778442383, 3.2528700828552246, 3.250032663345337, 3.2422804832458496, 3.231519937515259, 3.2216625213623047, 3.219912528991699, 3.2107439041137695, 3.1988728046417236, 3.204078197479248, 3.1879794597625732, 3.179272413253784, 3.1763107776641846, 3.1669631004333496, 3.1663520336151123, 3.156172275543213, 3.1521947383880615, 3.1415719985961914, 3.1325693130493164, 3.128201484680176, 3.1239748001098633, 3.1095027923583984, 3.107022762298584, 3.109335422515869, 3.0960733890533447, 3.0828840732574463, 3.0855002403259277, 3.0722806453704834, 3.0718817710876465, 3.060788631439209, 3.0544981956481934, 3.0475826263427734, 3.041982889175415, 3.0337958335876465, 3.0354974269866943, 3.026292562484741, 3.0212693214416504, 3.012688398361206, 3.0056071281433105, 2.999680995941162, 2.9940104484558105, 2.9915013313293457, 2.9830503463745117, 2.9763174057006836, 2.9720370769500732, 2.9659273624420166, 2.957529067993164, 2.9552230834960938, 2.952772617340088, 2.943516492843628, 2.943812370300293, 2.927532434463501, 2.9208385944366455, 2.9204330444335938, 2.920297622680664, 2.9069883823394775, 2.9014029502868652, 2.899669885635376, 2.887300729751587, 2.887101411819458, 2.878067970275879, 2.8732335567474365, 2.872183322906494, 2.869094133377075, 2.8626792430877686, 2.8570947647094727, 2.8543055057525635, 2.8468871116638184, 2.835191249847412, 2.8365352153778076, 2.8314614295959473, 2.8236448764801025, 2.818066120147705, 2.8167624473571777, 2.8123903274536133, 2.8090639114379883, 2.8043978214263916, 2.798945665359497, 2.796839714050293, 2.7929646968841553, 2.7829573154449463, 2.778717517852783, 2.7744243144989014, 2.7651422023773193, 2.7624804973602295, 2.75708270072937, 2.7479960918426514, 2.7508907318115234, 2.742372751235962, 2.739006280899048, 2.7318272590637207, 2.73349928855896, 2.7258694171905518, 2.720759868621826, 2.7181174755096436, 2.71685528755188, 2.7080912590026855, 2.6990468502044678, 2.6948909759521484, 2.6980807781219482, 2.6961491107940674, 2.685594081878662, 2.6833837032318115, 2.67678165435791, 2.669595956802368, 2.6717209815979004, 2.666579008102417, 2.6554994583129883, 2.6524298191070557, 2.6517951488494873, 2.6420364379882812, 2.6392295360565186, 2.6405091285705566, 2.6317214965820312, 2.6298704147338867, 2.6316287517547607, 2.6189308166503906, 2.6266093254089355, 2.6110169887542725, 2.613149881362915, 2.6061320304870605, 2.6067609786987305, 2.5941669940948486, 2.596419095993042, 2.593614339828491, 2.584463596343994, 2.580822229385376, 2.580596446990967, 2.5706098079681396, 2.5698089599609375, 2.5620675086975098, 2.563743829727173, 2.5594749450683594, 2.553781270980835, 2.5495808124542236, 2.5503041744232178, 2.5479135513305664, 2.541914701461792, 2.540370225906372, 2.5333597660064697, 2.5280609130859375, 2.527148723602295, 2.5240111351013184, 2.5221266746520996, 2.517972469329834, 2.510897636413574, 2.5106399059295654, 2.4961678981781006, 2.50069260597229, 2.4992992877960205, 2.4951581954956055, 2.4896724224090576, 2.4896066188812256, 2.484933614730835, 2.4841721057891846, 2.4836010932922363, 2.480484962463379, 2.4681806564331055, 2.461735725402832, 2.4697868824005127, 2.4554059505462646, 2.4540064334869385, 2.447173595428467, 2.451136350631714, 2.4495129585266113, 2.447692632675171, 2.440633535385132, 2.43415904045105, 2.432251214981079, 2.4319276809692383, 2.4283690452575684, 2.428084135055542, 2.423860788345337, 2.4220213890075684, 2.4219110012054443, 2.41408634185791, 2.407965660095215, 2.404655694961548, 2.408109188079834, 2.4001846313476562, 2.4002151489257812, 2.394381523132324, 2.3932816982269287, 2.3838207721710205, 2.3845086097717285, 2.381629228591919, 2.3772876262664795, 2.380197286605835, 2.3700618743896484, 2.361698865890503, 2.3691210746765137, 2.354217290878296, 2.3582186698913574, 2.3483285903930664, 2.34909725189209, 2.3508801460266113, 2.352296829223633, 2.3456146717071533, 2.343899726867676, 2.3429369926452637, 2.3420512676239014, 2.334465503692627, 2.3293683528900146, 2.332944631576538, 2.3179967403411865, 2.322411298751831, 2.3163552284240723, 2.31400465965271, 2.31575345993042, 2.308544397354126, 2.3045167922973633, 2.304391860961914, 2.3031654357910156, 2.2948250770568848, 2.3007946014404297, 2.2943413257598877, 2.299194097518921, 2.2900660037994385, 2.2869091033935547, 2.280423879623413, 2.2785377502441406, 2.276789665222168, 2.279853582382202, 2.275507688522339, 2.2724356651306152, 2.269101142883301, 2.2687745094299316, 2.2627599239349365, 2.2564404010772705, 2.2533669471740723, 2.256253242492676, 2.2519936561584473, 2.252096652984619, 2.25284481048584, 2.2436461448669434, 2.2367539405822754, 2.241771936416626, 2.2350378036499023, 2.2347187995910645, 2.2325217723846436, 2.234553813934326, 2.228029251098633, 2.220820188522339, 2.2201356887817383, 2.2250750064849854, 2.2170023918151855, 2.214937210083008, 2.2161030769348145, 2.2099266052246094, 2.2059361934661865, 2.2041614055633545, 2.197805643081665, 2.1955502033233643, 2.201632022857666, 2.2009034156799316, 2.1934211254119873, 2.1948812007904053, 2.184741497039795, 2.1895227432250977, 2.1845178604125977, 2.1759610176086426, 2.176597833633423, 2.1796576976776123, 2.1687614917755127, 2.1743369102478027, 2.1618545055389404, 2.1677160263061523, 2.1629109382629395, 2.157944679260254, 2.1609809398651123, 2.1588261127471924, 2.1591691970825195, 2.1612415313720703, 2.148361921310425, 2.1509604454040527, 2.1545166969299316, 2.143122673034668, 2.1410958766937256, 2.141697883605957, 2.1300461292266846, 2.1318600177764893, 2.128767251968384, 2.1309731006622314, 2.1296863555908203, 2.127523422241211, 2.1259565353393555, 2.1223254203796387, 2.115030527114868, 2.119023561477661, 2.116718292236328, 2.1130118370056152, 2.114192247390747, 2.108396530151367, 2.10859751701355, 2.106961965560913, 2.0974416732788086, 2.1060073375701904, 2.0964243412017822, 2.0923948287963867, 2.0999016761779785, 2.093358278274536, 2.0858376026153564, 2.087390422821045, 2.084538698196411, 2.0864334106445312, 2.079694986343384, 2.081512928009033, 2.0752129554748535, 2.08119797706604, 2.0702297687530518, 2.0719404220581055, 2.0688765048980713, 2.0731663703918457, 2.0700695514678955, 2.0657315254211426, 2.058476686477661, 2.0574235916137695, 2.0610194206237793, 2.0544745922088623, 2.052564859390259, 2.058309555053711, 2.048804759979248, 2.053652286529541, 2.041518449783325, 2.0463902950286865, 2.039344310760498, 2.0366008281707764, 2.0353167057037354, 2.038710117340088, 2.0290632247924805, 2.034236431121826, 2.033118486404419, 2.0306506156921387, 2.027523994445801, 2.029937982559204, 2.0225539207458496, 2.024111270904541, 2.0212676525115967, 2.018556594848633, 2.0204620361328125, 2.017644166946411, 2.0152645111083984]}, {\"line\": {\"color\": \"firebrick\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"train_accuracy\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"xaxis\": \"x\", \"y\": [0.0541599839925766, 0.05779228359460831, 0.05779852718114853, 0.0578172504901886, 0.0645139142870903, 0.07083611935377121, 0.07786355912685394, 0.08414207398891449, 0.08962173014879227, 0.09450848400592804, 0.09875865280628204, 0.10367037355899811, 0.10773954540491104, 0.11044817417860031, 0.1136186346411705, 0.11635845899581909, 0.11837432533502579, 0.12124522030353546, 0.12284917384386063, 0.12506474554538727, 0.1273365020751953, 0.12904655933380127, 0.13121220469474792, 0.1330907642841339, 0.13485074043273926, 0.13576817512512207, 0.13764050602912903, 0.13923820853233337, 0.1411791890859604, 0.14272072911262512, 0.14338228106498718, 0.14515474438667297, 0.14648409187793732, 0.1483376920223236, 0.14953598380088806, 0.15018504858016968, 0.15170162916183472, 0.15447266399860382, 0.15454131364822388, 0.155702143907547, 0.15698781609535217, 0.1587165892124176, 0.15924707055091858, 0.16103202104568481, 0.16219286620616913, 0.1639029085636139, 0.16659905016422272, 0.16744160652160645, 0.16827790439128876, 0.16945746541023254, 0.1713859587907791, 0.17171673476696014, 0.17364521324634552, 0.1752803772687912, 0.1763538420200348, 0.1779453158378601, 0.17909367382526398, 0.18139038980007172, 0.18241392076015472, 0.18423007428646088, 0.18559686839580536, 0.18609614670276642, 0.18806833028793335, 0.1887548416852951, 0.19060219824314117, 0.1924121081829071, 0.19377265870571136, 0.19562625885009766, 0.1970929056406021, 0.19883416593074799, 0.19952069222927094, 0.20134307444095612, 0.20247894525527954, 0.20446985960006714, 0.20503778755664825, 0.20734699070453644, 0.20800229907035828, 0.20974980294704437, 0.21161587536334991, 0.21383145451545715, 0.21460534632205963, 0.216696098446846, 0.21679596602916718, 0.21816898882389069, 0.22108358144760132, 0.22125832736492157, 0.2232055366039276, 0.22429148852825165, 0.22633855044841766, 0.22724974155426025, 0.22884121537208557, 0.22905965149402618, 0.23065736889839172, 0.23375917971134186, 0.23399010300636292, 0.2360808551311493, 0.2375599890947342, 0.2381778508424759, 0.24095512926578522, 0.2411423623561859, 0.24287737905979156, 0.2432643324136734, 0.24540501832962036, 0.24626627564430237, 0.2474333643913269, 0.24871277809143066, 0.2504790127277374, 0.2525697648525238, 0.25258225202560425, 0.25417372584342957, 0.25564661622047424, 0.2575688660144806, 0.25863608717918396, 0.260520875453949, 0.2619188725948334, 0.26328566670417786, 0.26449644565582275, 0.2626241147518158, 0.26522040367126465, 0.26653727889060974, 0.2687341272830963, 0.2685968279838562, 0.27151140570640564, 0.27336499094963074, 0.27242258191108704, 0.2752123475074768, 0.27509376406669617, 0.2755680978298187, 0.27858254313468933, 0.27923783659935, 0.2786199748516083, 0.28127244114875793, 0.2823334038257599, 0.2830074429512024, 0.28568485379219055, 0.28665223717689514, 0.28694555163383484, 0.2884184420108795, 0.29057785868644714, 0.29015347361564636, 0.29294323921203613, 0.29165756702423096, 0.29429754614830017, 0.295545756816864, 0.2984790503978729, 0.29732444882392883, 0.29741182923316956, 0.29980841279029846, 0.30133122205734253, 0.30106285214424133, 0.3028790056705475, 0.3036404252052307, 0.30362793803215027, 0.3058185577392578, 0.3073538541793823, 0.3087456226348877, 0.30862703919410706, 0.30980658531188965, 0.3106616139411926, 0.31209081411361694, 0.3127586245536804, 0.3134576082229614, 0.3164907693862915, 0.3174331784248352, 0.3164595663547516, 0.31737077236175537, 0.32045385241508484, 0.31871259212493896, 0.3201417922973633, 0.3216833472251892, 0.3225383758544922, 0.32280048727989197, 0.3240424692630768, 0.32656386494636536, 0.32553407549858093, 0.3271068334579468, 0.3286171555519104, 0.32924750447273254, 0.3300651013851166, 0.33253031969070435, 0.3334040641784668, 0.33323556184768677, 0.33427780866622925, 0.3356570899486542, 0.33639976382255554, 0.33715495467185974, 0.3374670147895813, 0.3392644226551056, 0.34051263332366943, 0.3404689431190491, 0.34261587262153625, 0.3427157402038574, 0.3416547477245331, 0.34538692235946655, 0.3444632291793823, 0.3474963903427124, 0.34747767448425293, 0.3484575152397156, 0.34944984316825867, 0.3494373559951782, 0.35025495290756226, 0.35143449902534485, 0.353737473487854, 0.3531508147716522, 0.3542117774486542, 0.3554038405418396, 0.3552727699279785, 0.35581573843955994, 0.3579813838005066, 0.3592483401298523, 0.3606463372707367, 0.3612954020500183, 0.36030930280685425, 0.36278077960014343, 0.36332374811172485, 0.3632800579071045, 0.36574527621269226, 0.36542072892189026, 0.3663007318973541, 0.3664068281650543, 0.36787348985671997, 0.3679858148097992, 0.3693276643753052, 0.3680731952190399, 0.3709503412246704, 0.3725043535232544, 0.3720799684524536, 0.3717554211616516, 0.37443283200263977, 0.3751380741596222, 0.3767233192920685, 0.37574347853660583, 0.37730371952056885, 0.37679195404052734, 0.37720388174057007, 0.37975022196769714, 0.3793882429599762, 0.38088610768318176, 0.3824900686740875, 0.3819658160209656, 0.38318905234336853, 0.3838880658149719, 0.3844497501850128, 0.3857167065143585, 0.3868962526321411, 0.38655298948287964, 0.388637512922287, 0.3876951038837433, 0.3900729715824127, 0.3896111249923706, 0.388806015253067, 0.39050358533859253, 0.3926505148410797, 0.3917580544948578, 0.3934306502342224, 0.3935367465019226, 0.3959770202636719, 0.3956587016582489, 0.39658862352371216, 0.3974062204360962, 0.397144079208374, 0.3978118896484375, 0.39940959215164185, 0.39943456649780273, 0.39968419075012207, 0.4017874300479889, 0.4022991955280304, 0.4012756645679474, 0.4031604826450348, 0.40432754158973694, 0.4050390422344208, 0.40538230538368225, 0.40599390864372253, 0.4057754874229431, 0.4077601432800293, 0.4071485102176666, 0.40780383348464966, 0.4092455208301544, 0.409401535987854, 0.4097760021686554, 0.4134270250797272, 0.41277173161506653, 0.4110679030418396, 0.4142758250236511, 0.41382020711898804, 0.4131337106227875, 0.4146253168582916, 0.4154990613460541, 0.41732147336006165, 0.41847604513168335, 0.4173027276992798, 0.4189004600048065, 0.4181889593601227, 0.4202422797679901, 0.4192998707294464, 0.4219648241996765, 0.4206417202949524, 0.4223829507827759, 0.42361870408058167, 0.42216452956199646, 0.42401811480522156, 0.4249979853630066, 0.4239557087421417, 0.4256220757961273, 0.42725101113319397, 0.4295476973056793, 0.42767539620399475, 0.4278251826763153, 0.429628849029541, 0.42915451526641846, 0.43006572127342224, 0.43087083101272583, 0.42972245812416077, 0.4296225905418396, 0.43103307485580444, 0.4329615831375122, 0.43375417590141296, 0.43163222074508667, 0.4365564286708832, 0.4353768527507782, 0.4367998242378235, 0.43699952960014343, 0.4363442361354828, 0.4356764256954193, 0.4375612437725067, 0.43915271759033203, 0.4390091598033905, 0.4396519958972931, 0.4407878816127777, 0.4402698576450348, 0.43995779752731323, 0.4418051540851593, 0.44232940673828125, 0.44324684143066406, 0.4441767632961273, 0.4453750550746918, 0.44461989402770996, 0.4444701075553894, 0.4459554851055145, 0.44568711519241333, 0.4464547634124756, 0.44802752137184143, 0.44768425822257996, 0.4482022523880005, 0.4484955966472626, 0.4489574432373047, 0.45026180148124695, 0.4520280361175537, 0.4498249292373657, 0.4541187882423401, 0.45279568433761597, 0.4532824754714966, 0.4544932544231415, 0.45383793115615845, 0.4532325565814972, 0.4546118378639221, 0.45664018392562866, 0.45631563663482666, 0.4550924003124237, 0.4574265480041504, 0.4574640095233917, 0.45537325739860535, 0.4590242803096771, 0.4579695165157318, 0.4593987464904785, 0.46039730310440063, 0.4611150324344635, 0.4620511829853058, 0.46137091517448425, 0.46206989884376526, 0.46265658736228943, 0.4647223651409149, 0.46357402205467224, 0.4628937244415283, 0.46300607919692993, 0.46443527936935425, 0.46357402205467224, 0.46822986006736755, 0.46717509627342224, 0.466731995344162, 0.46645113825798035, 0.46657595038414, 0.4687977731227875, 0.46779295802116394, 0.46949678659439087, 0.46954047679901123, 0.4705016016960144, 0.46989619731903076, 0.4707637131214142, 0.47185590863227844, 0.46979010105133057, 0.47167491912841797, 0.473022997379303, 0.47487035393714905, 0.47252368927001953, 0.4749452471733093, 0.4740215539932251, 0.47412142157554626, 0.47397786378860474, 0.4761996865272522, 0.4772981107234955, 0.477279394865036, 0.4765491783618927, 0.47761017084121704, 0.4777037799358368, 0.47756025195121765, 0.47973838448524475, 0.4799380898475647, 0.4807431995868683, 0.4823783338069916, 0.48210999369621277, 0.4807494282722473, 0.4811488687992096, 0.48229098320007324, 0.4826467037200928, 0.48360782861709595, 0.483245849609375, 0.48328953981399536, 0.48498088121414185, 0.4841570556163788, 0.483938604593277, 0.48630398511886597, 0.48556753993034363, 0.4873899221420288, 0.4865598678588867, 0.48796409368515015, 0.48780182003974915, 0.4879578649997711, 0.488519549369812, 0.48809516429901123, 0.4881700575351715, 0.4912032186985016, 0.48931217193603516, 0.4886007010936737, 0.49066025018692017, 0.4916151165962219, 0.49162137508392334, 0.49328774213790894, 0.4924951195716858, 0.4955906867980957, 0.4936809241771698, 0.493793249130249, 0.494442343711853, 0.49484801292419434, 0.49582159519195557, 0.49737563729286194, 0.4948916733264923, 0.4962584674358368, 0.4972071349620819, 0.49522244930267334, 0.4976939260959625, 0.49799349904060364, 0.4980309307575226, 0.4987673759460449, 0.49928539991378784, 0.5002776980400085, 0.5001591444015503, 0.4989047050476074, 0.5019253492355347, 0.500933051109314, 0.5020189881324768, 0.5015820860862732, 0.5023372769355774, 0.5038164258003235, 0.5035542845726013, 0.5042282938957214, 0.5025744438171387, 0.5066685676574707, 0.5043219327926636, 0.5047088861465454, 0.5030050873756409, 0.5049210786819458, 0.505233108997345, 0.5060444474220276, 0.5073738098144531, 0.507604718208313, 0.5079230070114136, 0.5082350969314575, 0.5075298547744751, 0.5096330642700195, 0.5079042911529541, 0.5109749436378479, 0.5108688473701477, 0.5107439756393433, 0.5106441378593445, 0.5122168660163879, 0.5119172930717468, 0.5130032896995544, 0.5115865468978882, 0.5120670795440674, 0.5143575668334961, 0.5131655335426331, 0.5122855305671692, 0.5125726461410522, 0.513359010219574, 0.5153873562812805, 0.5146009922027588, 0.5140954256057739, 0.5155371427536011, 0.51540607213974], \"yaxis\": \"y2\"}, {\"line\": {\"color\": \"royalblue\", \"dash\": \"dot\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"validation_loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [7.0811052322387695, 7.101848602294922, 7.090435981750488, 7.001238822937012, 6.886741638183594, 6.814002513885498, 6.741083145141602, 6.700465679168701, 6.6743316650390625, 6.641850471496582, 6.620259761810303, 6.593625068664551, 6.589068412780762, 6.578842639923096, 6.558661937713623, 6.549444675445557, 6.537961959838867, 6.5289506912231445, 6.517361640930176, 6.5008158683776855, 6.494589328765869, 6.5000691413879395, 6.493002414703369, 6.504294395446777, 6.5040154457092285, 6.504066467285156, 6.505837440490723, 6.513614177703857, 6.524117946624756, 6.528754711151123, 6.533158779144287, 6.53879976272583, 6.550942420959473, 6.569360256195068, 6.568551540374756, 6.577001571655273, 6.596731662750244, 6.602609157562256, 6.616481304168701, 6.630683898925781, 6.641595363616943, 6.642998218536377, 6.65492057800293, 6.673959255218506, 6.6845855712890625, 6.693271160125732, 6.723233222961426, 6.723042964935303, 6.735398292541504, 6.756357192993164, 6.756956100463867, 6.7870869636535645, 6.791988372802734, 6.813998699188232, 6.819335460662842, 6.841073989868164, 6.853143215179443, 6.858806133270264, 6.877065658569336, 6.890848159790039, 6.906373977661133, 6.9317545890808105, 6.93049430847168, 6.944567680358887, 6.974958896636963, 6.98774528503418, 7.0010857582092285, 7.020697116851807, 7.033700466156006, 7.0398454666137695, 7.05448579788208, 7.063262939453125, 7.078580379486084, 7.105457305908203, 7.130711078643799, 7.128904342651367, 7.146759986877441, 7.168460369110107, 7.1587371826171875, 7.198734283447266, 7.221553802490234, 7.219066143035889, 7.239317893981934, 7.244234085083008, 7.274664878845215, 7.272515773773193, 7.283438205718994, 7.2960357666015625, 7.329097747802734, 7.3549299240112305, 7.370049953460693, 7.3714599609375, 7.3922014236450195, 7.388037204742432, 7.429225921630859, 7.4100661277771, 7.434469223022461, 7.465261936187744, 7.485400199890137, 7.483192443847656, 7.49373722076416, 7.506464004516602, 7.536664962768555, 7.545938014984131, 7.562707424163818, 7.571969032287598, 7.596616268157959, 7.625345706939697, 7.601678371429443, 7.63006591796875, 7.651721477508545, 7.6534743309021, 7.692101001739502, 7.675325393676758, 7.701205730438232, 7.708262920379639, 7.715651988983154, 7.722330570220947, 7.752479076385498, 7.760960578918457, 7.776294708251953, 7.795327186584473, 7.798788547515869, 7.835936546325684, 7.829690933227539, 7.852116584777832, 7.875899314880371, 7.889072418212891, 7.887316703796387, 7.883529186248779, 7.9167962074279785, 7.944774627685547, 7.943746566772461, 7.945460319519043, 7.948983192443848, 7.991881370544434, 7.969258785247803, 8.028912544250488, 8.011883735656738, 8.017423629760742, 8.041860580444336, 8.071242332458496, 8.081646919250488, 8.058266639709473, 8.115095138549805, 8.113784790039062, 8.12731647491455, 8.14806842803955, 8.153298377990723, 8.143675804138184, 8.154038429260254, 8.157585144042969, 8.223134994506836, 8.203485488891602, 8.198715209960938, 8.22368049621582, 8.26600456237793, 8.288798332214355, 8.26154899597168, 8.259553909301758, 8.280402183532715, 8.290225982666016, 8.315468788146973, 8.329297065734863, 8.334005355834961, 8.346994400024414, 8.327754974365234, 8.3567533493042, 8.384794235229492, 8.396097183227539, 8.423657417297363, 8.40749454498291, 8.436782836914062, 8.436640739440918, 8.455733299255371, 8.460367202758789, 8.4896879196167, 8.473045349121094, 8.5108642578125, 8.54369831085205, 8.488730430603027, 8.53065299987793, 8.525118827819824, 8.557458877563477, 8.58045482635498, 8.57884693145752, 8.579798698425293, 8.604087829589844, 8.601811408996582, 8.62378978729248, 8.622810363769531, 8.641777038574219, 8.64533805847168, 8.65832233428955, 8.683515548706055, 8.691615104675293, 8.754294395446777, 8.668593406677246, 8.712483406066895, 8.71349811553955, 8.722867965698242, 8.726900100708008, 8.746829986572266, 8.767716407775879, 8.787223815917969, 8.769930839538574, 8.798505783081055, 8.807829856872559, 8.799479484558105, 8.810611724853516, 8.834456443786621, 8.814477920532227, 8.830406188964844, 8.826130867004395, 8.867487907409668, 8.923562049865723, 8.8707914352417, 8.890682220458984, 8.918107032775879, 8.932426452636719, 8.932161331176758, 8.956990242004395, 8.923675537109375, 8.926551818847656, 8.928370475769043, 8.962406158447266, 9.01309871673584, 8.975807189941406, 9.013347625732422, 9.0341157913208, 8.972091674804688, 9.007434844970703, 9.026371955871582, 9.042062759399414, 9.048633575439453, 9.028782844543457, 9.02031421661377, 9.03646469116211, 9.08127498626709, 9.08030891418457, 9.09367561340332, 9.134988784790039, 9.103424072265625, 9.138724327087402, 9.099093437194824, 9.148544311523438, 9.121636390686035, 9.139052391052246, 9.174615859985352, 9.154459953308105, 9.182940483093262, 9.19161319732666, 9.176226615905762, 9.191561698913574, 9.189057350158691, 9.243419647216797, 9.255827903747559, 9.227087020874023, 9.200124740600586, 9.278923034667969, 9.262886047363281, 9.245074272155762, 9.294384956359863, 9.316097259521484, 9.320127487182617, 9.271126747131348, 9.268879890441895, 9.312228202819824, 9.32699966430664, 9.30872631072998, 9.277027130126953, 9.315185546875, 9.353362083435059, 9.345064163208008, 9.292256355285645, 9.399405479431152, 9.392840385437012, 9.387420654296875, 9.384929656982422, 9.436405181884766, 9.38337230682373, 9.37002182006836, 9.40088939666748, 9.454127311706543, 9.420952796936035, 9.450101852416992, 9.475909233093262, 9.432161331176758, 9.438759803771973, 9.441463470458984, 9.47204303741455, 9.403733253479004, 9.456913948059082, 9.489250183105469, 9.472453117370605, 9.467790603637695, 9.447869300842285, 9.50535774230957, 9.495697975158691, 9.550504684448242, 9.494595527648926, 9.526615142822266, 9.571975708007812, 9.5604248046875, 9.520313262939453, 9.625659942626953, 9.599746704101562, 9.614375114440918, 9.619553565979004, 9.54300594329834, 9.58000659942627, 9.659143447875977, 9.65058422088623, 9.589821815490723, 9.686781883239746, 9.678272247314453, 9.641669273376465, 9.632264137268066, 9.678268432617188, 9.668156623840332, 9.700486183166504, 9.670654296875, 9.736562728881836, 9.714948654174805, 9.747123718261719, 9.710671424865723, 9.726539611816406, 9.767770767211914, 9.706463813781738, 9.687151908874512, 9.706415176391602, 9.693958282470703, 9.683018684387207, 9.801871299743652, 9.735254287719727, 9.724715232849121, 9.698816299438477, 9.753783226013184, 9.760106086730957, 9.839076042175293, 9.845850944519043, 9.855257987976074, 9.79184341430664, 9.765809059143066, 9.850480079650879, 9.797479629516602, 9.804856300354004, 9.857549667358398, 9.81493091583252, 9.823090553283691, 9.786590576171875, 9.7866849899292, 9.8911714553833, 9.788925170898438, 9.90186595916748, 9.879353523254395, 9.860163688659668, 9.89746379852295, 9.779236793518066, 9.85682201385498, 9.872174263000488, 9.904027938842773, 9.97659969329834, 9.994986534118652, 9.93973159790039, 9.892644882202148, 9.907636642456055, 9.947927474975586, 9.991455078125, 9.984703063964844, 9.985127449035645, 9.975567817687988, 9.97638988494873, 9.952582359313965, 9.976996421813965, 9.938711166381836, 10.002985000610352, 10.014431953430176, 10.014984130859375, 10.060877799987793, 10.088844299316406, 10.02536392211914, 9.989370346069336, 10.029144287109375, 9.94666576385498, 10.048295974731445, 10.043638229370117, 10.055829048156738, 10.002419471740723, 10.03606128692627, 9.985846519470215, 10.056924819946289, 10.046748161315918, 10.043429374694824, 10.057329177856445, 10.089051246643066, 10.093863487243652, 10.153517723083496, 10.104842185974121, 10.064675331115723, 10.149765968322754, 10.174967765808105, 10.1535005569458, 10.148859977722168, 10.098540306091309, 10.18799114227295, 10.151997566223145, 10.110013008117676, 10.152444839477539, 10.204248428344727, 10.182126998901367, 10.177501678466797, 10.136330604553223, 10.192605018615723, 10.156577110290527, 10.183003425598145, 10.133132934570312, 10.110381126403809, 10.160709381103516, 10.199335098266602, 10.191929817199707, 10.230559349060059, 10.198878288269043, 10.20158863067627, 10.221324920654297, 10.235447883605957, 10.189408302307129, 10.219768524169922, 10.25086498260498, 10.29871654510498, 10.25256061553955, 10.304192543029785, 10.194900512695312, 10.247417449951172, 10.246251106262207, 10.29559326171875, 10.261749267578125, 10.271471977233887, 10.33713436126709, 10.273933410644531, 10.226823806762695, 10.266448974609375, 10.23887825012207, 10.33549976348877, 10.311206817626953, 10.280656814575195, 10.298513412475586, 10.356525421142578, 10.356785774230957, 10.358695983886719, 10.318659782409668, 10.338214874267578, 10.333951950073242, 10.359245300292969, 10.304974555969238, 10.414581298828125, 10.360187530517578, 10.422396659851074, 10.383631706237793, 10.391356468200684, 10.337711334228516, 10.399136543273926, 10.380802154541016, 10.424531936645508, 10.395012855529785, 10.465195655822754, 10.338091850280762, 10.4320068359375, 10.43752384185791, 10.413241386413574, 10.444828987121582, 10.409032821655273, 10.471036911010742, 10.342068672180176, 10.471902847290039, 10.459872245788574, 10.41877269744873, 10.499322891235352, 10.444331169128418, 10.44308853149414, 10.461743354797363, 10.463170051574707, 10.444615364074707, 10.432865142822266, 10.487117767333984, 10.523528099060059, 10.545230865478516, 10.504417419433594, 10.468966484069824, 10.44719123840332, 10.427950859069824, 10.499178886413574, 10.495201110839844, 10.548486709594727, 10.526596069335938, 10.525449752807617, 10.437758445739746, 10.4793119430542, 10.520360946655273, 10.438840866088867]}, {\"line\": {\"color\": \"firebrick\", \"dash\": \"dot\", \"width\": 2}, \"mode\": \"lines\", \"name\": \"validation_accuracy\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"xaxis\": \"x\", \"y\": [0.05711717903614044, 0.05711717903614044, 0.05711717903614044, 0.05711717903614044, 0.06867542117834091, 0.0748165175318718, 0.08243047446012497, 0.08530130982398987, 0.09149233251810074, 0.0948624461889267, 0.0980827808380127, 0.10175246000289917, 0.10320036113262177, 0.1059214174747467, 0.10819312185049057, 0.10959109663963318, 0.11243696510791779, 0.11338558793067932, 0.11515802145004272, 0.11842828243970871, 0.11912726610898972, 0.11875280737876892, 0.12055020034313202, 0.12147386372089386, 0.1216985359787941, 0.12402017414569855, 0.12509360909461975, 0.12649159133434296, 0.12626691162586212, 0.1270158290863037, 0.12721553444862366, 0.1295870989561081, 0.1320335566997528, 0.13081032037734985, 0.13368116319179535, 0.13223326206207275, 0.13413050770759583, 0.13512906432151794, 0.13575315475463867, 0.1353038102388382, 0.1381496787071228, 0.13877378404140472, 0.13944779336452484, 0.13972240686416626, 0.1399720460176468, 0.1403215378522873, 0.14094562828540802, 0.14189425110816956, 0.14184433221817017, 0.143466979265213, 0.14449049532413483, 0.14518947899341583, 0.1459883153438568, 0.14611314237117767, 0.14696191251277924, 0.14796045422554016, 0.14791053533554077, 0.1489090770483017, 0.15008237957954407, 0.14908382296562195, 0.15083129703998566, 0.1503569781780243, 0.1499325931072235, 0.15220430493354797, 0.14950820803642273, 0.1517299860715866, 0.15220430493354797, 0.152104452252388, 0.15267860889434814, 0.15312796831130981, 0.1549253612756729, 0.15417644381523132, 0.1553747057914734, 0.1549253612756729, 0.1560736894607544, 0.1570972055196762, 0.1564231812953949, 0.1574966311454773, 0.1588696390390396, 0.15879474580287933, 0.15772131085395813, 0.158220574259758, 0.15809576213359833, 0.16054221987724304, 0.15944381058216095, 0.16059213876724243, 0.16169054806232452, 0.16061709821224213, 0.16064207255840302, 0.16079185903072357, 0.16418692469596863, 0.1635378748178482, 0.16411203145980835, 0.16383743286132812, 0.16483598947525024, 0.1661091446876526, 0.16481103003025055, 0.16590942442417145, 0.16735732555389404, 0.1653851866722107, 0.16673323512077332, 0.1692545861005783, 0.16673323512077332, 0.1682560294866562, 0.1682060956954956, 0.1678066849708557, 0.16942933201789856, 0.1703280210494995, 0.1689799726009369, 0.17087723314762115, 0.17025312781333923, 0.1728493720293045, 0.17349842190742493, 0.1710519790649414, 0.17257477343082428, 0.17377303540706635, 0.17384791374206543, 0.17449697852134705, 0.17459683120250702, 0.17552049458026886, 0.17621947824954987, 0.17479655146598816, 0.1764940768480301, 0.17761746048927307, 0.1761196255683899, 0.17584502696990967, 0.1761445850133896, 0.1775175929069519, 0.17844125628471375, 0.17869089543819427, 0.17661890387535095, 0.1778670996427536, 0.1785660833120346, 0.18066303431987762, 0.1807878613471985, 0.18023864924907684, 0.1821608692407608, 0.18131209909915924, 0.18305955827236176, 0.18410804867744446, 0.18445754051208496, 0.18498177826404572, 0.18378351628780365, 0.18600529432296753, 0.18488192558288574, 0.18498177826404572, 0.1843077540397644, 0.185755655169487, 0.18568076193332672, 0.18627989292144775, 0.1868790239095688, 0.18655449151992798, 0.18750311434268951, 0.18979978561401367, 0.18872635066509247, 0.1882520318031311, 0.18977482616901398, 0.1886514574289322, 0.188975989818573, 0.19027410447597504, 0.1904488503932953, 0.19172200560569763, 0.18977482616901398, 0.18984971940517426, 0.19029906392097473, 0.19349443912506104, 0.1917968988418579, 0.1925957351922989, 0.1936691850423813, 0.19334465265274048, 0.19481751322746277, 0.19496729969978333, 0.19461780786514282, 0.19314493238925934, 0.19411852955818176, 0.1957411766052246, 0.19496729969978333, 0.19671475887298584, 0.19678965210914612, 0.19621548056602478, 0.19693943858146667, 0.19676469266414642, 0.1968895047903061, 0.19691447913646698, 0.1975885033607483, 0.19918617606163025, 0.19873683154582977, 0.19803784787654877, 0.1996854543685913, 0.19943581521511078, 0.199710413813591, 0.19996005296707153, 0.20060911774635315, 0.2007589042186737, 0.2004842907190323, 0.20065905153751373, 0.20303060114383698, 0.20195716619491577, 0.20280593633651733, 0.20375455915927887, 0.202930748462677, 0.2036047726869583, 0.20323032140731812, 0.2043287307024002, 0.2043786495923996, 0.20457835495471954, 0.20642568171024323, 0.20712466537952423, 0.2054770588874817, 0.20592640340328217, 0.20487792789936066, 0.2065255343914032, 0.20709970593452454, 0.20729941129684448, 0.20879724621772766, 0.20802336931228638, 0.20804832875728607, 0.20907184481620789, 0.20912177860736847, 0.20949622988700867, 0.2090219110250473, 0.2097209095954895, 0.21051974594593048, 0.2115432620048523, 0.2108442783355713, 0.210769385099411, 0.21174296736717224, 0.21209245920181274, 0.2111688107252121, 0.21246692538261414, 0.2125917375087738, 0.2147136628627777, 0.21391482651233673, 0.2129662036895752, 0.21458885073661804, 0.2147386223077774, 0.21319086849689484, 0.21411453187465668, 0.21271656453609467, 0.2158370316028595, 0.21641120314598083, 0.21683558821678162, 0.21641120314598083, 0.21701033413410187, 0.2165110558271408, 0.2187328338623047, 0.21698537468910217, 0.21743471920490265, 0.21628639101982117, 0.21960656344890594, 0.21890757977962494, 0.21883268654346466, 0.21985620260238647, 0.21980628371238708, 0.21930700540542603, 0.22053022682666779, 0.22055520117282867, 0.2208547592163086, 0.22205302119255066, 0.22397524118423462, 0.2222527265548706, 0.22160367667675018, 0.2233511358499527, 0.22275200486183167, 0.22360077500343323, 0.22355085611343384, 0.22177842259407043, 0.2241000533103943, 0.22340106964111328, 0.2237256020307541, 0.22392530739307404, 0.2233012169599533, 0.22492386400699615, 0.22599729895591736, 0.2262219786643982, 0.22529831528663635, 0.22697089612483978, 0.22524839639663696, 0.2265964299440384, 0.2265215367078781, 0.22642168402671814, 0.22709570825099945, 0.22604723274707794, 0.22664636373519897, 0.22776973247528076, 0.22724549472332, 0.2290928214788437, 0.22891807556152344, 0.22854360938072205, 0.22931748628616333, 0.22999151051044464, 0.23006640374660492, 0.22829397022724152, 0.23061560094356537, 0.23006640374660492, 0.23034100234508514, 0.23143941164016724, 0.23099006712436676, 0.2322881817817688, 0.23039093613624573, 0.23223824799060822, 0.2316141575574875, 0.23178890347480774, 0.23103998601436615, 0.23293724656105042, 0.23391082882881165, 0.23293724656105042, 0.23308701813220978, 0.23276250064373016, 0.23321184515953064, 0.2337111234664917, 0.2326127141714096, 0.23308701813220978, 0.23426032066345215, 0.23416046798229218, 0.234385147690773, 0.23493434488773346, 0.23530879616737366, 0.23383593559265137, 0.23448500037193298, 0.23388586938381195, 0.23533377051353455, 0.23323680460453033, 0.23595786094665527, 0.23685656487941742, 0.23650705814361572, 0.2354835420846939, 0.23783014714717865, 0.23638224601745605, 0.23758050799369812, 0.23788008093833923, 0.23608267307281494, 0.23720605671405792, 0.2376554012298584, 0.23827949166297913, 0.23795495927333832, 0.23920315504074097, 0.23815467953681946, 0.2390533685684204, 0.2409256547689438, 0.23758050799369812, 0.24005192518234253, 0.24080084264278412, 0.24032652378082275, 0.2401517778635025, 0.23817963898181915, 0.24112537503242493, 0.23967747390270233, 0.239802286028862, 0.24070098996162415, 0.24115033447742462, 0.24212391674518585, 0.24152478575706482, 0.23985221982002258, 0.2416246384382248, 0.24284787476062775, 0.24137499928474426, 0.24257327616214752, 0.24359677731990814, 0.2426231950521469, 0.2441210299730301, 0.24147486686706543, 0.24169953167438507, 0.243022620677948, 0.24437066912651062, 0.24312247335910797, 0.24357181787490845, 0.2433970719575882, 0.24422088265419006, 0.24344700574874878, 0.24462029337882996, 0.24579359591007233, 0.24486993253231049, 0.2448200136423111, 0.24634280800819397, 0.24529431760311127, 0.24651755392551422, 0.2462679147720337, 0.24828997254371643, 0.24631783366203308, 0.2465674728155136, 0.2455189973115921, 0.24779070913791656, 0.24566878378391266, 0.24853961169719696, 0.24794048070907593, 0.246941938996315, 0.24789056181907654, 0.24809026718139648, 0.24736632406711578, 0.24601827561855316, 0.2483898401260376, 0.24958810210227966, 0.24973787367343903, 0.24918867647647858, 0.24796545505523682, 0.2498627007007599, 0.24901393055915833, 0.2509361505508423, 0.25076138973236084, 0.2501123249530792, 0.2512606680393219, 0.24828997254371643, 0.25138548016548157, 0.25036197900772095, 0.25243398547172546, 0.25255879759788513, 0.25111088156700134, 0.25041189789772034, 0.2519596517086029, 0.25255879759788513, 0.254181444644928, 0.25315791368484497, 0.2512107491493225, 0.2512856423854828, 0.254181444644928, 0.2534574866294861, 0.2533825933933258, 0.25223425030708313, 0.25353237986564636, 0.25508013367652893, 0.2544809877872467, 0.2548554539680481, 0.2540316581726074, 0.2552049458026886, 0.25353237986564636, 0.2541564702987671, 0.2541564702987671, 0.25530481338500977, 0.2556293308734894, 0.25605371594429016, 0.25537970662117004, 0.2547306418418884, 0.2549053728580475, 0.25602877140045166, 0.25652801990509033, 0.2566278874874115, 0.25607869029045105, 0.2566778063774109, 0.2573268711566925, 0.2577013373374939, 0.2573518455028534, 0.2563033699989319, 0.25650307536125183, 0.25787606835365295, 0.25782614946365356, 0.25707724690437317, 0.2583504021167755, 0.25790104269981384, 0.2572270333766937, 0.25902441143989563, 0.2580508291721344, 0.2602476477622986, 0.25967347621917725, 0.25862500071525574, 0.2597982883453369, 0.2594238221645355, 0.2590493857860565, 0.25972339510917664, 0.258000910282135, 0.26007288694381714, 0.26137101650238037, 0.260097861289978, 0.26034748554229736, 0.2612212300300598, 0.26004794239997864, 0.261321097612381, 0.2611713111400604, 0.262020081281662, 0.259773313999176, 0.26137101650238037, 0.260996550321579, 0.2620450258255005, 0.2627440094947815, 0.2612462043762207, 0.26291877031326294, 0.2625942528247833, 0.263093501329422, 0.26319336891174316, 0.26366767287254333, 0.2624194920063019, 0.2626441717147827, 0.2641420066356659, 0.26329323649406433, 0.26286885142326355, 0.26394227147102356, 0.26359277963638306, 0.2640671133995056, 0.2644166052341461, 0.26549002528190613, 0.26651355624198914, 0.26501572132110596, 0.2655898928642273, 0.26466622948646545, 0.26509061455726624, 0.2665884494781494, 0.265839546918869, 0.2656398117542267, 0.2661890387535095, 0.2666383683681488], \"yaxis\": \"y2\"}],\n",
              "                        {\"legend\": {\"orientation\": \"h\"}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Training Metrics\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.94]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"tickfont\": {\"color\": \"royalblue\"}, \"title\": {\"font\": {\"color\": \"royalblue\"}, \"text\": \"Loss\"}}, \"yaxis2\": {\"anchor\": \"x\", \"overlaying\": \"y\", \"side\": \"right\", \"tickfont\": {\"color\": \"firebrick\"}, \"title\": {\"font\": {\"color\": \"firebrick\"}, \"text\": \"Accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dc2c2089-688c-4027-84aa-30c456c68a31');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMTPlfM8I_Wd",
        "colab_type": "text"
      },
      "source": [
        "# Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W8epfdOM9vm",
        "colab_type": "text"
      },
      "source": [
        "### 100 Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn7qrsCu-LI9",
        "colab_type": "text"
      },
      "source": [
        "<tr>\n",
        "    <th>LSTM Layers</th>\n",
        "    <th>LSTM Cells per Layer</th>\n",
        "    <th>Dropout %</th>\n",
        "    <th>Validation Loss</th>\n",
        "    <th>Validation Accuracy</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2</td>\n",
        "    <td>8.9918</td>\n",
        "    <td>0.2271</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2</td>\n",
        "    <td>10.7950</td>\n",
        "    <td>0.2354</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5</td>\n",
        "    <td>8.9682</td>\n",
        "    <td>0.2153</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>6.9549</td>\n",
        "    <td>0.1490</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>7.4581</td>\n",
        "    <td>0.1683</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5, 0.5</td>\n",
        "    <td>7.3286</td>\n",
        "    <td>0.1650</td>\n",
        "</tr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXjdqe2PNKFv",
        "colab_type": "text"
      },
      "source": [
        "### 500 Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUnZhzWmNGKs",
        "colab_type": "text"
      },
      "source": [
        "<tr>\n",
        "    <th>LSTM Layers</th>\n",
        "    <th>LSTM Cells per Layer</th>\n",
        "    <th>Dropout %</th>\n",
        "    <th>Validation Loss</th>\n",
        "    <th>Validation Accuracy</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2</td>\n",
        "    <td>16.4351</td>\n",
        "    <td>0.3229</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2</td>\n",
        "    <td>22.5922</td>\n",
        "    <td>0.3373</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>1</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5</td>\n",
        "    <td>18.8511</td>\n",
        "    <td>0.3329</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>64</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>8.8127</td>\n",
        "    <td>0.2230</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.2, 0.5</td>\n",
        "    <td>10.8240</td>\n",
        "    <td>0.2854</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>2</td>\n",
        "    <td>256</td>\n",
        "    <td>0.5, 0.5</td>\n",
        "    <td>10.4388</td>\n",
        "    <td>0.2666</td>\n",
        "</tr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TPnMMNvDAXC",
        "colab_type": "text"
      },
      "source": [
        "# Generate Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rmhEm6UESld",
        "colab_type": "text"
      },
      "source": [
        "### Load Objects To Infer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbsosSoSDBVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model_filepath = os.path.join(ROOT_FOLDER, 'models', f'{content_type}-custom-2.h5')\n",
        "model = load_model(model_filepath)\n",
        "TRAINING_LENGTH = 10"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j56FokGJQUKc",
        "colab_type": "text"
      },
      "source": [
        "## Existing Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68jLzLENLLOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5c9b4e76-5287-47bb-b9c0-1bb73862d4f0"
      },
      "source": [
        "original_sequence, gen_list, a = predict_utils.generate_output(\n",
        "    model,\n",
        "    sequences,\n",
        "    idx_word,\n",
        "    seed_length=TRAINING_LENGTH,\n",
        "    new_words=20,\n",
        "    diversity=1,\n",
        "    n_gen=1\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py:42: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFkDMZOGLRKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f59f7157-a176-43bd-a9f1-c9f52549a78b"
      },
      "source": [
        "' '.join(word for word in original_sequence)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'retirement under 65 employers will no longer be able to'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x2HCA1iLRMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "99e39057-3550-4d77-a502-02b2629423aa"
      },
      "source": [
        "' '.join(word for word in gen_list[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"< --- > advise for the companies said mr rubinsohn is due to delivering improvements and the us's biggest manufacturers could be above\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmsySNZULlku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "496d09c4-ff3a-407b-f4ec-f4041e2bf11c"
      },
      "source": [
        "' '.join(word for word in a)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'< --- > force workers to retire before 65 unless they can justify it the government has announced that firms will be barred'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6sCL1XKLlo7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6fe4777a-4d2c-4c67-b8a3-cd03a3b87ed1"
      },
      "source": [
        "original_sequence, gen_list, a = predict_utils.generate_output(\n",
        "    model,\n",
        "    sequences,\n",
        "    idx_word,\n",
        "    seed_length=TRAINING_LENGTH,\n",
        "    new_words=20,\n",
        "    diversity=1,\n",
        "    n_gen=1\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py:42: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EzOOc7FXzUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4edd5821-3067-4710-864b-8d0fb5f00b39"
      },
      "source": [
        "' '.join(word for word in original_sequence)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'surging and there are many companies who will continue with'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQTuziQBXzZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "722bf091-5b0b-40ce-8406-6cb94bf4374a"
      },
      "source": [
        "' '.join(word for word in gen_list[0])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'< --- > forecasts by 90 of its known manufacturing making currently frozen goods the firm recently indicated that the merger for an'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-5Jj-0LXzXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "02b9ad56-cd11-4411-8a03-ed9fd3178a94"
      },
      "source": [
        "' '.join(word for word in a)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'< --- > existing trading relationships christian aid has called on british firms not to simply cut and run but look after their'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxudAXauQRiT",
        "colab_type": "text"
      },
      "source": [
        "## Custom Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvCuoZqgLls7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "875e9092-97c5-4101-d8ee-46ddf5c6a9e0"
      },
      "source": [
        "sentence = 'Stocks of major large technology firms are becoming even more fragile even though'\n",
        "predict_utils.generate_custom_sentence(sentence, word_idx, idx_word, model, new_words=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 3, 546, 490, 45, 126, 13, 518, 150, 24, 9544, 150, 456]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c04f970c9511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Stocks of major large technology firms are becoming even more fragile even though'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_custom_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py\u001b[0m in \u001b[0;36mgenerate_custom_sentence\u001b[0;34m(sentence, word_idx, idx_word, model, new_words, diversity)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Make a prediction from the seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \t\tpreds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n\u001b[0m\u001b[1;32m     93\u001b[0m \t\t\tnp.float64)\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8_8bcGDQaG7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "74b90cde-dcae-4035-ffc8-a9cb39a67f38"
      },
      "source": [
        "sentence = 'However, there have been many instances of'\n",
        "predict_utils.generate_custom_sentence(sentence, word_idx, idx_word, model, new_words=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 56, 18, 46, 67, 7424, 3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-833bdbd219b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'However, there have been many instances of'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_custom_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/Code/autocomplete_me/src/predict_utils.py\u001b[0m in \u001b[0;36mgenerate_custom_sentence\u001b[0;34m(sentence, word_idx, idx_word, model, new_words, diversity)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Make a prediction from the seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \t\tpreds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n\u001b[0m\u001b[1;32m     93\u001b[0m \t\t\tnp.float64)\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "240jLlPSQvDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}